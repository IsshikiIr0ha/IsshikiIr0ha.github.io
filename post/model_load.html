<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="iroha"><meta name="copyright" content="iroha"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>Huggingfaceçš„æ¨¡å‹åŠ è½½æµç¨‹ | iroha</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...undefined?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"example.com","root":"/","title":"ä¸æƒ³æ‘†çƒ‚","version":"1.10.9","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"æœç´¢...","empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹: ${query}","hits":"æ‰¾åˆ° ${hits} æ¡ç»“æœ","hits_time":"æ‰¾åˆ° ${hits} æ¡ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><meta name="description" content="æˆ‘ä»¬ä»¥ä¸‹é¢çš„è¿™ä¸€å¥è¯­å¥ä½œä¸ºå¼€å§‹ï¼Œä»¥ä»æœ¬åœ°åŠ è½½æ¨¡å‹ä¸ºä¾‹ã€‚ model &#x3D; AutoModelForSeq2SeqLM.from_pretrained(&quot;bigscience&#x2F;T0_3B&quot;) inputs &#x3D; tokenizer.encode(q.strip()+&quot; ? To answer this question, we need to know&quot;, return_tensors&#x3D;&quot;pt&quot;) o">
<meta property="og:type" content="article">
<meta property="og:title" content="Huggingfaceçš„æ¨¡å‹åŠ è½½æµç¨‹">
<meta property="og:url" content="http://example.com/post/model_load.html">
<meta property="og:site_name" content="iroha">
<meta property="og:description" content="æˆ‘ä»¬ä»¥ä¸‹é¢çš„è¿™ä¸€å¥è¯­å¥ä½œä¸ºå¼€å§‹ï¼Œä»¥ä»æœ¬åœ°åŠ è½½æ¨¡å‹ä¸ºä¾‹ã€‚ model &#x3D; AutoModelForSeq2SeqLM.from_pretrained(&quot;bigscience&#x2F;T0_3B&quot;) inputs &#x3D; tokenizer.encode(q.strip()+&quot; ? To answer this question, we need to know&quot;, return_tensors&#x3D;&quot;pt&quot;) o">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20240422014943359.png">
<meta property="article:published_time" content="2024-04-11T08:00:00.000Z">
<meta property="article:modified_time" content="2024-07-22T09:14:59.982Z">
<meta property="article:author" content="iroha">
<meta property="article:tag" content="å¤§æ¨¡å‹">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20240422014943359.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="æ–‡ç« ç›®å½•"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="ç«™ç‚¹æ¦‚è§ˆ"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="iroha"><img width="96" loading="lazy" src="/yun.png" alt="iroha"><span class="site-author-status" title="æ°¸è¿œç›¸ä¿¡ç¾å¥½çš„äº‹æƒ…å³å°†å‘ç”Ÿ">ğŸ˜Š</span></a><div class="site-author-name"><a href="/about/">iroha</a></div><span class="site-name">iroha</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="é¦–é¡µ"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="å½’æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">14</span></a></div><div class="site-state-item"><a href="/categories/" title="åˆ†ç±»"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">4</span></a></div><div class="site-state-item"><a href="/tags/" title="æ ‡ç­¾"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">2</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="æ–‡æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="å‹é“¾" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.</span> <span class="toc-text">åŠ è½½</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E8%AF%8D"><span class="toc-number">2.</span> <span class="toc-text">åˆ†è¯</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%9F%E6%88%90"><span class="toc-number">3.</span> <span class="toc-text">ç”Ÿæˆ</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://example.com/post/model_load.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="iroha"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="iroha"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Huggingfaceçš„æ¨¡å‹åŠ è½½æµç¨‹</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="åˆ›å»ºæ—¶é—´ï¼š2024-04-11 16:00:00" itemprop="dateCreated datePublished" datetime="2024-04-11T16:00:00+08:00">2024-04-11</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="ä¿®æ”¹æ—¶é—´ï¼š2024-07-22 17:14:59" itemprop="dateModified" datetime="2024-07-22T17:14:59+08:00">2024-07-22</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">NLP</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">å¤§æ¨¡å‹</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>æˆ‘ä»¬ä»¥ä¸‹é¢çš„è¿™ä¸€å¥è¯­å¥ä½œä¸ºå¼€å§‹ï¼Œä»¥ä»æœ¬åœ°åŠ è½½æ¨¡å‹ä¸ºä¾‹ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B")
inputs = tokenizer.encode(q.strip()+" ? To answer this question, we need to know", return_tensors="pt")
outputs = model.generate(inputs.cuda(), max_new_tokens=100, do_sample=False, top_k=50)</code></pre>
<h1 id="åŠ è½½">åŠ è½½</h1>
<p>AutoModelForSeq2SeqLMç»§æ‰¿äº†_BaseAutoModelClassï¼Œè¿™ä¸ªç±»æ˜¯æ‰€æœ‰AutoModelçš„åŸºç±»ï¼Œä¿å­˜åœ¨transformers/models/auto/auto_factory.pyä¸­ã€‚è°ƒç”¨çš„from_pretrainedæ–¹æ³•å®é™…ä¸Šå°±æ¥è‡ªäºè¿™ä¸ªåŸºç±»ã€‚æˆ‘ä»¬å‡è®¾æ¨¡å‹ä¿å­˜åœ¨æœ¬åœ°ï¼Œä¸€äº›ä¸‹è½½çš„é€»è¾‘ä¸çœ‹ï¼Œä¸”kwargså’Œconfigä¸ºNoneã€‚æœ€ç»ˆä¼šå¾—åˆ°æ¨¡å‹çš„å“ˆå¸Œå€¼ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if not isinstance(config, PretrainedConfig):
    # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible
    resolved_config_file = cached_file(
        pretrained_model_name_or_path,
        CONFIG_NAME,
        _raise_exceptions_for_gated_repo=False,
        _raise_exceptions_for_missing_entries=False,
        _raise_exceptions_for_connection_errors=False,
        **hub_kwargs,
    )
    commit_hash = extract_commit_hash(resolved_config_file, commit_hash)</code></pre>
<p>é¦–å…ˆéœ€è¦åŠ è½½configï¼Œé€šè¿‡cached_fileæ¥åŠ è½½ã€‚CONFIG_NAMEé»˜è®¤ä¸ºconfig.jsonï¼Œpretrained_model_name_or_pathåˆ™æ˜¯from_pretrainedä¼ å…¥çš„å­—ç¬¦ä¸²ã€‚å†çœ‹å…·ä½“å®ç°ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">path_or_repo_id = str(path_or_repo_id) # "bigscience/T0_3B"
    full_filename = os.path.join(subfolder, filename) # filename=config.json
    if os.path.isdir(path_or_repo_id):
        resolved_file = os.path.join(os.path.join(path_or_repo_id, subfolder), filename) # subfolderä¸æŒ‡å®š=None
        if not os.path.isfile(resolved_file):
            if _raise_exceptions_for_missing_entries:
                raise EnvironmentError(
                    f"{path_or_repo_id} does not appear to have a file named {full_filename}. Checkout "
                    f"'https://huggingface.co/{path_or_repo_id}/tree/{revision}' for available files."
                )
            else:
                return None
        return resolved_file # è¿”å›bigscience/T0_3B/config.json</code></pre>
<p>æ­¤æ—¶å¯¹åº”çš„config.jsonå·²ç»è¢«åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œä¹‹åéœ€è¦åŠ è½½åˆ°AutoConfigä¸­ã€‚å¯ä»¥çœ‹åˆ°å°±æ˜¯T0_3B/config.json</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">            config, kwargs = AutoConfig.from_pretrained(
                pretrained_model_name_or_path,
                return_unused_kwargs=True,
                trust_remote_code=trust_remote_code,
                code_revision=code_revision,
                _commit_hash=commit_hash,
                **hub_kwargs,
                **kwargs,
            )
---------config---------
T5Config {
  "_name_or_path": "/data2/wtf/model/bigscience/T0_3B",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 5120,
  "d_kv": 64,
  "d_model": 2048,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 24,
  "num_heads": 32,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.38.2",
  "use_cache": true,
  "vocab_size": 32128
}</code></pre>
<p>æœ€ç»ˆå°†æ¨¡å‹åŠ è½½ï¼Œè¿”å›modelå®ä¾‹ã€‚model_classå°±æ˜¯jsonä¸­çš„architectureså¯¹åº”å€¼ã€‚_get_model_classæ–¹æ³•å°±æ˜¯å¾—åˆ°æ¨¡å‹çš„ç±»å‹ï¼</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">          model_class = _get_model_class(config, cls._model_mapping) # T5ForConditionalGeneration
  		"""
  		"architectures": [
  "T5ForConditionalGeneration"
]
			å°±æ˜¯è¿”å›archä¸­çš„æ¨¡å‹ç±»å‹
  		"""
          return model_class.from_pretrained(
              pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
          )</code></pre>
<p>ä¸Šé¢çš„<code>cls._model_mapping</code>å°±æ˜¯æ¨¡å‹æ ¹æ®ä½ çš„è¾“å…¥ï¼Œå¾—åˆ°å½“å‰æ¨¡å‹ç±»å‹çš„æ˜ å°„ã€‚</p>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">class AutoModelForSeq2SeqLM(_BaseAutoModelClass):
    _model_mapping = MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING
------
MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING = _LazyAutoMapping(
    CONFIG_MAPPING_NAMES, MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES
)
</code></pre>
<p><code>CONFIG_MAPPING_NAMES</code>æ˜¯æ ¹æ®ä½ ä¼ å…¥çš„è·¯å¾„æ¥åŒ¹é…ï¼Œè€Œ<code>MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES</code>åŒç†ï¼Œä¹Ÿæœ‰å’Œä¸‹å›¾ç±»ä¼¼çš„å­—å…¸ã€‚æœ¬ä¾‹ä¸­<code>CONFIG_MAPPING_NAMES="T5Config"</code>ï¼Œ<code>MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES=T5ForConditionalGeneration</code>ï¼Œå’Œconfig.jsonä¸­çš„ä¸€æ ·ã€‚</p>
<figure>
<img src="../images/image-20240422014943359.png" alt="image-20240422014943359" loading="lazy">
<figcaption aria-hidden="true">image-20240422014943359</figcaption>
</figure>
<p><code>model_class.from_pretrained</code>ä¸­çš„<code>from_pretrained</code>æ¥è‡ªäºPretrainedModelç±»ï¼Œè¿™æ˜¯æ‰€æœ‰æ¨¡å‹çš„åŸºç±»(æ³¨æ„ä¸æ˜¯AutoModel)ã€‚è¿™ä¸ªå‡½æ•°æ˜¯æœ¬ç¯‡çš„æ ¸å¿ƒã€‚è¿”å›çš„æ¨¡å‹å®ä¾‹é»˜è®¤æ˜¯å¼€å¯model.eval()æ¨¡å¼ï¼Œè‹¥è¦å¾®è°ƒorè®­ç»ƒæ¨¡å‹ï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®šmodel.train()ã€‚</p>
<p>è¿™é‡Œé¡ºä¾¿æä¸€å˜´model.trainå’Œevalä¸‹çš„åŒºåˆ«ï¼š</p>
<ol type="1">
<li>Dropout å’Œ BatchNorm è¡Œä¸ºã€‚model.train()ä¸‹ Dropout
å±‚ä¼šéšæœºä¸¢å¼ƒä¸€éƒ¨åˆ†ç¥ç»å…ƒ, BatchNorm å±‚ä¼šè®¡ç®—å½“å‰ batch çš„ç»Ÿè®¡é‡ã€‚
model.eval()ä¸‹Dropout å±‚ä¼šå…¨éƒ¨ä¿ç•™ç¥ç»å…ƒ, BatchNorm
å±‚ä¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ»‘åŠ¨å¹³å‡ç»Ÿè®¡é‡ã€‚</li>
<li>æ¢¯åº¦ä¸ä¼˜åŒ–å™¨ã€‚model.train()ä¼šè®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°æ¨¡å‹å‚æ•°ã€‚model.eval()ä¸ä¼šè®¡ç®—æ¢¯åº¦,
ä¹Ÿä¸ä¼šæ›´æ–°æ¨¡å‹å‚æ•°ã€‚</li>
<li>æ•°æ®å¢å¼ºã€‚model.train()é€šå¸¸ä¼šåº”ç”¨ä¸€äº›æ•°æ®å¢å¼ºæŠ€æœ¯,
å¦‚ç¿»è½¬ã€æ—‹è½¬ç­‰ã€‚model.eval()ä¸€èˆ¬ä¸éœ€è¦æ•°æ®å¢å¼º,
ç›´æ¥ä½¿ç”¨åŸå§‹çš„è¾“å…¥æ•°æ®ã€‚</li>
<li>å†…å­˜ä¸è®¡ç®—å¼€é”€ã€‚model.train()éœ€è¦ä¿å­˜ä¸­é—´æ¿€æ´»å€¼ç”¨äºåå‘ä¼ æ’­,
è®¡ç®—å¼€é”€ç›¸å¯¹æ›´å¤§ã€‚model.eval()åªéœ€è¦å‰å‘ä¼ æ’­, ä¸éœ€è¦ä¿å­˜ä¸­é—´æ¿€æ´»å€¼,
è®¡ç®—å¼€é”€ç›¸å¯¹æ›´å°ã€‚</li>
</ol>
<p>ä¸‹é¢çœ‹å‡ ä¸ªæ¯”è¾ƒå…³é”®çš„å‚æ•°ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">@classmethod
def from_pretrained(
    cls,
    pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
    *model_args,
    config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
    cache_dir: Optional[Union[str, os.PathLike]] = None,
    ignore_mismatched_sizes: bool = False,
    force_download: bool = False,
    local_files_only: bool = False,
    token: Optional[Union[str, bool]] = None,
    revision: str = "main",
    use_safetensors: bool = None,
    **kwargs,
):</code></pre>
<ul>
<li><p>pretrained_model_name_or_pathï¼Œæ¨¡å‹çš„è·¯å¾„oråœ¨huggingfaceä¸­çš„åå­—ã€‚</p></li>
<li><p>force_downloadï¼Œä¸è®ºæœ‰æ²¡æœ‰ä¸‹è½½å¥½æ¨¡å‹ï¼Œéƒ½ä¸‹è½½ï¼Œè‹¥å­˜åœ¨åˆ™è¦†ç›–ã€‚</p></li>
<li><p>torch_dtypeï¼Œ<code>torch.float16</code> or
<code>torch.bfloat16</code> or
<code>torch.float</code>ï¼ŒæŒ‡å®šæ¨¡å‹å‚æ•°çš„è½½å…¥ç²¾åº¦ï¼Œä¸æŒ‡å®šåˆ™é»˜è®¤ä¸º<code>torch.float</code>ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œ<code>config.json</code>
ä¸­çš„ <code>torch_dtype</code> è®¾ç½®æ‹¥æœ‰æœ€é«˜ä¼˜å…ˆçº§ã€‚å¦‚æœ
<code>torch_dtype</code> å‚æ•°è¢«è®¾ç½®ä¸º
<code>"auto"</code>ï¼Œé‚£ä¹ˆå®ƒä¼šé¦–å…ˆä½¿ç”¨ <code>config.json</code>
ä¸­çš„è®¾ç½®ã€‚åªæœ‰å½“ <code>config.json</code> ä¸­æ²¡æœ‰æ‰¾åˆ°
<code>torch_dtype</code> ä¸” <code>torch_dtype</code> å‚æ•°è¢«è®¾ç½®ä¸º
<code>"auto"</code>æ—¶ï¼Œå®ƒæ‰ä¼šå›é€€åˆ°ä½¿ç”¨æƒé‡checkpointä¸­çš„æ•°æ®ç±»å‹ï¼ŒæŸ¥çœ‹ç¬¬ä¸€ä¸ªæ•°æ®æ˜¯ä»€ä¹ˆç±»å‹å°±ç”¨ä»€ä¹ˆç±»å‹ã€‚è‹¥æ ¹æœ¬æ²¡æœ‰è®¾ç½®è¯¥å‚æ•°ï¼Œåˆ™ä½¿ç”¨torch.floatã€‚</p></li>
<li><p>device_mapï¼Œå¯ä»¥ä¼ å…¥ä¸‰ç§ç±»å‹çš„å‚æ•°ã€‚<strong>å­—ç¬¦ä¸²ç±»å‹</strong>ï¼Œå¦‚æœä¼ å…¥ä¸€ä¸ªå­—ç¬¦ä¸²ç±»å‹çš„è®¾å¤‡åç§°(ä¾‹å¦‚
"cpu", "cuda:1", "mps")ï¼Œé‚£ä¹ˆæ•´ä¸ªæ¨¡å‹ä¼šè¢«åˆ†é…åˆ°æŒ‡å®šçš„è®¾å¤‡ä¸Šï¼Œå¦‚æœä¼ å…¥
"auto"ï¼ŒAccelerate
åº“ä¼šè‡ªåŠ¨è®¡ç®—å‡ºæœ€ä¼˜çš„è®¾å¤‡åˆ†å¸ƒã€‚<strong>å­—å…¸ç±»å‹</strong>ï¼Œè¿™ç§æƒ…å†µä¸‹
device_map
æ˜¯ä¸€ä¸ªå­—å…¸,é”®æ˜¯æ¨¡å‹çš„å­æ¨¡å—åç§°,å€¼æ˜¯å¯¹åº”çš„è®¾å¤‡ç¼–å·æˆ–è®¾å¤‡åç§°ï¼Œè¿™å…è®¸ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®šæ¨¡å‹çš„å„ä¸ªå­æ¨¡å—åº”è¯¥åˆ†å¸ƒåœ¨å“ªäº›è®¾å¤‡ä¸Šï¼Œåªéœ€è¦æŒ‡å®šåˆ°æ¨¡å—åç§°çš„çº§åˆ«,å­æ¨¡å—ä¼šè‡ªåŠ¨åˆ†é…åˆ°åŒä¸€è®¾å¤‡ï¼Œå¦‚</p>
<pre class="line-numbers language-none"><code class="language-none">device_map = {
    "transformer.encoder": "cuda:0",
    "transformer.decoder": "cuda:1",
    "transformer.pooler": "cuda:0",
    "lm_head": "cuda:1"
}</code></pre>
<p>è¿˜å¯ä»¥ä¼ å…¥æ•´æ•°æˆ–<code>torch.device</code>ï¼Œä»£è¡¨å°†æ•´ä¸ªæ¨¡å‹æ”¾åœ¨æŒ‡å®šç¼–å·çš„
GPU
ä¸Šã€‚å¦‚<code>device = torch.device("cuda:1"),device_map = deveice</code>ã€‚åªè¦æŒ‡å®šäº†device_mapï¼Œé‚£ä¹ˆéƒ½ä¼šè®©
<code>low_cpu_mem_usage=True</code>ã€‚ä¸æŒ‡å®šå°±ç”¨cpuã€‚</p></li>
<li><p>quantization_configï¼ŒæŒ‡å®šæ¨¡å‹çš„é‡åŒ–ç­–ç•¥ã€‚å¯ä»¥æ˜¯ä¸€ä¸ªå­—å…¸æˆ–è€…ç»§æ‰¿è‡ª
<code>QuantizationConfigMixin</code>
çš„å¯¹è±¡ï¼Œå®ƒç”¨äºé…ç½®æ¨¡å‹çš„é‡åŒ–å‚æ•°ã€‚é™¤äº† <code>quantization_config</code>
ä¹‹å¤–,è¿˜å¯ä»¥ä½¿ç”¨ <code>load_in_4bit</code> å’Œ <code>load_in_8bit</code>
ç­‰å‚æ•°æ¥æŒ‡å®šé‡åŒ–æ–¹å¼,ä½†è¿™ç§æ–¹å¼ä¸è¢«æ¨èï¼Œåªé‡åŒ–äº†å‚æ•°ï¼Œå¹¶ä¸é‡åŒ–æ¢¯åº¦ã€‚ä½†æ¨ç†é˜¶æ®µæ— æ‰€è°“ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ã€‚</p></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import bitsandbytes as bnb
from transformers import QuantizationConfig

quantization_config = QuantizationConfig(
    quantization_method=bnb.QuantizationMethod.DYNAMIC_QUANT,
    weight_bits=8,# æƒé‡ä¸ºINT8
    grad_bits=8,# æ¢¯åº¦ä¹ŸINT8
    per_channel=False
)
model = AutoModelForSeq2SeqLM.from_pretrained(
    "bigscience/T0_3B",
    quantization_config=quantization_config
)</code></pre>
<ul>
<li>local_files_onlyï¼Œå¦‚æœæ˜¯Trueï¼Œåˆ™ä¸ä¼šä»Hubä¸Šä¸‹è½½ã€‚</li>
<li>low_cpu_mem_usageï¼Œä½œç”¨æ˜¯å°è¯•åœ¨åŠ è½½æ¨¡å‹æ—¶ä¸ä½¿ç”¨è¶…è¿‡æ¨¡å‹å¤§å° 1 å€çš„
CPU å†…å­˜(åŒ…æ‹¬å³°å€¼å†…å­˜)ã€‚</li>
<li>attn_implementationï¼Œå¯ä»¥é€‰æ‹©<code>flash_attention_2</code>,<code>sdpa(default)</code>,<code>eager(æ‰‹åŠ¨å®ç°)</code></li>
</ul>
<p>ä¹‹åçœ‹å‡ å¤„æ¯”è¾ƒå…³é”®çš„æºç ã€‚</p>
<p>ä»ä¼ å…¥çš„è·¯å¾„ä¸­æå–configã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># Load config if we don't provide a configuration
if not isinstance(config, PretrainedConfig):
    config_path = config if config is not None else pretrained_model_name_or_path
    config, model_kwargs = cls.config_class.from_pretrained</code></pre>
<p>é‡åŒ–æ“ä½œã€‚æ³¨æ„åˆ°é‡åŒ–æ“ä½œä¼šå¼ºåˆ¶å¼€å¯low_cpu_mem_usageã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pre_quantized = getattr(config, "quantization_config", None) is not None
if pre_quantized or quantization_config is not None:
    if pre_quantized:
        config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
            config.quantization_config, quantization_config
        )
    else:
        config.quantization_config = quantization_config
    hf_quantizer = AutoHfQuantizer.from_config(config.quantization_config, pre_quantized=pre_quantized)
else:
    hf_quantizer = None

if hf_quantizer is not None:
    hf_quantizer.validate_environment(
        torch_dtype=torch_dtype, from_tf=from_tf, from_flax=from_flax, device_map=device_map
    )
    torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)
    device_map = hf_quantizer.update_device_map(device_map)

    # Force-set to `True` for more mem efficiency
    if low_cpu_mem_usage is None:
        low_cpu_mem_usage = True
        logger.warning("`low_cpu_mem_usage` was None, now set to True since model is quantized.")
is_quantized = hf_quantizer is not None</code></pre>
<p>åŠ è½½æƒé‡ï¼Œtfç›¸å…³çš„å°±ä¸çœ‹äº†ã€‚åœ¨åŠ è½½pytorchæƒé‡ä¸­ï¼Œä¼šå»ä½ æŒ‡å®šçš„æ–‡ä»¶å¤¹ä¸­æ‰¾<code>pytorch_model.bin</code>è¿™ä¸ªæƒé‡æ–‡ä»¶ã€‚<code>subfolder,variant</code>è‹¥ä¸åœ¨å‚æ•°ä¸­æŒ‡å®šéƒ½ä¸ºç©ºå­—ç¬¦ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">elif os.path.isfile(
    os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_NAME, variant))
):
    # Load from a PyTorch checkpoint,ä¼šæ‹¼æˆmodel_path/pytorch_model.bin
    archive_file = os.path.join(
        pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_NAME, variant)
    )
</code></pre>
<p>ä¸€äº›æ¨¡å‹çš„æƒé‡å¯èƒ½ä»¥å¤šä¸ªcheckpointæ–‡ä»¶æ¥ä¿å­˜ï¼Œè¿™æ—¶å€™è¦æ±‚æœ‰ä¸€ä¸ª<code>WEIGHTS_INDEX_NAME = "pytorch_model.bin.index.json"</code>æ–‡ä»¶æ¥è¿›è¡Œç´¢å¼•ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">                elif os.path.isfile(
                    os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_INDEX_NAME, variant))
                ):
                    # Load from a sharded PyTorch checkpoint
                    archive_file = os.path.join(
                        pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_INDEX_NAME, variant)
                    )
                    is_sharded = True # æ³¨æ„è¿™é‡Œ
----
{
    "checkpoint_files": ["pytorch_model.bin.0", "pytorch_model.bin.1", "pytorch_model.bin.2"],
    "num_checkpoint_files": 3,
    "size_checkpoint_files": [100000, 200000, 50000],
    "weight_map": {
        "layer1.weight": [0, 0],
        "layer1.bias": [0, 50000],
        "layer2.weight": [1, 0],
        "layer2.bias": [1, 100000]
    }
}</code></pre>
<p>è¿˜æœ‰ä¸€ç§æƒ…å†µï¼Œå°±æ˜¯æŒ‡å®šçš„è·¯å¾„ä¸æ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œè€Œæ˜¯æƒé‡æ–‡ä»¶æœ¬èº«ï¼Œå¦‚<code>bigscience/T0_3B/pytorch_model.bin</code>ï¼Œé‚£ä¹Ÿå¯ä»¥åŠ è½½ã€‚å› ä¸ºæœ€ç»ˆéƒ½æ˜¯è®©<code>archive_file = weight_file</code>ã€‚</p>
<pre class="line-numbers language-none"><code class="language-none">elif os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):
    archive_file = pretrained_model_name_or_path
    is_local = True</code></pre>
<p>æœ€ç»ˆï¼Œ<code>resolved_archive_file = archive_file</code>ï¼Œè·å–æƒé‡æ–‡ä»¶è·¯å¾„ã€‚å¦‚æœæ˜¯åˆ†æ•£çš„checkpointï¼Œä¹Ÿå°±æ˜¯<code>is_sharded</code>æ˜¯Trueï¼Œè¿˜è¦è¿›è¡Œé¢å¤–çš„æ“ä½œï¼Œè¿™é‡Œå°±ä¸æ·±å…¥äº†ã€‚</p>
<p>æ¥ä¸‹æ¥å°±è¦åŠ è½½æƒé‡äº†ï¼Œé¦–å…ˆåˆ¤æ–­æ˜¯ä¸æ˜¯pytorchï¼Œè‹¥æ˜¯ï¼Œåˆ™åŠ è½½æƒé‡æ–‡ä»¶ã€‚è¯¦ç»†çš„åŠ è½½æºç å°±ä¸èµ˜è¿°ï¼Œæœ€ç»ˆä¼šè¿”å›ç”±torch.loadåŠ è½½æ¨¡å‹ç»“æ„å’Œæƒé‡å‚æ•°ã€‚</p>
<pre class="line-numbers language-none"><code class="language-none">if from_pt:
    if not is_sharded and state_dict is None:
        # Time to load the checkpoint
        state_dict = load_state_dict(resolved_archive_file)</code></pre>
<p>æ¥ä¸‹æ¥å†³å®šæƒé‡çš„æ•°æ®ç±»å‹ï¼Œæ­£å¦‚ä¸Šé¢äº¤ä»£torch_dtypeå‚æ•°æ‰€è¯´ï¼Œå…ˆè€ƒè™‘<code>torch_dtype=auto</code>ï¼Œä¹Ÿå°±æ˜¯config.jsonä¸­çš„æ•°æ®ç±»å‹ã€‚ç„¶åå†è€ƒè™‘</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if torch_dtype is not None:
    if isinstance(torch_dtype, str):
        if torch_dtype == "auto":
            if hasattr(config, "torch_dtype") and config.torch_dtype is not None:
                torch_dtype = config.torch_dtype
                logger.info(f"Will use torch_dtype={torch_dtype} as defined in model's config object")

        else:
            raise ValueError(
                f'`torch_dtype` can be either `torch.dtype` or `"auto"`, but received {torch_dtype}'
            )
    dtype_orig = cls._set_default_torch_dtype(torch_dtype)</code></pre>
<p>è‹¥æ˜¯åˆ†ç‰‡æƒ…å†µï¼Œåˆ™å»åˆ†ç‰‡jsonä¸­æ‰¾æœ‰æ²¡æœ‰æŒ‡å®šã€‚å¦‚æœä¸æ˜¯åˆ†ç‰‡çš„æƒ…å†µï¼Œåˆ™æŒ‰æƒé‡æ–‡ä»¶ä¸­ç¬¬ä¸€ä¸ªæ•°æ®çš„ç±»å‹ã€‚è‹¥ä¸æ˜¾å¼æŒ‡å®štorch_dtype(None)ï¼Œåˆ™ä½¿ç”¨float32ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">                    else:
                        if is_sharded and "dtype" in sharded_metadata:
                            torch_dtype = sharded_metadata["dtype"]
                        elif not is_sharded:
                            torch_dtype = get_state_dict_dtype(state_dict)
                        else:
                            one_state_dict = load_state_dict(resolved_archive_file[0])
                            torch_dtype = get_state_dict_dtype(one_state_dict)
                            del one_state_dict  # free CPU memory
                        logger.info(
                            "Since the `torch_dtype` attribute can't be found in model's config object, "
                            "will use torch_dtype={torch_dtype} as derived from model's weights"
                        )    
-------------get_state_dict_dtype(state_dict)---------
# if no floating dtype was found return whatever the first dtype is
else:
    return next(state_dict.values()).dtype</code></pre>
<p>è¿˜æœ‰æ··åˆç²¾åº¦çš„æƒ…å†µï¼Œåœ¨åˆå§‹nn.Moduleçš„æ—¶å€™å¯ä»¥è®¾ç½®å•ç‹¬è®¾ç½®_keep_in_fp32_moduleså“ªäº›æ¨¡å—ä¿æŒfp32ç²¾åº¦ã€‚</p>
<pre class="line-numbers language-none"><code class="language-none"># Check if `_keep_in_fp32_modules` is not None
use_keep_in_fp32_modules = (cls._keep_in_fp32_modules is not None) and (
    (torch_dtype == torch.float16) or hasattr(hf_quantizer, "use_keep_in_fp32_modules")
)</code></pre>
<p>åˆ›å»ºæ¨¡å‹å®ä¾‹ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">with ContextManagers(init_contexts):
    # Let's make sure we don't run the init function of buffer modules
    model = cls(config, *model_args, **model_kwargs)</code></pre>
<p>æ¥çœ‹device_mapçš„é€»è¾‘ã€‚é¦–å…ˆæ˜¯å­—ç¬¦ä¸²çš„æƒ…å†µï¼Œå¿…é¡»è¦æ˜¯<code>"auto", "balanced", "balanced_low_0", "sequential"</code>è¿™å‡ ç§ï¼Œå¦åˆ™æŠ¥é”™ã€‚"auto"ä¼šè‡ªåŠ¨æ“ä½œï¼Œå°½å¯èƒ½å‡åŒ€åœ°åˆ†é…è®¡ç®—è´Ÿè½½ã€‚<code>balanced</code>åˆ™æ˜¯å¹³å‡åˆ†é…æ¨¡å‹å±‚ä¸­çš„å‚æ•°ç»™ä¸åŒçš„å¡ã€‚<code>balanced_low_0</code>åˆ™æ˜¯å°‘ç»™0åˆ†ä¸€äº›ï¼Œå› ä¸º0å¾€å¾€è¿˜æœ‰å…¶ä»–äº‹æƒ…è¦åšã€‚<code>sequential</code>åˆ™æ˜¯æŒ‰æ¨¡å‹å±‚çš„é¡ºåºæ¥åˆ†é…ç»™ä¸åŒçš„å¡ï¼Œä¿æŒæ¨¡å‹å±‚çš„æ‹“æ‰‘ç»“æ„,å‡å°‘è·¨è®¾å¤‡çš„æ•°æ®ä¼ è¾“ï¼Œå¦‚attentionä¸€å¼ å¡ï¼ŒMLPä¸€å¼ å¡ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if device_map not in ["auto", "balanced", "balanced_low_0", "sequential"]:
        raise ValueError
if device_map != "sequential":
                max_memory = get_balanced_memory(
                    model,
                    dtype=target_dtype,
                    low_zero=(device_map == "balanced_low_0"),
                    max_memory=max_memory,
                    **device_map_kwargs,
                )</code></pre>
<p>å®é™…ä¸Šå¯ä»¥çœ‹åˆ°ï¼Œ<code>auto</code>å°±æ˜¯<code>balanced</code>ç­–ç•¥ã€‚</p>
<p>å…¶ä»–æƒ…å†µï¼Œè¾“å…¥çš„ä»€ä¹ˆè®¾å¤‡å°±ç»‘å®šä»€ä¹ˆè®¾å¤‡ï¼Œè‹¥æ²¡æœ‰æŒ‡å®šdevice_mapï¼Œå°±åŠ è½½åˆ°cpuã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if isinstance(device_map, torch.device):
     device_map = {"": device_map}
elif isinstance(device_map, str) and device_map not in ["auto", "balanced", "balanced_low_0", "sequential"]:
      try:
          device_map = {"": torch.device(device_map)}
elif isinstance(device_map, int):# å°äº0æŠ¥é”™
	device_map = {"": device_map}</code></pre>
<p>è¿™é‡Œæœ‰ä¸€ä¸ª<code>tie_weights</code>å‡½æ•°ï¼Œå®ç°äº†å‚æ•°çš„ç»‘å®šæ“ä½œï¼Œæœ¬è´¨ä¸Šå°±æ˜¯é»˜è®¤è®©è¾“å…¥åµŒå…¥å±‚å’Œè¾“å‡ºåµŒå…¥å±‚çš„æƒé‡ç»‘å®šåœ¨ä¸€èµ·ã€‚è‹¥æ˜¯åœ¨configä¸­æŒ‡å®š<code>is_encoder_decoder=True</code>ä¸”<code>tie_encoder_decoder=True</code>ï¼Œé‚£ä¹ˆEncoderå’ŒDecoderçš„å‚æ•°ä¹Ÿä¼šå…±ç”¨(éƒ½ä½¿ç”¨Decoderçš„Weights)ï¼Œä¸è¿‡T5ä¸­å¹¶ä¸è¿™ä¹ˆåšï¼Œä¸€èˆ¬æ˜¯<strong>BERT-based</strong>æ¨¡å‹åœ¨å¾®è°ƒæˆEncoder-Decoderæ¨¡å‹çš„æ—¶å€™ä¼šè¿™ä¹ˆåšã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def tie_weights(self):
    """
    Tie the weights between the input embeddings and the output embeddings.

    If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the
    weights instead.
    """
    if getattr(self.config, "tie_word_embeddings", True):
        output_embeddings = self.get_output_embeddings()
        if output_embeddings is not None:
            self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())

    if getattr(self.config, "is_encoder_decoder", False) and getattr(self.config, "tie_encoder_decoder", False):
        if hasattr(self, self.base_model_prefix):
            self = getattr(self, self.base_model_prefix)
        self._tie_encoder_decoder_weights(self.encoder, self.decoder, self.base_model_prefix)

    for module in self.modules():
        if hasattr(module, "_tie_weights"):
            module._tie_weights()</code></pre>
<p>å¼€å¯model.evalã€‚</p>
<pre class="line-numbers language-none"><code class="language-none">model.eval()</code></pre>
<p>è‹¥æ¨¡å‹æ˜¯ç”Ÿæˆå¼æ¨¡å‹ï¼Œé‚£ä¹ˆè¿˜éœ€è¦é…ç½®ç”Ÿæˆçš„å‚æ•°ã€‚<code>GenerationConfig</code>å®é™…ä¸Šå°±æ˜¯model.generate()æ–¹æ³•ä¸­æ‰€è¦ç”¨çš„å‚æ•°ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if model.can_generate() and pretrained_model_name_or_path is not None:
    try:
        model.generation_config = GenerationConfig.from_pretrained(
            pretrained_model_name_or_path,
            cache_dir=cache_dir,
            force_download=force_download,
            resume_download=resume_download,
            proxies=proxies,
            local_files_only=local_files_only,
            token=token,
            revision=revision,
            subfolder=subfolder,
            _from_auto=from_auto_class,
            _from_pipeline=from_pipeline,
            **kwargs,
        )
    except OSError:
        logger.info(
            "Generation config file not found, using a generation config created from the model config."
        )
        pass</code></pre>
<p>æœ€ç»ˆè¾“å‡ºä¸€äº›åŠ è½½å‚æ•°æ—¶è¾“å‡ºçš„ä¿¡æ¯ï¼Œç„¶åè¿”å›æ¨¡å‹ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if output_loading_info:
    if loading_info is None:
        loading_info = {
            "missing_keys": missing_keys,
            "unexpected_keys": unexpected_keys,
            "mismatched_keys": mismatched_keys,
            "error_msgs": error_msgs,
        }
    return model, loading_info</code></pre>
<p>æ€»ç»“ï¼Œæ ¹æ®æœ€å¸¸ç”¨çš„æ–¹æ³•ï¼Œä¸»è¦æ˜¯åšä»¥ä¸‹å‡ ä¸ªæ“ä½œã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B",device_map="auto")</code></pre>
<ol type="1">
<li>æ ¹æ®è¾“å…¥è·¯å¾„æ‹¿åˆ°config.jsonï¼ŒåŠ è½½åˆ°Configã€‚</li>
<li>æ ¹æ®è¾“å…¥è·¯å¾„æ‹¿åˆ°æƒé‡æ–‡ä»¶pytorch_model.binï¼Œç”±torch.loadåŠ è½½æ¨¡å‹ç»“æ„å’Œæƒé‡å‚æ•°ã€‚ã€‚</li>
<li>å†³å®šæƒé‡çš„æ•°æ®ç±»å‹ï¼ŒæœªæŒ‡å®šåˆ™æ˜¯float32ã€‚</li>
<li>å¹³å‡åˆ†é…å‚æ•°ç»™å„å¼ å¡ã€‚</li>
<li>ç»‘å®šinputå’Œoutputçš„Embeddingï¼Œè®©å…¶ä½¿ç”¨åŒä¸€ä»½Embeddingå‚æ•°ã€‚</li>
<li>model.eval()ã€‚</li>
<li>è‹¥æ˜¯ç”Ÿæˆå¼æ¨¡å‹ï¼Œé…ç½®ç”Ÿæˆå‚æ•°ã€‚</li>
<li>è¿”å›æ¨¡å‹å®ä¾‹ã€‚</li>
</ol>
<p>æ‰€ä»¥ï¼Œå®é™…ä¸Šä¼šè°ƒç”¨ä¸¤ä¸ªä¸åŒçš„<code>from_pretrained</code>æ–¹æ³•ï¼Œç¬¬ä¸€ä¸ªæ˜¯AutoModelåŸºç±»_BaseAutoModelClassçš„ï¼Œåœ¨æœ€åè°ƒç”¨<code>get_model_class</code>æ–¹æ³•å¾—åˆ°æ¨¡å‹æœ¬èº«çš„ç±»ï¼Œæœ¬ä¾‹ä¸­æ˜¯<code>T5ForConditionalGeneration</code>ï¼Œç„¶åå†è°ƒç”¨è¿™ä¸ªç±»çš„<code>from_pretrained</code>ï¼Œè€Œè¿™ä¸ªç±»çš„<code>from_pretrained</code>åœ¨å…¶åŸºç±»<code>PreTrainedModel</code>å®ç°ï¼Œæ‰€ä»¥å†ä¼šè°ƒç”¨<code>PreTrainedModel</code>çš„<code>from_pretrained</code>æ–¹æ³•ã€‚åˆ†æå®Œæ¯•ã€‚</p>
<h1 id="åˆ†è¯">åˆ†è¯</h1>
<p>åç»­æ›´æ–°ï¼ŒæŒ–å‘</p>
<h1 id="ç”Ÿæˆ">ç”Ÿæˆ</h1>
<p>å¹¶ä¸æ˜¯æ¯ä¸€ä¸ªæ¨¡å‹éƒ½å¯ä»¥ä½¿ç”¨.generate()è¿›è¡Œåºåˆ—ç”Ÿæˆï¼Œéœ€è¦é€šè¿‡å‡½æ•°åˆ¤æ–­æ˜¯å¦èƒ½å¤Ÿè¿›è¡Œåºåˆ—ç”Ÿæˆä»»åŠ¡ï¼Œæ‰€ä»¥æ¯ä¸€ä¸ªæ¨¡å‹éƒ½éœ€è¦é‡å†™<code>prepare_inputs_for_generation</code>æ–¹æ³•ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin, PeftAdapterMixin):
	    @classmethod
    def can_generate(cls) -&gt; bool:
        """
        Returns whether this model can generate sequences with `.generate()`.

        Returns:
            `bool`: Whether this model can generate sequences with `.generate()`.
        """
        # Detects whether `prepare_inputs_for_generation` has been overwritten, which is a requirement for generation.
        # Alternativelly, the model can also have a custom `generate` function.
        if "GenerationMixin" in str(cls.prepare_inputs_for_generation) and "GenerationMixin" in str(cls.generate):
            return False
        return True</code></pre>
<p>è€Œgenerateæ–¹æ³•æœ¬èº«å°±åœ¨<code>GenerationMixin</code>ç±»ä¸­å®ç°ã€‚</p>
<pre class="line-numbers language-none"><code class="language-none">Class that holds a configuration for a generation task. A `generate` call supports the following generation methods
for text-decoder, text-to-text, speech-to-text, and vision-to-text models:

    - *greedy decoding* by calling [`~generation.GenerationMixin._greedy_search`] if `num_beams=1` and
        `do_sample=False`
    - *contrastive search* by calling [`~generation.GenerationMixin._contrastive_search`] if `penalty_alpha&gt;0.`
        and `top_k&gt;1`
    - *multinomial sampling* by calling [`~generation.GenerationMixin._sample`] if `num_beams=1` and
        `do_sample=True`
    - *beam-search decoding* by calling [`~generation.GenerationMixin._beam_search`] if `num_beams&gt;1` and
        `do_sample=False`
    - *beam-search multinomial sampling* by calling [`~generation.GenerationMixin._beam_sample`] if
        `num_beams&gt;1` and `do_sample=True`
    - *diverse beam-search decoding* by calling [`~generation.GenerationMixin._group_beam_search`], if
        `num_beams&gt;1` and `num_beam_groups&gt;1`
    - *constrained beam-search decoding* by calling [`~generation.GenerationMixin._constrained_beam_search`], if
        `constraints!=None` or `force_words_ids!=None`
    - *assisted decoding* by calling [`~generation.GenerationMixin._assisted_decoding`], if
        `assistant_model` or `prompt_lookup_num_tokens` is passed to `.generate()`

You do not need to call any of the above methods directly. Pass custom parameter values to '.generate()'. To learn
more about decoding strategies refer to the [text generation strategies guide](../generation_strategies).</code></pre>
<p>å°±ä¸çœ‹è¯¦ç»†çš„å®ç°ï¼Œå…ˆçœ‹<code>GenerationConfig</code>çš„ç”Ÿæˆç­–ç•¥ã€‚è‹¥ä¸ç”¨GenerationConfigï¼Œä¹Ÿå¯ä»¥ç›´æ¥è¾“å…¥kwargsã€‚</p>
<ul>
<li>greedy
decodingè´ªå©ªè§£ç ï¼Œ<code>num_beams=1</code>ä¸”<code>do_sample=False</code>ï¼Œä¼šä¸€ç›´é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„tokenï¼Œä¸€æ¡è·¯èµ°åˆ°é»‘ã€‚</li>
<li>Contrastive
searchå¯¹æ¯”æœç´¢ï¼Œåœ¨NIPS22è¢«æå‡ºï¼Œèƒ½åœ¨ä¿æŒæµç•…æ€§çš„å‰æä¸‹ï¼Œé¼“åŠ±å¤šæ ·æ€§ç”Ÿæˆï¼Œå‡å°‘é‡å¤è¾“å‡ºã€‚éœ€è¦<code>penalty_alpha&gt;0</code>
and
<code>top_k&gt;1</code>ã€‚ä¸€ä¸ªå€™é€‰tokenä¸å½“å‰tokenéå¸¸ç›¸ä¼¼(ç›¸ä¼¼åº¦å¾—åˆ†é«˜)ï¼Œé‚£ä¹ˆå®ƒçš„æ¦‚ç‡å°±ä¼šè¢«è¾ƒå¤šåœ°é™ä½ã€‚è¿™æ ·åšçš„ç›®çš„æ˜¯é¼“åŠ±ç”Ÿæˆæ›´åŠ å¤šæ ·åŒ–çš„æ–‡æœ¬ï¼Œé¿å…åŒç±»å‹çš„tokenè¿‡äºé›†ä¸­å‡ºç°ã€‚æœ€å,ç®—æ³•åœ¨ç»è¿‡è°ƒæ•´çš„
<code>scores</code>
å‘é‡ä¸Šå–Top-1ã€‚æ ¸å¿ƒå…¬å¼<code>scores = (1.0 - alpha) * next_top_k_probs - alpha * scores</code>ã€‚<code>next_top_k_probs</code>æ˜¯å½“å‰tokençš„Top-kæ¦‚ç‡åˆ†å¸ƒï¼Œç­‰å¼å³è¾¹çš„<code>scores</code>æ˜¯å½“å‰tokenå’Œä¸‹ä¸€ä¸ªtokenä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°ã€‚æ‰€ä»¥å½“å‰tokenä¸next
tokenè¶Šç›¸ä¼¼ï¼Œæƒ©ç½šå°±è¶Šå¤§ã€‚</li>
<li>multinomial samplingï¼Œ<code>num_beams=1</code>
and<code>do_sample=True</code>ã€‚å’Œè´ªå©ªè§£ç çš„åŒºåˆ«ä¸ä¸€å®šé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„tokenï¼Œè€Œæ˜¯æ ¹æ®æ¦‚ç‡åˆ†å¸ƒæ¥é‡‡æ ·ã€‚</li>
<li>beam-searchï¼Œ<code>num_beams&gt;1</code>
and<code>do_sample=False</code>ï¼Œä¿ç•™top-kä¸ªå¾—åˆ†æœ€é«˜çš„å€™é€‰åºåˆ—ï¼Œç§°ä¸º"beam"ã€‚è¿™é‡Œé€‰æ‹©ä¸é‡‡æ ·ï¼Œæ˜¯é€‰æ‹©å¾—åˆ†æœ€é«˜çš„2*num_beamsä¸ªtokenã€‚</li>
<li>diverse beam-searchï¼Œ<code>num_beams&gt;1</code> and
<code>num_beam_groups&gt;1</code>ï¼Œé€šè¿‡åˆ†ç»„æœºåˆ¶ï¼Œç¡®ä¿äº†ä¸åŒbeamä¹‹é—´çš„å·®å¼‚æ€§ã€‚</li>
</ul>
<p>æ¥ä¸‹æ¥ä»‹ç»ä¸€äº›æ¯”è¾ƒå¸¸ç”¨çš„å‚æ•°ã€‚</p>
<ul>
<li><code>do_sample</code>ï¼Œæ˜¯å¦æ ¹æ®æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·ã€‚</li>
<li><code>temperature</code>ï¼Œé»˜è®¤1.0ã€‚å°äº1æ—¶ï¼Œå½“
<code>temperature &lt; 1.0</code> æ—¶,
ç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒä¼šè¢«"å¹³æ»‘"(å³°å€¼å˜å¾—æ›´é™¡å³­)ï¼Œä½¿å¾—æ¨¡å‹æ›´å€¾å‘äºé€‰æ‹©æ¦‚ç‡è¾ƒé«˜çš„tokenï¼Œç”Ÿæˆçš„æ–‡æœ¬ä¼šæ›´åŠ é›†ä¸­å’Œä¿å®ˆã€‚å½“<code>temperature &gt; 1.0</code>æ—¶ï¼Œ
ç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒä¼šè¢«"æ‹‰å¹³"(å³°å€¼å˜å¾—æ›´å¹³ç¼“)ï¼Œä½¿å¾—æ¨¡å‹ä¼šé€‰æ‹©æ¦‚ç‡è¾ƒä½çš„tokenï¼Œç”Ÿæˆçš„æ–‡æœ¬ä¼šæ›´åŠ å¤šæ ·å’Œæ¢ç´¢æ€§ã€‚</li>
<li><code>top_k</code>ï¼Œé€‰æ‹©ä¸‹ä¸€ä¸ªtokenæ—¶ï¼Œåªä¿ç•™æ¦‚ç‡æœ€é«˜çš„å‰
<code>top_k</code>
ä¸ªtokenï¼Œæœ‰æ•ˆåœ°é¿å…æ¨¡å‹é€‰æ‹©æ¦‚ç‡å¾ˆä½çš„ä¸åˆç†tokenã€‚</li>
<li><code>top_p</code>ï¼ŒåŠ¨æ€åœ°é€‰æ‹©æ¦‚ç‡æ€»å’Œè¾¾åˆ° top_p
é˜ˆå€¼çš„æœ€å°tokené›†åˆã€‚</li>
<li><code>num_return_sequences</code>
ï¼ŒæŒ‡å®šè¦ç”Ÿæˆçš„ç‹¬ç«‹åºåˆ—æ•°é‡ã€‚é»˜è®¤ä¸º1ï¼Œå³åªç”Ÿæˆ1ä¸ªåºåˆ—ã€‚</li>
<li><code>output_scores</code>æ˜¯å¦è¾“å‡ºæ¯ä¸ªtokençš„é¢„æµ‹åˆ†æ•°ã€‚</li>
<li><code>output_logits</code>æ˜¯å¦è¾“å‡ºæœªç»å¤„ç†çš„åŸå§‹é¢„æµ‹logitsã€‚</li>
<li><code>pad_token_id,bos_token_id,eos_token_id</code>ï¼Œéœ€è¦æ ¹æ®æ¨¡å‹çš„è¯è¡¨æ¥çœ‹ã€‚ä¸è®¾ç½®åˆ™ä¸ºNoneã€‚</li>
<li><code>max_length</code>:
æœ€å¤§è¾“å‡ºé•¿åº¦,åŒ…æ‹¬promptå’Œç”Ÿæˆçš„æ–°tokensã€‚é»˜è®¤æ˜¯20</li>
<li><code>max_new_tokens</code>:
æœ€å¤§ç”Ÿæˆæ–°tokensæ•°é‡,ä¸åŒ…æ‹¬prompté•¿åº¦ã€‚</li>
<li><code>min_length</code>:
æœ€å°è¾“å‡ºé•¿åº¦,åŒ…æ‹¬promptå’Œç”Ÿæˆçš„æ–°tokensã€‚</li>
<li><code>min_new_tokens</code>:
æœ€å°ç”Ÿæˆæ–°tokensæ•°é‡,ä¸åŒ…æ‹¬prompté•¿åº¦ã€‚</li>
<li><code>early_stopping</code>: æ§åˆ¶beam searchåœæ­¢çš„æ¡ä»¶ã€‚å¯é€‰å€¼ä¸º:
<ul>
<li><code>True</code>: å½“ç”Ÿæˆäº† <code>num_beams</code>
ä¸ªå®Œæ•´å€™é€‰åºåˆ—æ—¶ç«‹å³åœæ­¢ã€‚</li>
<li><code>False</code>:
æ ¹æ®å¯å‘å¼åœæ­¢,å³å½“å¾ˆéš¾æ‰¾åˆ°æ›´å¥½çš„å€™é€‰æ—¶åœæ­¢ã€‚</li>
<li><code>"never"</code>: ä¸€ç›´è¿è¡Œç›´åˆ°æ— æ³•æ‰¾åˆ°æ›´å¥½çš„å€™é€‰ä¸ºæ­¢ã€‚</li>
</ul></li>
</ul>
<p>æ¥ä¸‹æ¥è§£ægenerateå‡½æ•°ä¸»è¦åšäº†å“ªäº›ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">@torch.no_grad()
def generate(
    self,
    inputs: Optional[torch.Tensor] = None,
    generation_config: Optional[GenerationConfig] = None,
    logits_processor: Optional[LogitsProcessorList] = None,
    stopping_criteria: Optional[StoppingCriteriaList] = None,
    prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,
    synced_gpus: Optional[bool] = None,
    assistant_model: Optional["PreTrainedModel"] = None,
    streamer: Optional["BaseStreamer"] = None,
    negative_prompt_ids: Optional[torch.Tensor] = None,
    negative_prompt_attention_mask: Optional[torch.Tensor] = None,
    **kwargs,
) -&gt; Union[GenerateOutput, torch.LongTensor]:</code></pre>
<ul>
<li>inputsï¼Œä¸€èˆ¬æ˜¯ç»è¿‡tokenizerå¤„ç†çš„åºåˆ—ï¼ŒåŒ…å«attention_maskçš„ã€‚å¦‚æœæ˜¯è°ƒç”¨tokenizer.encode()ï¼Œé‚£ä¹ˆä¸ä¼šæœ‰attention_maskã€‚</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call
"å¤„ç† `generation_config` å’Œå¯èƒ½æ›´æ–°å®ƒçš„ `kwargs`ï¼Œå¹¶éªŒè¯ `.generate()` çš„è°ƒç”¨ï¼Œç•¥"

# 2. Set generation parameters if not already defined
"""
ç•¥ï¼Œè®¾ç½®ç”Ÿæˆæ‰€éœ€çš„ä¸€äº›é»˜è®¤å‚æ•°
"""
if generation_config.pad_token_id is None and generation_config.eos_token_id is not None:
    if model_kwargs.get("attention_mask", None) is None:
        logger.warning(
            "The attention mask and the pad token id were not set. As a consequence, you may observe "
            "unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results."
        )
    eos_token_id = generation_config.eos_token_id
    # å¤šè¯­è¨€æ¨¡å‹çš„eoså¯èƒ½ä¼šæœ‰å¤šä¸ª
    if isinstance(eos_token_id, list):
        eos_token_id = eos_token_id[0]
    logger.warning(f"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.")
    generation_config.pad_token_id = eos_token_id</code></pre>
<p>è¿™ä¸€æ®µè¯´æ˜ç¡®è¯´æ˜å¦‚æœä½ æ²¡æœ‰ä¼ å…¥<code>pad_token_id</code>ï¼Œé‚£ä¹ˆä¼šä»¥<code>eos_token_id</code>æ›¿ä»£ã€‚è‹¥ä½ æ²¡æœ‰ä¼ å…¥<code>attention_mask</code>ï¼Œä¼šè­¦å‘Šä½ ä¼ å…¥ï¼ŒAttention
Maskä¸­å€¼ä¸º0çš„ä½ç½®å¯¹åº”çš„Attentionæƒé‡è®¾ä¸ºéå¸¸å°çš„è´Ÿå€¼ï¼Œé€šå¸¸æ˜¯-1e9ã€‚</p>
<p>æ¥ä¸‹æ¥å°±æ˜¯å¤„ç†æ¨¡å‹çš„è¾“å…¥ã€‚è·å–è¾“å…¥çš„tensorå’Œbatch_sizeã€‚<code>_prepare_model_inputs</code>æ–¹æ³•è¿‡æ»¤æ‰
model_kwargsä¸­éç©ºä¸”ä¸æ˜¯æ¨¡å‹ä¸»è¦è¾“å…¥çš„å‚æ•°ã€‚å¯¹äºæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œè¦çœ‹æ¨¡å‹çš„encoderæ˜¯å¦æ”¯æŒç›´æ¥è¾“å…¥embeddingï¼Œå¦åˆ™ä¸€å¾‹è®¾ç½®æˆinput_idsã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 3. Define model inputs
# inputs_tensor has to be defined
# model_input_name is defined if model-specific keyword input is passed
# otherwise model_input_name is None
# all model-specific keyword inputs are removed from `model_kwargs`
inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(
    inputs, generation_config.bos_token_id, model_kwargs
)
batch_size = inputs_tensor.shape[0]</code></pre>
<p>decode-onlyçš„æ¨¡å‹åº”è¯¥ä½¿ç”¨å·¦å¯¹é½ã€‚ä½¿ç”¨å³å¯¹é½ä¼šè­¦å‘Šï¼Œåˆå§‹åŒ– tokenizer
æ—¶è®¾ç½®
<code>padding_side='left'</code>ä»¥ç¡®ä¿æ­£ç¡®çš„ç”Ÿæˆç»“æœã€‚æ¥ä¸‹æ¥çš„é€»è¾‘éƒ½ä¸çœ‹äº†ï¼Œæ— éå°±æ˜¯å¤„ç†ä¸€äº›æ¨¡å‹ç”Ÿæˆå‚æ•°ï¼Œå¦‚max_lengthã€‚æ ¹æ®ä¸åŒçš„ç”Ÿæˆç­–ç•¥ï¼Œä¼šè¿è¡Œä¸åŒçš„ç”Ÿæˆå‡½æ•°ï¼Œå°±çœ‹ä¸€ä¸ªç®€å•çš„è´ªå©ªè§£ç çš„éƒ¨åˆ†ä»£ç ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">        if generation_mode == GenerationMode.GREEDY_SEARCH:
            # 11. run greedy search
            result = self._greedy_search(
                input_ids,
                logits_processor=prepared_logits_processor, # logitså¤„ç†å™¨ï¼Œmin_lengthä½œç”¨äºè¿™ä¸ªï¼Œåœ¨æ»¡è¶³å‰å‡å°eosçš„æ¦‚ç‡ã€‚
                stopping_criteria=prepared_stopping_criteria, # åœæ­¢åˆ¤å®šå™¨ï¼Œmax_lengthå°±ä½œç”¨äºè¿™ä¸ª
                pad_token_id=generation_config.pad_token_id,
                eos_token_id=generation_config.eos_token_id,
                output_attentions = generation_config.output_attentionsï¼Œ# æ˜¯å¦è¾“å‡ºæ³¨æ„åŠ›å±‚åˆ†æ•°
                output_hidden_states = generation_config.output_hidden_states # æ˜¯å¦è¿”å›éšè—çŠ¶æ€
                output_scores=generation_config.output_scores,
                output_logits=generation_config.output_logits,
                return_dict_in_generate=generation_config.return_dict_in_generate,
                synced_gpus=synced_gpus,
                streamer=streamer,
                **model_kwargs,
            )
-----------------------------------------------------------------------------------

    </code></pre>
<p>é¦–å…ˆæ‹¿åˆ°å…¨éƒ¨çš„è¾“å‡ºï¼Œå¹¶åªéœ€è¦ä¸‹ä¸€ä¸ªtokençš„å†…å®¹ã€‚<code>next_tokens</code>åœ¨åºåˆ—ç»“æŸçš„æƒ…å†µä¸‹ï¼Œä¸€å®šæ˜¯pad_idã€‚ç”Ÿæˆåæ›´æ–°<code>input_ids</code>ï¼Œè‹¥ç”Ÿæˆäº†eos_idï¼Œå°±è®¤ä¸ºåºåˆ—å·²ç»å®Œæˆã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):            
  		outputs = self(
              **model_inputs,
              return_dict=True,
              output_attentions=output_attentions,
              output_hidden_states=output_hidden_states,
          )
          next_token_logits = outputs.logits[:, -1, :] # æœ€åä¸€ä¸ªæ—¶é—´æ­¥
          next_tokens_scores = logits_processor(input_ids, next_token_logits)
          next_tokens = torch.argmax(next_tokens_scores, dim=-1) # æœ€å¤§å€¼ç´¢å¼•ï¼Œè´ªå©ªç­–ç•¥
          # finished sentences should have their next token be a padding token
          if eos_token_id is not None:
              if pad_token_id is None:
                  raise ValueError("If `eos_token_id` is defined, make sure that `pad_token_id` is defined.")
                 	# unfinished_sequencesåˆå§‹åŒ–æ˜¯torch.ones(batch_size,dtype = torch.long)                   
              next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)
          # update generated ids, model inputs, and length for next step
          input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
          # if eos_token was found in one sentence, set sentence to finished
          if eos_token_id_tensor is not None:
              unfinished_sequences = unfinished_sequences.mul(
                  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)
              )</code></pre>
<p>æœ€ågenerateæ–¹æ³•è¿”å›ä¸€ä¸ªUnion[GenerateOutput,torch.LongTensor]ã€‚ä¸€èˆ¬æ¥è¯´æ˜¯å‰è€…ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">GenerateNonBeamOutput = Union[GenerateDecoderOnlyOutput, GenerateEncoderDecoderOutput]
GenerateBeamOutput = Union[GenerateBeamDecoderOnlyOutput, GenerateBeamEncoderDecoderOutput]
GenerateOutput = Union[GenerateNonBeamOutput, GenerateBeamOutput]</code></pre>
<p>æˆ‘ä»¬å°±æ‹¿<code>GenerateBeamDecoderOnlyOutput</code>æ¥çœ‹ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">sequences: torch.LongTensor = None # è¿”å›çš„åºåˆ—ï¼Œéœ€è¦è¿›è¡Œdecodeï¼Œä¸€èˆ¬åªç”¨è¿™ä¸ª
sequences_scores: Optional[torch.FloatTensor] = None # åºåˆ—beam_searchçš„åˆ†æ•°
scores: Optional[Tuple[torch.FloatTensor]] = None
logits: Optional[Tuple[torch.FloatTensor]] = None
beam_indices: Optional[torch.LongTensor] = None
attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None</code></pre>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>æœ¬æ–‡ä½œè€…ï¼š</strong>iroha</li><li class="post-copyright-link"><strong>æœ¬æ–‡é“¾æ¥ï¼š</strong><a href="http://example.com/post/model_load.html" title="Huggingfaceçš„æ¨¡å‹åŠ è½½æµç¨‹">http://example.com/post/model_load.html</a></li><li class="post-copyright-license"><strong>ç‰ˆæƒå£°æ˜ï¼š</strong>æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é»˜è®¤é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> è®¸å¯åè®®ã€‚</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/post/rl_surface.html" rel="prev" title="DRL introduce"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">DRL introduce</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/post/ft_survey.html" rel="next" title="å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ç»¼è¿°"><span class="post-nav-text">å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ç»¼è¿°</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 â€“ 2024 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> iroha</span></div><div class="powered"><span>ç”± <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> é©±åŠ¨ v6.3.0</span><span class="footer-separator">|</span><span>ä¸»é¢˜ - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.9</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>