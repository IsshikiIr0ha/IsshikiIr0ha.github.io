<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="iroha"><meta name="copyright" content="iroha"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>Huggingfaceçš„æ¨¡å‹åŠ è½½æµç¨‹ | iroha</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...undefined?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"example.com","root":"/","title":"ä¸æƒ³æ‘†çƒ‚","version":"1.10.11","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"æœç´¢...","empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹: ${query}","hits":"æ‰¾åˆ° ${hits} æ¡ç»“æœ","hits_time":"æ‰¾åˆ° ${hits} æ¡ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js" defer></script><script src="/js/load-aplayer.js" defer></script><meta name="description" content="æˆ‘ä»¬ä»¥ä¸‹é¢çš„è¿™ä¸€å¥è¯­å¥ä½œä¸ºå¼€å§‹ï¼Œä»¥ä»æœ¬åœ°åŠ è½½æ¨¡å‹ä¸ºä¾‹ã€‚ model &#x3D; AutoModelForSeq2SeqLM.from_pretrained(&quot;bigscience&#x2F;T0_3B&quot;) inputs &#x3D; tokenizer.encode(q.strip()+&quot; ? To answer this question, we need to know&quot;, return_tensors&#x3D;&quot;pt&quot;) o">
<meta property="og:type" content="article">
<meta property="og:title" content="Huggingfaceçš„æ¨¡å‹åŠ è½½æµç¨‹">
<meta property="og:url" content="http://example.com/post/model_load.html">
<meta property="og:site_name" content="iroha">
<meta property="og:description" content="æˆ‘ä»¬ä»¥ä¸‹é¢çš„è¿™ä¸€å¥è¯­å¥ä½œä¸ºå¼€å§‹ï¼Œä»¥ä»æœ¬åœ°åŠ è½½æ¨¡å‹ä¸ºä¾‹ã€‚ model &#x3D; AutoModelForSeq2SeqLM.from_pretrained(&quot;bigscience&#x2F;T0_3B&quot;) inputs &#x3D; tokenizer.encode(q.strip()+&quot; ? To answer this question, we need to know&quot;, return_tensors&#x3D;&quot;pt&quot;) o">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20240422014943359.png">
<meta property="article:published_time" content="2024-04-11T08:00:00.000Z">
<meta property="article:modified_time" content="2024-07-22T09:14:59.982Z">
<meta property="article:author" content="iroha">
<meta property="article:tag" content="å¤§æ¨¡å‹">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20240422014943359.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="æ–‡ç« ç›®å½•"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="ç«™ç‚¹æ¦‚è§ˆ"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="iroha"><img width="96" loading="lazy" src="/yun.png" alt="iroha"><span class="site-author-status" title="æ°¸è¿œç›¸ä¿¡ç¾å¥½çš„äº‹æƒ…å³å°†å‘ç”Ÿ">ğŸ˜Š</span></a><div class="site-author-name"><a href="/about/">iroha</a></div><span class="site-name">iroha</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="é¦–é¡µ"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="å½’æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">16</span></a></div><div class="site-state-item"><a href="/categories/" title="åˆ†ç±»"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">5</span></a></div><div class="site-state-item"><a href="/tags/" title="æ ‡ç­¾"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">2</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="æ–‡æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="å‹é“¾" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.</span> <span class="toc-text">åŠ è½½</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E8%AF%8D"><span class="toc-number">2.</span> <span class="toc-text">åˆ†è¯</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%9F%E6%88%90"><span class="toc-number">3.</span> <span class="toc-text">ç”Ÿæˆ</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://example.com/post/model_load.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="iroha"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="iroha"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Huggingfaceçš„æ¨¡å‹åŠ è½½æµç¨‹</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="åˆ›å»ºæ—¶é—´ï¼š2024-04-11 16:00:00" itemprop="dateCreated datePublished" datetime="2024-04-11T16:00:00+08:00">2024-04-11</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="ä¿®æ”¹æ—¶é—´ï¼š2024-07-22 17:14:59" itemprop="dateModified" datetime="2024-07-22T17:14:59+08:00">2024-07-22</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">NLP</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">å¤§æ¨¡å‹</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>æˆ‘ä»¬ä»¥ä¸‹é¢çš„è¿™ä¸€å¥è¯­å¥ä½œä¸ºå¼€å§‹ï¼Œä»¥ä»æœ¬åœ°åŠ è½½æ¨¡å‹ä¸ºä¾‹ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> AutoModelForSeq2SeqLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"bigscience/T0_3B"</span><span class="token punctuation">)</span>
inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>q<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">" ? To answer this question, we need to know"</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>
outputs <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>inputs<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> max_new_tokens<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> do_sample<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> top_k<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h1 id="åŠ è½½">åŠ è½½</h1>
<p>AutoModelForSeq2SeqLMç»§æ‰¿äº†_BaseAutoModelClassï¼Œè¿™ä¸ªç±»æ˜¯æ‰€æœ‰AutoModelçš„åŸºç±»ï¼Œä¿å­˜åœ¨transformers/models/auto/auto_factory.pyä¸­ã€‚è°ƒç”¨çš„from_pretrainedæ–¹æ³•å®é™…ä¸Šå°±æ¥è‡ªäºè¿™ä¸ªåŸºç±»ã€‚æˆ‘ä»¬å‡è®¾æ¨¡å‹ä¿å­˜åœ¨æœ¬åœ°ï¼Œä¸€äº›ä¸‹è½½çš„é€»è¾‘ä¸çœ‹ï¼Œä¸”kwargså’Œconfigä¸ºNoneã€‚æœ€ç»ˆä¼šå¾—åˆ°æ¨¡å‹çš„å“ˆå¸Œå€¼ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>config<span class="token punctuation">,</span> PretrainedConfig<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># We make a call to the config file first (which may be absent) to get the commit hash as soon as possible</span>
    resolved_config_file <span class="token operator">=</span> cached_file<span class="token punctuation">(</span>
        pretrained_model_name_or_path<span class="token punctuation">,</span>
        CONFIG_NAME<span class="token punctuation">,</span>
        _raise_exceptions_for_gated_repo<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        _raise_exceptions_for_missing_entries<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        _raise_exceptions_for_connection_errors<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        <span class="token operator">**</span>hub_kwargs<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    commit_hash <span class="token operator">=</span> extract_commit_hash<span class="token punctuation">(</span>resolved_config_file<span class="token punctuation">,</span> commit_hash<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>é¦–å…ˆéœ€è¦åŠ è½½configï¼Œé€šè¿‡cached_fileæ¥åŠ è½½ã€‚CONFIG_NAMEé»˜è®¤ä¸ºconfig.jsonï¼Œpretrained_model_name_or_pathåˆ™æ˜¯from_pretrainedä¼ å…¥çš„å­—ç¬¦ä¸²ã€‚å†çœ‹å…·ä½“å®ç°ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">path_or_repo_id <span class="token operator">=</span> <span class="token builtin">str</span><span class="token punctuation">(</span>path_or_repo_id<span class="token punctuation">)</span> <span class="token comment"># "bigscience/T0_3B"</span>
    full_filename <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>subfolder<span class="token punctuation">,</span> filename<span class="token punctuation">)</span> <span class="token comment"># filename=config.json</span>
    <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span>path_or_repo_id<span class="token punctuation">)</span><span class="token punctuation">:</span>
        resolved_file <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>path_or_repo_id<span class="token punctuation">,</span> subfolder<span class="token punctuation">)</span><span class="token punctuation">,</span> filename<span class="token punctuation">)</span> <span class="token comment"># subfolderä¸æŒ‡å®š=None</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isfile<span class="token punctuation">(</span>resolved_file<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> _raise_exceptions_for_missing_entries<span class="token punctuation">:</span>
                <span class="token keyword">raise</span> EnvironmentError<span class="token punctuation">(</span>
                    <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>path_or_repo_id<span class="token punctuation">&#125;</span></span><span class="token string"> does not appear to have a file named </span><span class="token interpolation"><span class="token punctuation">&#123;</span>full_filename<span class="token punctuation">&#125;</span></span><span class="token string">. Checkout "</span></span>
                    <span class="token string-interpolation"><span class="token string">f"'https://huggingface.co/</span><span class="token interpolation"><span class="token punctuation">&#123;</span>path_or_repo_id<span class="token punctuation">&#125;</span></span><span class="token string">/tree/</span><span class="token interpolation"><span class="token punctuation">&#123;</span>revision<span class="token punctuation">&#125;</span></span><span class="token string">' for available files."</span></span>
                <span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token keyword">return</span> <span class="token boolean">None</span>
        <span class="token keyword">return</span> resolved_file <span class="token comment"># è¿”å›bigscience/T0_3B/config.json</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>æ­¤æ—¶å¯¹åº”çš„config.jsonå·²ç»è¢«åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œä¹‹åéœ€è¦åŠ è½½åˆ°AutoConfigä¸­ã€‚å¯ä»¥çœ‹åˆ°å°±æ˜¯T0_3B/config.json</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">            config<span class="token punctuation">,</span> kwargs <span class="token operator">=</span> AutoConfig<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
                pretrained_model_name_or_path<span class="token punctuation">,</span>
                return_unused_kwargs<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                trust_remote_code<span class="token operator">=</span>trust_remote_code<span class="token punctuation">,</span>
                code_revision<span class="token operator">=</span>code_revision<span class="token punctuation">,</span>
                _commit_hash<span class="token operator">=</span>commit_hash<span class="token punctuation">,</span>
                <span class="token operator">**</span>hub_kwargs<span class="token punctuation">,</span>
                <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>config<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
T5Config <span class="token punctuation">&#123;</span>
  <span class="token string">"_name_or_path"</span><span class="token punctuation">:</span> <span class="token string">"/data2/wtf/model/bigscience/T0_3B"</span><span class="token punctuation">,</span>
  <span class="token string">"architectures"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>
    <span class="token string">"T5ForConditionalGeneration"</span>
  <span class="token punctuation">]</span><span class="token punctuation">,</span>
  <span class="token string">"classifier_dropout"</span><span class="token punctuation">:</span> <span class="token number">0.0</span><span class="token punctuation">,</span>
  <span class="token string">"d_ff"</span><span class="token punctuation">:</span> <span class="token number">5120</span><span class="token punctuation">,</span>
  <span class="token string">"d_kv"</span><span class="token punctuation">:</span> <span class="token number">64</span><span class="token punctuation">,</span>
  <span class="token string">"d_model"</span><span class="token punctuation">:</span> <span class="token number">2048</span><span class="token punctuation">,</span>
  <span class="token string">"decoder_start_token_id"</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
  <span class="token string">"dense_act_fn"</span><span class="token punctuation">:</span> <span class="token string">"gelu_new"</span><span class="token punctuation">,</span>
  <span class="token string">"dropout_rate"</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span>
  <span class="token string">"eos_token_id"</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
  <span class="token string">"feed_forward_proj"</span><span class="token punctuation">:</span> <span class="token string">"gated-gelu"</span><span class="token punctuation">,</span>
  <span class="token string">"gradient_checkpointing"</span><span class="token punctuation">:</span> false<span class="token punctuation">,</span>
  <span class="token string">"initializer_factor"</span><span class="token punctuation">:</span> <span class="token number">1.0</span><span class="token punctuation">,</span>
  <span class="token string">"is_encoder_decoder"</span><span class="token punctuation">:</span> true<span class="token punctuation">,</span>
  <span class="token string">"is_gated_act"</span><span class="token punctuation">:</span> true<span class="token punctuation">,</span>
  <span class="token string">"layer_norm_epsilon"</span><span class="token punctuation">:</span> <span class="token number">1e-06</span><span class="token punctuation">,</span>
  <span class="token string">"model_type"</span><span class="token punctuation">:</span> <span class="token string">"t5"</span><span class="token punctuation">,</span>
  <span class="token string">"num_decoder_layers"</span><span class="token punctuation">:</span> <span class="token number">24</span><span class="token punctuation">,</span>
  <span class="token string">"num_heads"</span><span class="token punctuation">:</span> <span class="token number">32</span><span class="token punctuation">,</span>
  <span class="token string">"num_layers"</span><span class="token punctuation">:</span> <span class="token number">24</span><span class="token punctuation">,</span>
  <span class="token string">"output_past"</span><span class="token punctuation">:</span> true<span class="token punctuation">,</span>
  <span class="token string">"pad_token_id"</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
  <span class="token string">"relative_attention_max_distance"</span><span class="token punctuation">:</span> <span class="token number">128</span><span class="token punctuation">,</span>
  <span class="token string">"relative_attention_num_buckets"</span><span class="token punctuation">:</span> <span class="token number">32</span><span class="token punctuation">,</span>
  <span class="token string">"tie_word_embeddings"</span><span class="token punctuation">:</span> false<span class="token punctuation">,</span>
  <span class="token string">"transformers_version"</span><span class="token punctuation">:</span> <span class="token string">"4.38.2"</span><span class="token punctuation">,</span>
  <span class="token string">"use_cache"</span><span class="token punctuation">:</span> true<span class="token punctuation">,</span>
  <span class="token string">"vocab_size"</span><span class="token punctuation">:</span> <span class="token number">32128</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>æœ€ç»ˆå°†æ¨¡å‹åŠ è½½ï¼Œè¿”å›modelå®ä¾‹ã€‚model_classå°±æ˜¯jsonä¸­çš„architectureså¯¹åº”å€¼ã€‚_get_model_classæ–¹æ³•å°±æ˜¯å¾—åˆ°æ¨¡å‹çš„ç±»å‹ï¼</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">          model_class <span class="token operator">=</span> _get_model_class<span class="token punctuation">(</span>config<span class="token punctuation">,</span> cls<span class="token punctuation">.</span>_model_mapping<span class="token punctuation">)</span> <span class="token comment"># T5ForConditionalGeneration</span>
  		<span class="token triple-quoted-string string">"""
  		"architectures": [
  "T5ForConditionalGeneration"
]
			å°±æ˜¯è¿”å›archä¸­çš„æ¨¡å‹ç±»å‹
  		"""</span>
          <span class="token keyword">return</span> model_class<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
              pretrained_model_name_or_path<span class="token punctuation">,</span> <span class="token operator">*</span>model_args<span class="token punctuation">,</span> config<span class="token operator">=</span>config<span class="token punctuation">,</span> <span class="token operator">**</span>hub_kwargs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs
          <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>ä¸Šé¢çš„<code>cls._model_mapping</code>å°±æ˜¯æ¨¡å‹æ ¹æ®ä½ çš„è¾“å…¥ï¼Œå¾—åˆ°å½“å‰æ¨¡å‹ç±»å‹çš„æ˜ å°„ã€‚</p>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">class AutoModelForSeq2SeqLM(_BaseAutoModelClass):
    _model_mapping &#x3D; MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING
------
MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING &#x3D; _LazyAutoMapping(
    CONFIG_MAPPING_NAMES, MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES
)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>CONFIG_MAPPING_NAMES</code>æ˜¯æ ¹æ®ä½ ä¼ å…¥çš„è·¯å¾„æ¥åŒ¹é…ï¼Œè€Œ<code>MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES</code>åŒç†ï¼Œä¹Ÿæœ‰å’Œä¸‹å›¾ç±»ä¼¼çš„å­—å…¸ã€‚æœ¬ä¾‹ä¸­<code>CONFIG_MAPPING_NAMES="T5Config"</code>ï¼Œ<code>MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES=T5ForConditionalGeneration</code>ï¼Œå’Œconfig.jsonä¸­çš„ä¸€æ ·ã€‚</p>
<figure>
<img src="../images/image-20240422014943359.png"
alt="image-20240422014943359" />
<figcaption aria-hidden="true">image-20240422014943359</figcaption>
</figure>
<p><code>model_class.from_pretrained</code>ä¸­çš„<code>from_pretrained</code>æ¥è‡ªäºPretrainedModelç±»ï¼Œè¿™æ˜¯æ‰€æœ‰æ¨¡å‹çš„åŸºç±»(æ³¨æ„ä¸æ˜¯AutoModel)ã€‚è¿™ä¸ªå‡½æ•°æ˜¯æœ¬ç¯‡çš„æ ¸å¿ƒã€‚è¿”å›çš„æ¨¡å‹å®ä¾‹é»˜è®¤æ˜¯å¼€å¯model.eval()æ¨¡å¼ï¼Œè‹¥è¦å¾®è°ƒorè®­ç»ƒæ¨¡å‹ï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®šmodel.train()ã€‚</p>
<p>è¿™é‡Œé¡ºä¾¿æä¸€å˜´model.trainå’Œevalä¸‹çš„åŒºåˆ«ï¼š</p>
<ol type="1">
<li>Dropout å’Œ BatchNorm è¡Œä¸ºã€‚model.train()ä¸‹ Dropout
å±‚ä¼šéšæœºä¸¢å¼ƒä¸€éƒ¨åˆ†ç¥ç»å…ƒ, BatchNorm å±‚ä¼šè®¡ç®—å½“å‰ batch çš„ç»Ÿè®¡é‡ã€‚
model.eval()ä¸‹Dropout å±‚ä¼šå…¨éƒ¨ä¿ç•™ç¥ç»å…ƒ, BatchNorm
å±‚ä¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ»‘åŠ¨å¹³å‡ç»Ÿè®¡é‡ã€‚</li>
<li>æ¢¯åº¦ä¸ä¼˜åŒ–å™¨ã€‚model.train()ä¼šè®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°æ¨¡å‹å‚æ•°ã€‚model.eval()ä¸ä¼šè®¡ç®—æ¢¯åº¦,
ä¹Ÿä¸ä¼šæ›´æ–°æ¨¡å‹å‚æ•°ã€‚</li>
<li>æ•°æ®å¢å¼ºã€‚model.train()é€šå¸¸ä¼šåº”ç”¨ä¸€äº›æ•°æ®å¢å¼ºæŠ€æœ¯,
å¦‚ç¿»è½¬ã€æ—‹è½¬ç­‰ã€‚model.eval()ä¸€èˆ¬ä¸éœ€è¦æ•°æ®å¢å¼º,
ç›´æ¥ä½¿ç”¨åŸå§‹çš„è¾“å…¥æ•°æ®ã€‚</li>
<li>å†…å­˜ä¸è®¡ç®—å¼€é”€ã€‚model.train()éœ€è¦ä¿å­˜ä¸­é—´æ¿€æ´»å€¼ç”¨äºåå‘ä¼ æ’­,
è®¡ç®—å¼€é”€ç›¸å¯¹æ›´å¤§ã€‚model.eval()åªéœ€è¦å‰å‘ä¼ æ’­, ä¸éœ€è¦ä¿å­˜ä¸­é—´æ¿€æ´»å€¼,
è®¡ç®—å¼€é”€ç›¸å¯¹æ›´å°ã€‚</li>
</ol>
<p>ä¸‹é¢çœ‹å‡ ä¸ªæ¯”è¾ƒå…³é”®çš„å‚æ•°ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token decorator annotation punctuation">@classmethod</span>
<span class="token keyword">def</span> <span class="token function">from_pretrained</span><span class="token punctuation">(</span>
    cls<span class="token punctuation">,</span>
    pretrained_model_name_or_path<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> os<span class="token punctuation">.</span>PathLike<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token operator">*</span>model_args<span class="token punctuation">,</span>
    config<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span>PretrainedConfig<span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">,</span> os<span class="token punctuation">.</span>PathLike<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    cache_dir<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> os<span class="token punctuation">.</span>PathLike<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    ignore_mismatched_sizes<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    force_download<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    local_files_only<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    token<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> <span class="token builtin">bool</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    revision<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">"main"</span><span class="token punctuation">,</span>
    use_safetensors<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
<span class="token punctuation">)</span><span class="token punctuation">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><p>pretrained_model_name_or_pathï¼Œæ¨¡å‹çš„è·¯å¾„oråœ¨huggingfaceä¸­çš„åå­—ã€‚</p></li>
<li><p>force_downloadï¼Œä¸è®ºæœ‰æ²¡æœ‰ä¸‹è½½å¥½æ¨¡å‹ï¼Œéƒ½ä¸‹è½½ï¼Œè‹¥å­˜åœ¨åˆ™è¦†ç›–ã€‚</p></li>
<li><p>torch_dtypeï¼Œ<code>torch.float16</code> or
<code>torch.bfloat16</code> or
<code>torch.float</code>ï¼ŒæŒ‡å®šæ¨¡å‹å‚æ•°çš„è½½å…¥ç²¾åº¦ï¼Œä¸æŒ‡å®šåˆ™é»˜è®¤ä¸º<code>torch.float</code>ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œ<code>config.json</code>
ä¸­çš„ <code>torch_dtype</code> è®¾ç½®æ‹¥æœ‰æœ€é«˜ä¼˜å…ˆçº§ã€‚å¦‚æœ
<code>torch_dtype</code> å‚æ•°è¢«è®¾ç½®ä¸º
<code>"auto"</code>ï¼Œé‚£ä¹ˆå®ƒä¼šé¦–å…ˆä½¿ç”¨ <code>config.json</code>
ä¸­çš„è®¾ç½®ã€‚åªæœ‰å½“ <code>config.json</code> ä¸­æ²¡æœ‰æ‰¾åˆ°
<code>torch_dtype</code> ä¸” <code>torch_dtype</code> å‚æ•°è¢«è®¾ç½®ä¸º
<code>"auto"</code>æ—¶ï¼Œå®ƒæ‰ä¼šå›é€€åˆ°ä½¿ç”¨æƒé‡checkpointä¸­çš„æ•°æ®ç±»å‹ï¼ŒæŸ¥çœ‹ç¬¬ä¸€ä¸ªæ•°æ®æ˜¯ä»€ä¹ˆç±»å‹å°±ç”¨ä»€ä¹ˆç±»å‹ã€‚è‹¥æ ¹æœ¬æ²¡æœ‰è®¾ç½®è¯¥å‚æ•°ï¼Œåˆ™ä½¿ç”¨torch.floatã€‚</p></li>
<li><p>device_mapï¼Œå¯ä»¥ä¼ å…¥ä¸‰ç§ç±»å‹çš„å‚æ•°ã€‚<strong>å­—ç¬¦ä¸²ç±»å‹</strong>ï¼Œå¦‚æœä¼ å…¥ä¸€ä¸ªå­—ç¬¦ä¸²ç±»å‹çš„è®¾å¤‡åç§°(ä¾‹å¦‚
"cpu", "cuda:1", "mps")ï¼Œé‚£ä¹ˆæ•´ä¸ªæ¨¡å‹ä¼šè¢«åˆ†é…åˆ°æŒ‡å®šçš„è®¾å¤‡ä¸Šï¼Œå¦‚æœä¼ å…¥
"auto"ï¼ŒAccelerate
åº“ä¼šè‡ªåŠ¨è®¡ç®—å‡ºæœ€ä¼˜çš„è®¾å¤‡åˆ†å¸ƒã€‚<strong>å­—å…¸ç±»å‹</strong>ï¼Œè¿™ç§æƒ…å†µä¸‹
device_map
æ˜¯ä¸€ä¸ªå­—å…¸,é”®æ˜¯æ¨¡å‹çš„å­æ¨¡å—åç§°,å€¼æ˜¯å¯¹åº”çš„è®¾å¤‡ç¼–å·æˆ–è®¾å¤‡åç§°ï¼Œè¿™å…è®¸ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®šæ¨¡å‹çš„å„ä¸ªå­æ¨¡å—åº”è¯¥åˆ†å¸ƒåœ¨å“ªäº›è®¾å¤‡ä¸Šï¼Œåªéœ€è¦æŒ‡å®šåˆ°æ¨¡å—åç§°çš„çº§åˆ«,å­æ¨¡å—ä¼šè‡ªåŠ¨åˆ†é…åˆ°åŒä¸€è®¾å¤‡ï¼Œå¦‚</p>
<pre class="line-numbers language-none"><code class="language-none">device_map &#x3D; &#123;
    &quot;transformer.encoder&quot;: &quot;cuda:0&quot;,
    &quot;transformer.decoder&quot;: &quot;cuda:1&quot;,
    &quot;transformer.pooler&quot;: &quot;cuda:0&quot;,
    &quot;lm_head&quot;: &quot;cuda:1&quot;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>è¿˜å¯ä»¥ä¼ å…¥æ•´æ•°æˆ–<code>torch.device</code>ï¼Œä»£è¡¨å°†æ•´ä¸ªæ¨¡å‹æ”¾åœ¨æŒ‡å®šç¼–å·çš„
GPU
ä¸Šã€‚å¦‚<code>device = torch.device("cuda:1"),device_map = deveice</code>ã€‚åªè¦æŒ‡å®šäº†device_mapï¼Œé‚£ä¹ˆéƒ½ä¼šè®©
<code>low_cpu_mem_usage=True</code>ã€‚ä¸æŒ‡å®šå°±ç”¨cpuã€‚</p></li>
<li><p>quantization_configï¼ŒæŒ‡å®šæ¨¡å‹çš„é‡åŒ–ç­–ç•¥ã€‚å¯ä»¥æ˜¯ä¸€ä¸ªå­—å…¸æˆ–è€…ç»§æ‰¿è‡ª
<code>QuantizationConfigMixin</code>
çš„å¯¹è±¡ï¼Œå®ƒç”¨äºé…ç½®æ¨¡å‹çš„é‡åŒ–å‚æ•°ã€‚é™¤äº† <code>quantization_config</code>
ä¹‹å¤–,è¿˜å¯ä»¥ä½¿ç”¨ <code>load_in_4bit</code> å’Œ <code>load_in_8bit</code>
ç­‰å‚æ•°æ¥æŒ‡å®šé‡åŒ–æ–¹å¼,ä½†è¿™ç§æ–¹å¼ä¸è¢«æ¨èï¼Œåªé‡åŒ–äº†å‚æ•°ï¼Œå¹¶ä¸é‡åŒ–æ¢¯åº¦ã€‚ä½†æ¨ç†é˜¶æ®µæ— æ‰€è°“ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ã€‚</p></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> bitsandbytes <span class="token keyword">as</span> bnb
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> QuantizationConfig

quantization_config <span class="token operator">=</span> QuantizationConfig<span class="token punctuation">(</span>
    quantization_method<span class="token operator">=</span>bnb<span class="token punctuation">.</span>QuantizationMethod<span class="token punctuation">.</span>DYNAMIC_QUANT<span class="token punctuation">,</span>
    weight_bits<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token comment"># æƒé‡ä¸ºINT8</span>
    grad_bits<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token comment"># æ¢¯åº¦ä¹ŸINT8</span>
    per_channel<span class="token operator">=</span><span class="token boolean">False</span>
<span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForSeq2SeqLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    <span class="token string">"bigscience/T0_3B"</span><span class="token punctuation">,</span>
    quantization_config<span class="token operator">=</span>quantization_config
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>local_files_onlyï¼Œå¦‚æœæ˜¯Trueï¼Œåˆ™ä¸ä¼šä»Hubä¸Šä¸‹è½½ã€‚</li>
<li>low_cpu_mem_usageï¼Œä½œç”¨æ˜¯å°è¯•åœ¨åŠ è½½æ¨¡å‹æ—¶ä¸ä½¿ç”¨è¶…è¿‡æ¨¡å‹å¤§å° 1 å€çš„
CPU å†…å­˜(åŒ…æ‹¬å³°å€¼å†…å­˜)ã€‚</li>
<li>attn_implementationï¼Œå¯ä»¥é€‰æ‹©<code>flash_attention_2</code>,<code>sdpa(default)</code>,<code>eager(æ‰‹åŠ¨å®ç°)</code></li>
</ul>
<p>ä¹‹åçœ‹å‡ å¤„æ¯”è¾ƒå…³é”®çš„æºç ã€‚</p>
<p>ä»ä¼ å…¥çš„è·¯å¾„ä¸­æå–configã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Load config if we don't provide a configuration</span>
<span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>config<span class="token punctuation">,</span> PretrainedConfig<span class="token punctuation">)</span><span class="token punctuation">:</span>
    config_path <span class="token operator">=</span> config <span class="token keyword">if</span> config <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> pretrained_model_name_or_path
    config<span class="token punctuation">,</span> model_kwargs <span class="token operator">=</span> cls<span class="token punctuation">.</span>config_class<span class="token punctuation">.</span>from_pretrained<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>é‡åŒ–æ“ä½œã€‚æ³¨æ„åˆ°é‡åŒ–æ“ä½œä¼šå¼ºåˆ¶å¼€å¯low_cpu_mem_usageã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pre_quantized <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>config<span class="token punctuation">,</span> <span class="token string">"quantization_config"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
<span class="token keyword">if</span> pre_quantized <span class="token keyword">or</span> quantization_config <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> pre_quantized<span class="token punctuation">:</span>
        config<span class="token punctuation">.</span>quantization_config <span class="token operator">=</span> AutoHfQuantizer<span class="token punctuation">.</span>merge_quantization_configs<span class="token punctuation">(</span>
            config<span class="token punctuation">.</span>quantization_config<span class="token punctuation">,</span> quantization_config
        <span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        config<span class="token punctuation">.</span>quantization_config <span class="token operator">=</span> quantization_config
    hf_quantizer <span class="token operator">=</span> AutoHfQuantizer<span class="token punctuation">.</span>from_config<span class="token punctuation">(</span>config<span class="token punctuation">.</span>quantization_config<span class="token punctuation">,</span> pre_quantized<span class="token operator">=</span>pre_quantized<span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    hf_quantizer <span class="token operator">=</span> <span class="token boolean">None</span>

<span class="token keyword">if</span> hf_quantizer <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    hf_quantizer<span class="token punctuation">.</span>validate_environment<span class="token punctuation">(</span>
        torch_dtype<span class="token operator">=</span>torch_dtype<span class="token punctuation">,</span> from_tf<span class="token operator">=</span>from_tf<span class="token punctuation">,</span> from_flax<span class="token operator">=</span>from_flax<span class="token punctuation">,</span> device_map<span class="token operator">=</span>device_map
    <span class="token punctuation">)</span>
    torch_dtype <span class="token operator">=</span> hf_quantizer<span class="token punctuation">.</span>update_torch_dtype<span class="token punctuation">(</span>torch_dtype<span class="token punctuation">)</span>
    device_map <span class="token operator">=</span> hf_quantizer<span class="token punctuation">.</span>update_device_map<span class="token punctuation">(</span>device_map<span class="token punctuation">)</span>

    <span class="token comment"># Force-set to `True` for more mem efficiency</span>
    <span class="token keyword">if</span> low_cpu_mem_usage <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        low_cpu_mem_usage <span class="token operator">=</span> <span class="token boolean">True</span>
        logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span><span class="token string">"`low_cpu_mem_usage` was None, now set to True since model is quantized."</span><span class="token punctuation">)</span>
is_quantized <span class="token operator">=</span> hf_quantizer <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>åŠ è½½æƒé‡ï¼Œtfç›¸å…³çš„å°±ä¸çœ‹äº†ã€‚åœ¨åŠ è½½pytorchæƒé‡ä¸­ï¼Œä¼šå»ä½ æŒ‡å®šçš„æ–‡ä»¶å¤¹ä¸­æ‰¾<code>pytorch_model.bin</code>è¿™ä¸ªæƒé‡æ–‡ä»¶ã€‚<code>subfolder,variant</code>è‹¥ä¸åœ¨å‚æ•°ä¸­æŒ‡å®šéƒ½ä¸ºç©ºå­—ç¬¦ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">elif</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isfile<span class="token punctuation">(</span>
    os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>pretrained_model_name_or_path<span class="token punctuation">,</span> subfolder<span class="token punctuation">,</span> _add_variant<span class="token punctuation">(</span>WEIGHTS_NAME<span class="token punctuation">,</span> variant<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Load from a PyTorch checkpoint,ä¼šæ‹¼æˆmodel_path/pytorch_model.bin</span>
    archive_file <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>
        pretrained_model_name_or_path<span class="token punctuation">,</span> subfolder<span class="token punctuation">,</span> _add_variant<span class="token punctuation">(</span>WEIGHTS_NAME<span class="token punctuation">,</span> variant<span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>ä¸€äº›æ¨¡å‹çš„æƒé‡å¯èƒ½ä»¥å¤šä¸ªcheckpointæ–‡ä»¶æ¥ä¿å­˜ï¼Œè¿™æ—¶å€™è¦æ±‚æœ‰ä¸€ä¸ª<code>WEIGHTS_INDEX_NAME = "pytorch_model.bin.index.json"</code>æ–‡ä»¶æ¥è¿›è¡Œç´¢å¼•ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">                <span class="token keyword">elif</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isfile<span class="token punctuation">(</span>
                    os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>pretrained_model_name_or_path<span class="token punctuation">,</span> subfolder<span class="token punctuation">,</span> _add_variant<span class="token punctuation">(</span>WEIGHTS_INDEX_NAME<span class="token punctuation">,</span> variant<span class="token punctuation">)</span><span class="token punctuation">)</span>
                <span class="token punctuation">)</span><span class="token punctuation">:</span>
                    <span class="token comment"># Load from a sharded PyTorch checkpoint</span>
                    archive_file <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>
                        pretrained_model_name_or_path<span class="token punctuation">,</span> subfolder<span class="token punctuation">,</span> _add_variant<span class="token punctuation">(</span>WEIGHTS_INDEX_NAME<span class="token punctuation">,</span> variant<span class="token punctuation">)</span>
                    <span class="token punctuation">)</span>
                    is_sharded <span class="token operator">=</span> <span class="token boolean">True</span> <span class="token comment"># æ³¨æ„è¿™é‡Œ</span>
<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
<span class="token punctuation">&#123;</span>
    <span class="token string">"checkpoint_files"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"pytorch_model.bin.0"</span><span class="token punctuation">,</span> <span class="token string">"pytorch_model.bin.1"</span><span class="token punctuation">,</span> <span class="token string">"pytorch_model.bin.2"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"num_checkpoint_files"</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span>
    <span class="token string">"size_checkpoint_files"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">100000</span><span class="token punctuation">,</span> <span class="token number">200000</span><span class="token punctuation">,</span> <span class="token number">50000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"weight_map"</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span>
        <span class="token string">"layer1.weight"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">"layer1.bias"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">50000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">"layer2.weight"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">"layer2.bias"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">100000</span><span class="token punctuation">]</span>
    <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>è¿˜æœ‰ä¸€ç§æƒ…å†µï¼Œå°±æ˜¯æŒ‡å®šçš„è·¯å¾„ä¸æ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œè€Œæ˜¯æƒé‡æ–‡ä»¶æœ¬èº«ï¼Œå¦‚<code>bigscience/T0_3B/pytorch_model.bin</code>ï¼Œé‚£ä¹Ÿå¯ä»¥åŠ è½½ã€‚å› ä¸ºæœ€ç»ˆéƒ½æ˜¯è®©<code>archive_file = weight_file</code>ã€‚</p>
<pre class="line-numbers language-none"><code class="language-none">elif os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):
    archive_file &#x3D; pretrained_model_name_or_path
    is_local &#x3D; True<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>æœ€ç»ˆï¼Œ<code>resolved_archive_file = archive_file</code>ï¼Œè·å–æƒé‡æ–‡ä»¶è·¯å¾„ã€‚å¦‚æœæ˜¯åˆ†æ•£çš„checkpointï¼Œä¹Ÿå°±æ˜¯<code>is_sharded</code>æ˜¯Trueï¼Œè¿˜è¦è¿›è¡Œé¢å¤–çš„æ“ä½œï¼Œè¿™é‡Œå°±ä¸æ·±å…¥äº†ã€‚</p>
<p>æ¥ä¸‹æ¥å°±è¦åŠ è½½æƒé‡äº†ï¼Œé¦–å…ˆåˆ¤æ–­æ˜¯ä¸æ˜¯pytorchï¼Œè‹¥æ˜¯ï¼Œåˆ™åŠ è½½æƒé‡æ–‡ä»¶ã€‚è¯¦ç»†çš„åŠ è½½æºç å°±ä¸èµ˜è¿°ï¼Œæœ€ç»ˆä¼šè¿”å›ç”±torch.loadåŠ è½½æ¨¡å‹ç»“æ„å’Œæƒé‡å‚æ•°ã€‚</p>
<pre class="line-numbers language-none"><code class="language-none">if from_pt:
    if not is_sharded and state_dict is None:
        # Time to load the checkpoint
        state_dict &#x3D; load_state_dict(resolved_archive_file)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>æ¥ä¸‹æ¥å†³å®šæƒé‡çš„æ•°æ®ç±»å‹ï¼Œæ­£å¦‚ä¸Šé¢äº¤ä»£torch_dtypeå‚æ•°æ‰€è¯´ï¼Œå…ˆè€ƒè™‘<code>torch_dtype=auto</code>ï¼Œä¹Ÿå°±æ˜¯config.jsonä¸­çš„æ•°æ®ç±»å‹ã€‚ç„¶åå†è€ƒè™‘</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> torch_dtype <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>torch_dtype<span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> torch_dtype <span class="token operator">==</span> <span class="token string">"auto"</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>config<span class="token punctuation">,</span> <span class="token string">"torch_dtype"</span><span class="token punctuation">)</span> <span class="token keyword">and</span> config<span class="token punctuation">.</span>torch_dtype <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                torch_dtype <span class="token operator">=</span> config<span class="token punctuation">.</span>torch_dtype
                logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Will use torch_dtype=</span><span class="token interpolation"><span class="token punctuation">&#123;</span>torch_dtype<span class="token punctuation">&#125;</span></span><span class="token string"> as defined in model's config object"</span></span><span class="token punctuation">)</span>

        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
                <span class="token string-interpolation"><span class="token string">f'`torch_dtype` can be either `torch.dtype` or `"auto"`, but received </span><span class="token interpolation"><span class="token punctuation">&#123;</span>torch_dtype<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span>
            <span class="token punctuation">)</span>
    dtype_orig <span class="token operator">=</span> cls<span class="token punctuation">.</span>_set_default_torch_dtype<span class="token punctuation">(</span>torch_dtype<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>è‹¥æ˜¯åˆ†ç‰‡æƒ…å†µï¼Œåˆ™å»åˆ†ç‰‡jsonä¸­æ‰¾æœ‰æ²¡æœ‰æŒ‡å®šã€‚å¦‚æœä¸æ˜¯åˆ†ç‰‡çš„æƒ…å†µï¼Œåˆ™æŒ‰æƒé‡æ–‡ä»¶ä¸­ç¬¬ä¸€ä¸ªæ•°æ®çš„ç±»å‹ã€‚è‹¥ä¸æ˜¾å¼æŒ‡å®štorch_dtype(None)ï¼Œåˆ™ä½¿ç”¨float32ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">                    <span class="token keyword">else</span><span class="token punctuation">:</span>
                        <span class="token keyword">if</span> is_sharded <span class="token keyword">and</span> <span class="token string">"dtype"</span> <span class="token keyword">in</span> sharded_metadata<span class="token punctuation">:</span>
                            torch_dtype <span class="token operator">=</span> sharded_metadata<span class="token punctuation">[</span><span class="token string">"dtype"</span><span class="token punctuation">]</span>
                        <span class="token keyword">elif</span> <span class="token keyword">not</span> is_sharded<span class="token punctuation">:</span>
                            torch_dtype <span class="token operator">=</span> get_state_dict_dtype<span class="token punctuation">(</span>state_dict<span class="token punctuation">)</span>
                        <span class="token keyword">else</span><span class="token punctuation">:</span>
                            one_state_dict <span class="token operator">=</span> load_state_dict<span class="token punctuation">(</span>resolved_archive_file<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
                            torch_dtype <span class="token operator">=</span> get_state_dict_dtype<span class="token punctuation">(</span>one_state_dict<span class="token punctuation">)</span>
                            <span class="token keyword">del</span> one_state_dict  <span class="token comment"># free CPU memory</span>
                        logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span>
                            <span class="token string">"Since the `torch_dtype` attribute can't be found in model's config object, "</span>
                            <span class="token string">"will use torch_dtype=&#123;torch_dtype&#125; as derived from model's weights"</span>
                        <span class="token punctuation">)</span>    
<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>get_state_dict_dtype<span class="token punctuation">(</span>state_dict<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
<span class="token comment"># if no floating dtype was found return whatever the first dtype is</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token builtin">next</span><span class="token punctuation">(</span>state_dict<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>dtype<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>è¿˜æœ‰æ··åˆç²¾åº¦çš„æƒ…å†µï¼Œåœ¨åˆå§‹nn.Moduleçš„æ—¶å€™å¯ä»¥è®¾ç½®å•ç‹¬è®¾ç½®_keep_in_fp32_moduleså“ªäº›æ¨¡å—ä¿æŒfp32ç²¾åº¦ã€‚</p>
<pre class="line-numbers language-none"><code class="language-none"># Check if &#96;_keep_in_fp32_modules&#96; is not None
use_keep_in_fp32_modules &#x3D; (cls._keep_in_fp32_modules is not None) and (
    (torch_dtype &#x3D;&#x3D; torch.float16) or hasattr(hf_quantizer, &quot;use_keep_in_fp32_modules&quot;)
)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>åˆ›å»ºæ¨¡å‹å®ä¾‹ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">with</span> ContextManagers<span class="token punctuation">(</span>init_contexts<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Let's make sure we don't run the init function of buffer modules</span>
    model <span class="token operator">=</span> cls<span class="token punctuation">(</span>config<span class="token punctuation">,</span> <span class="token operator">*</span>model_args<span class="token punctuation">,</span> <span class="token operator">**</span>model_kwargs<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>æ¥çœ‹device_mapçš„é€»è¾‘ã€‚é¦–å…ˆæ˜¯å­—ç¬¦ä¸²çš„æƒ…å†µï¼Œå¿…é¡»è¦æ˜¯<code>"auto", "balanced", "balanced_low_0", "sequential"</code>è¿™å‡ ç§ï¼Œå¦åˆ™æŠ¥é”™ã€‚"auto"ä¼šè‡ªåŠ¨æ“ä½œï¼Œå°½å¯èƒ½å‡åŒ€åœ°åˆ†é…è®¡ç®—è´Ÿè½½ã€‚<code>balanced</code>åˆ™æ˜¯å¹³å‡åˆ†é…æ¨¡å‹å±‚ä¸­çš„å‚æ•°ç»™ä¸åŒçš„å¡ã€‚<code>balanced_low_0</code>åˆ™æ˜¯å°‘ç»™0åˆ†ä¸€äº›ï¼Œå› ä¸º0å¾€å¾€è¿˜æœ‰å…¶ä»–äº‹æƒ…è¦åšã€‚<code>sequential</code>åˆ™æ˜¯æŒ‰æ¨¡å‹å±‚çš„é¡ºåºæ¥åˆ†é…ç»™ä¸åŒçš„å¡ï¼Œä¿æŒæ¨¡å‹å±‚çš„æ‹“æ‰‘ç»“æ„,å‡å°‘è·¨è®¾å¤‡çš„æ•°æ®ä¼ è¾“ï¼Œå¦‚attentionä¸€å¼ å¡ï¼ŒMLPä¸€å¼ å¡ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> device_map <span class="token keyword">not</span> <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"auto"</span><span class="token punctuation">,</span> <span class="token string">"balanced"</span><span class="token punctuation">,</span> <span class="token string">"balanced_low_0"</span><span class="token punctuation">,</span> <span class="token string">"sequential"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> ValueError
<span class="token keyword">if</span> device_map <span class="token operator">!=</span> <span class="token string">"sequential"</span><span class="token punctuation">:</span>
                max_memory <span class="token operator">=</span> get_balanced_memory<span class="token punctuation">(</span>
                    model<span class="token punctuation">,</span>
                    dtype<span class="token operator">=</span>target_dtype<span class="token punctuation">,</span>
                    low_zero<span class="token operator">=</span><span class="token punctuation">(</span>device_map <span class="token operator">==</span> <span class="token string">"balanced_low_0"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    max_memory<span class="token operator">=</span>max_memory<span class="token punctuation">,</span>
                    <span class="token operator">**</span>device_map_kwargs<span class="token punctuation">,</span>
                <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>å®é™…ä¸Šå¯ä»¥çœ‹åˆ°ï¼Œ<code>auto</code>å°±æ˜¯<code>balanced</code>ç­–ç•¥ã€‚</p>
<p>å…¶ä»–æƒ…å†µï¼Œè¾“å…¥çš„ä»€ä¹ˆè®¾å¤‡å°±ç»‘å®šä»€ä¹ˆè®¾å¤‡ï¼Œè‹¥æ²¡æœ‰æŒ‡å®šdevice_mapï¼Œå°±åŠ è½½åˆ°cpuã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>device_map<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">:</span>
     device_map <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">""</span><span class="token punctuation">:</span> device_map<span class="token punctuation">&#125;</span>
<span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>device_map<span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token keyword">and</span> device_map <span class="token keyword">not</span> <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"auto"</span><span class="token punctuation">,</span> <span class="token string">"balanced"</span><span class="token punctuation">,</span> <span class="token string">"balanced_low_0"</span><span class="token punctuation">,</span> <span class="token string">"sequential"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
      <span class="token keyword">try</span><span class="token punctuation">:</span>
          device_map <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">""</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span>device_map<span class="token punctuation">)</span><span class="token punctuation">&#125;</span>
<span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>device_map<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment"># å°äº0æŠ¥é”™</span>
	device_map <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">""</span><span class="token punctuation">:</span> device_map<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>è¿™é‡Œæœ‰ä¸€ä¸ª<code>tie_weights</code>å‡½æ•°ï¼Œå®ç°äº†å‚æ•°çš„ç»‘å®šæ“ä½œï¼Œæœ¬è´¨ä¸Šå°±æ˜¯é»˜è®¤è®©è¾“å…¥åµŒå…¥å±‚å’Œè¾“å‡ºåµŒå…¥å±‚çš„æƒé‡ç»‘å®šåœ¨ä¸€èµ·ã€‚è‹¥æ˜¯åœ¨configä¸­æŒ‡å®š<code>is_encoder_decoder=True</code>ä¸”<code>tie_encoder_decoder=True</code>ï¼Œé‚£ä¹ˆEncoderå’ŒDecoderçš„å‚æ•°ä¹Ÿä¼šå…±ç”¨(éƒ½ä½¿ç”¨Decoderçš„Weights)ï¼Œä¸è¿‡T5ä¸­å¹¶ä¸è¿™ä¹ˆåšï¼Œä¸€èˆ¬æ˜¯<strong>BERT-based</strong>æ¨¡å‹åœ¨å¾®è°ƒæˆEncoder-Decoderæ¨¡å‹çš„æ—¶å€™ä¼šè¿™ä¹ˆåšã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">tie_weights</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Tie the weights between the input embeddings and the output embeddings.

    If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the
    weights instead.
    """</span>
    <span class="token keyword">if</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">,</span> <span class="token string">"tie_word_embeddings"</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        output_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>get_output_embeddings<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> output_embeddings <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>_tie_or_clone_weights<span class="token punctuation">(</span>output_embeddings<span class="token punctuation">,</span> self<span class="token punctuation">.</span>get_input_embeddings<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">,</span> <span class="token string">"is_encoder_decoder"</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">,</span> <span class="token string">"tie_encoder_decoder"</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> self<span class="token punctuation">.</span>base_model_prefix<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> self<span class="token punctuation">.</span>base_model_prefix<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_tie_encoder_decoder_weights<span class="token punctuation">(</span>self<span class="token punctuation">.</span>encoder<span class="token punctuation">,</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">,</span> self<span class="token punctuation">.</span>base_model_prefix<span class="token punctuation">)</span>

    <span class="token keyword">for</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> <span class="token string">"_tie_weights"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            module<span class="token punctuation">.</span>_tie_weights<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>å¼€å¯model.evalã€‚</p>
<pre class="line-numbers language-none"><code class="language-none">model.eval()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>è‹¥æ¨¡å‹æ˜¯ç”Ÿæˆå¼æ¨¡å‹ï¼Œé‚£ä¹ˆè¿˜éœ€è¦é…ç½®ç”Ÿæˆçš„å‚æ•°ã€‚<code>GenerationConfig</code>å®é™…ä¸Šå°±æ˜¯model.generate()æ–¹æ³•ä¸­æ‰€è¦ç”¨çš„å‚æ•°ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> model<span class="token punctuation">.</span>can_generate<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">and</span> pretrained_model_name_or_path <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    <span class="token keyword">try</span><span class="token punctuation">:</span>
        model<span class="token punctuation">.</span>generation_config <span class="token operator">=</span> GenerationConfig<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
            pretrained_model_name_or_path<span class="token punctuation">,</span>
            cache_dir<span class="token operator">=</span>cache_dir<span class="token punctuation">,</span>
            force_download<span class="token operator">=</span>force_download<span class="token punctuation">,</span>
            resume_download<span class="token operator">=</span>resume_download<span class="token punctuation">,</span>
            proxies<span class="token operator">=</span>proxies<span class="token punctuation">,</span>
            local_files_only<span class="token operator">=</span>local_files_only<span class="token punctuation">,</span>
            token<span class="token operator">=</span>token<span class="token punctuation">,</span>
            revision<span class="token operator">=</span>revision<span class="token punctuation">,</span>
            subfolder<span class="token operator">=</span>subfolder<span class="token punctuation">,</span>
            _from_auto<span class="token operator">=</span>from_auto_class<span class="token punctuation">,</span>
            _from_pipeline<span class="token operator">=</span>from_pipeline<span class="token punctuation">,</span>
            <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
    <span class="token keyword">except</span> OSError<span class="token punctuation">:</span>
        logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span>
            <span class="token string">"Generation config file not found, using a generation config created from the model config."</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>æœ€ç»ˆè¾“å‡ºä¸€äº›åŠ è½½å‚æ•°æ—¶è¾“å‡ºçš„ä¿¡æ¯ï¼Œç„¶åè¿”å›æ¨¡å‹ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> output_loading_info<span class="token punctuation">:</span>
    <span class="token keyword">if</span> loading_info <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        loading_info <span class="token operator">=</span> <span class="token punctuation">&#123;</span>
            <span class="token string">"missing_keys"</span><span class="token punctuation">:</span> missing_keys<span class="token punctuation">,</span>
            <span class="token string">"unexpected_keys"</span><span class="token punctuation">:</span> unexpected_keys<span class="token punctuation">,</span>
            <span class="token string">"mismatched_keys"</span><span class="token punctuation">:</span> mismatched_keys<span class="token punctuation">,</span>
            <span class="token string">"error_msgs"</span><span class="token punctuation">:</span> error_msgs<span class="token punctuation">,</span>
        <span class="token punctuation">&#125;</span>
    <span class="token keyword">return</span> model<span class="token punctuation">,</span> loading_info<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>æ€»ç»“ï¼Œæ ¹æ®æœ€å¸¸ç”¨çš„æ–¹æ³•ï¼Œä¸»è¦æ˜¯åšä»¥ä¸‹å‡ ä¸ªæ“ä½œã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> AutoModelForSeq2SeqLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"bigscience/T0_3B"</span><span class="token punctuation">,</span>device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<ol type="1">
<li>æ ¹æ®è¾“å…¥è·¯å¾„æ‹¿åˆ°config.jsonï¼ŒåŠ è½½åˆ°Configã€‚</li>
<li>æ ¹æ®è¾“å…¥è·¯å¾„æ‹¿åˆ°æƒé‡æ–‡ä»¶pytorch_model.binï¼Œç”±torch.loadåŠ è½½æ¨¡å‹ç»“æ„å’Œæƒé‡å‚æ•°ã€‚ã€‚</li>
<li>å†³å®šæƒé‡çš„æ•°æ®ç±»å‹ï¼ŒæœªæŒ‡å®šåˆ™æ˜¯float32ã€‚</li>
<li>å¹³å‡åˆ†é…å‚æ•°ç»™å„å¼ å¡ã€‚</li>
<li>ç»‘å®šinputå’Œoutputçš„Embeddingï¼Œè®©å…¶ä½¿ç”¨åŒä¸€ä»½Embeddingå‚æ•°ã€‚</li>
<li>model.eval()ã€‚</li>
<li>è‹¥æ˜¯ç”Ÿæˆå¼æ¨¡å‹ï¼Œé…ç½®ç”Ÿæˆå‚æ•°ã€‚</li>
<li>è¿”å›æ¨¡å‹å®ä¾‹ã€‚</li>
</ol>
<p>æ‰€ä»¥ï¼Œå®é™…ä¸Šä¼šè°ƒç”¨ä¸¤ä¸ªä¸åŒçš„<code>from_pretrained</code>æ–¹æ³•ï¼Œç¬¬ä¸€ä¸ªæ˜¯AutoModelåŸºç±»_BaseAutoModelClassçš„ï¼Œåœ¨æœ€åè°ƒç”¨<code>get_model_class</code>æ–¹æ³•å¾—åˆ°æ¨¡å‹æœ¬èº«çš„ç±»ï¼Œæœ¬ä¾‹ä¸­æ˜¯<code>T5ForConditionalGeneration</code>ï¼Œç„¶åå†è°ƒç”¨è¿™ä¸ªç±»çš„<code>from_pretrained</code>ï¼Œè€Œè¿™ä¸ªç±»çš„<code>from_pretrained</code>åœ¨å…¶åŸºç±»<code>PreTrainedModel</code>å®ç°ï¼Œæ‰€ä»¥å†ä¼šè°ƒç”¨<code>PreTrainedModel</code>çš„<code>from_pretrained</code>æ–¹æ³•ã€‚åˆ†æå®Œæ¯•ã€‚</p>
<h1 id="åˆ†è¯">åˆ†è¯</h1>
<p>åç»­æ›´æ–°ï¼ŒæŒ–å‘</p>
<h1 id="ç”Ÿæˆ">ç”Ÿæˆ</h1>
<p>å¹¶ä¸æ˜¯æ¯ä¸€ä¸ªæ¨¡å‹éƒ½å¯ä»¥ä½¿ç”¨.generate()è¿›è¡Œåºåˆ—ç”Ÿæˆï¼Œéœ€è¦é€šè¿‡å‡½æ•°åˆ¤æ–­æ˜¯å¦èƒ½å¤Ÿè¿›è¡Œåºåˆ—ç”Ÿæˆä»»åŠ¡ï¼Œæ‰€ä»¥æ¯ä¸€ä¸ªæ¨¡å‹éƒ½éœ€è¦é‡å†™<code>prepare_inputs_for_generation</code>æ–¹æ³•ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PreTrainedModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">,</span> ModuleUtilsMixin<span class="token punctuation">,</span> GenerationMixin<span class="token punctuation">,</span> PushToHubMixin<span class="token punctuation">,</span> PeftAdapterMixin<span class="token punctuation">)</span><span class="token punctuation">:</span>
	    <span class="token decorator annotation punctuation">@classmethod</span>
    <span class="token keyword">def</span> <span class="token function">can_generate</span><span class="token punctuation">(</span>cls<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">bool</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Returns whether this model can generate sequences with `.generate()`.

        Returns:
            `bool`: Whether this model can generate sequences with `.generate()`.
        """</span>
        <span class="token comment"># Detects whether `prepare_inputs_for_generation` has been overwritten, which is a requirement for generation.</span>
        <span class="token comment"># Alternativelly, the model can also have a custom `generate` function.</span>
        <span class="token keyword">if</span> <span class="token string">"GenerationMixin"</span> <span class="token keyword">in</span> <span class="token builtin">str</span><span class="token punctuation">(</span>cls<span class="token punctuation">.</span>prepare_inputs_for_generation<span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token string">"GenerationMixin"</span> <span class="token keyword">in</span> <span class="token builtin">str</span><span class="token punctuation">(</span>cls<span class="token punctuation">.</span>generate<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> <span class="token boolean">False</span>
        <span class="token keyword">return</span> <span class="token boolean">True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>è€Œgenerateæ–¹æ³•æœ¬èº«å°±åœ¨<code>GenerationMixin</code>ç±»ä¸­å®ç°ã€‚</p>
<pre class="line-numbers language-none"><code class="language-none">Class that holds a configuration for a generation task. A &#96;generate&#96; call supports the following generation methods
for text-decoder, text-to-text, speech-to-text, and vision-to-text models:

    - *greedy decoding* by calling [&#96;~generation.GenerationMixin._greedy_search&#96;] if &#96;num_beams&#x3D;1&#96; and
        &#96;do_sample&#x3D;False&#96;
    - *contrastive search* by calling [&#96;~generation.GenerationMixin._contrastive_search&#96;] if &#96;penalty_alpha&gt;0.&#96;
        and &#96;top_k&gt;1&#96;
    - *multinomial sampling* by calling [&#96;~generation.GenerationMixin._sample&#96;] if &#96;num_beams&#x3D;1&#96; and
        &#96;do_sample&#x3D;True&#96;
    - *beam-search decoding* by calling [&#96;~generation.GenerationMixin._beam_search&#96;] if &#96;num_beams&gt;1&#96; and
        &#96;do_sample&#x3D;False&#96;
    - *beam-search multinomial sampling* by calling [&#96;~generation.GenerationMixin._beam_sample&#96;] if
        &#96;num_beams&gt;1&#96; and &#96;do_sample&#x3D;True&#96;
    - *diverse beam-search decoding* by calling [&#96;~generation.GenerationMixin._group_beam_search&#96;], if
        &#96;num_beams&gt;1&#96; and &#96;num_beam_groups&gt;1&#96;
    - *constrained beam-search decoding* by calling [&#96;~generation.GenerationMixin._constrained_beam_search&#96;], if
        &#96;constraints!&#x3D;None&#96; or &#96;force_words_ids!&#x3D;None&#96;
    - *assisted decoding* by calling [&#96;~generation.GenerationMixin._assisted_decoding&#96;], if
        &#96;assistant_model&#96; or &#96;prompt_lookup_num_tokens&#96; is passed to &#96;.generate()&#96;

You do not need to call any of the above methods directly. Pass custom parameter values to &#39;.generate()&#39;. To learn
more about decoding strategies refer to the [text generation strategies guide](..&#x2F;generation_strategies).<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>å°±ä¸çœ‹è¯¦ç»†çš„å®ç°ï¼Œå…ˆçœ‹<code>GenerationConfig</code>çš„ç”Ÿæˆç­–ç•¥ã€‚è‹¥ä¸ç”¨GenerationConfigï¼Œä¹Ÿå¯ä»¥ç›´æ¥è¾“å…¥kwargsã€‚</p>
<ul>
<li>greedy
decodingè´ªå©ªè§£ç ï¼Œ<code>num_beams=1</code>ä¸”<code>do_sample=False</code>ï¼Œä¼šä¸€ç›´é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„tokenï¼Œä¸€æ¡è·¯èµ°åˆ°é»‘ã€‚</li>
<li>Contrastive
searchå¯¹æ¯”æœç´¢ï¼Œåœ¨NIPS22è¢«æå‡ºï¼Œèƒ½åœ¨ä¿æŒæµç•…æ€§çš„å‰æä¸‹ï¼Œé¼“åŠ±å¤šæ ·æ€§ç”Ÿæˆï¼Œå‡å°‘é‡å¤è¾“å‡ºã€‚éœ€è¦<code>penalty_alpha&gt;0</code>
and
<code>top_k&gt;1</code>ã€‚ä¸€ä¸ªå€™é€‰tokenä¸å½“å‰tokenéå¸¸ç›¸ä¼¼(ç›¸ä¼¼åº¦å¾—åˆ†é«˜)ï¼Œé‚£ä¹ˆå®ƒçš„æ¦‚ç‡å°±ä¼šè¢«è¾ƒå¤šåœ°é™ä½ã€‚è¿™æ ·åšçš„ç›®çš„æ˜¯é¼“åŠ±ç”Ÿæˆæ›´åŠ å¤šæ ·åŒ–çš„æ–‡æœ¬ï¼Œé¿å…åŒç±»å‹çš„tokenè¿‡äºé›†ä¸­å‡ºç°ã€‚æœ€å,ç®—æ³•åœ¨ç»è¿‡è°ƒæ•´çš„
<code>scores</code>
å‘é‡ä¸Šå–Top-1ã€‚æ ¸å¿ƒå…¬å¼<code>scores = (1.0 - alpha) * next_top_k_probs - alpha * scores</code>ã€‚<code>next_top_k_probs</code>æ˜¯å½“å‰tokençš„Top-kæ¦‚ç‡åˆ†å¸ƒï¼Œç­‰å¼å³è¾¹çš„<code>scores</code>æ˜¯å½“å‰tokenå’Œä¸‹ä¸€ä¸ªtokenä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°ã€‚æ‰€ä»¥å½“å‰tokenä¸next
tokenè¶Šç›¸ä¼¼ï¼Œæƒ©ç½šå°±è¶Šå¤§ã€‚</li>
<li>multinomial samplingï¼Œ<code>num_beams=1</code>
and<code>do_sample=True</code>ã€‚å’Œè´ªå©ªè§£ç çš„åŒºåˆ«ä¸ä¸€å®šé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„tokenï¼Œè€Œæ˜¯æ ¹æ®æ¦‚ç‡åˆ†å¸ƒæ¥é‡‡æ ·ã€‚</li>
<li>beam-searchï¼Œ<code>num_beams&gt;1</code>
and<code>do_sample=False</code>ï¼Œä¿ç•™top-kä¸ªå¾—åˆ†æœ€é«˜çš„å€™é€‰åºåˆ—ï¼Œç§°ä¸º"beam"ã€‚è¿™é‡Œé€‰æ‹©ä¸é‡‡æ ·ï¼Œæ˜¯é€‰æ‹©å¾—åˆ†æœ€é«˜çš„2*num_beamsä¸ªtokenã€‚</li>
<li>diverse beam-searchï¼Œ<code>num_beams&gt;1</code> and
<code>num_beam_groups&gt;1</code>ï¼Œé€šè¿‡åˆ†ç»„æœºåˆ¶ï¼Œç¡®ä¿äº†ä¸åŒbeamä¹‹é—´çš„å·®å¼‚æ€§ã€‚</li>
</ul>
<p>æ¥ä¸‹æ¥ä»‹ç»ä¸€äº›æ¯”è¾ƒå¸¸ç”¨çš„å‚æ•°ã€‚</p>
<ul>
<li><code>do_sample</code>ï¼Œæ˜¯å¦æ ¹æ®æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·ã€‚</li>
<li><code>temperature</code>ï¼Œé»˜è®¤1.0ã€‚å°äº1æ—¶ï¼Œå½“
<code>temperature &lt; 1.0</code> æ—¶,
ç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒä¼šè¢«"å¹³æ»‘"(å³°å€¼å˜å¾—æ›´é™¡å³­)ï¼Œä½¿å¾—æ¨¡å‹æ›´å€¾å‘äºé€‰æ‹©æ¦‚ç‡è¾ƒé«˜çš„tokenï¼Œç”Ÿæˆçš„æ–‡æœ¬ä¼šæ›´åŠ é›†ä¸­å’Œä¿å®ˆã€‚å½“<code>temperature &gt; 1.0</code>æ—¶ï¼Œ
ç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒä¼šè¢«"æ‹‰å¹³"(å³°å€¼å˜å¾—æ›´å¹³ç¼“)ï¼Œä½¿å¾—æ¨¡å‹ä¼šé€‰æ‹©æ¦‚ç‡è¾ƒä½çš„tokenï¼Œç”Ÿæˆçš„æ–‡æœ¬ä¼šæ›´åŠ å¤šæ ·å’Œæ¢ç´¢æ€§ã€‚</li>
<li><code>top_k</code>ï¼Œé€‰æ‹©ä¸‹ä¸€ä¸ªtokenæ—¶ï¼Œåªä¿ç•™æ¦‚ç‡æœ€é«˜çš„å‰
<code>top_k</code>
ä¸ªtokenï¼Œæœ‰æ•ˆåœ°é¿å…æ¨¡å‹é€‰æ‹©æ¦‚ç‡å¾ˆä½çš„ä¸åˆç†tokenã€‚</li>
<li><code>top_p</code>ï¼ŒåŠ¨æ€åœ°é€‰æ‹©æ¦‚ç‡æ€»å’Œè¾¾åˆ° top_p
é˜ˆå€¼çš„æœ€å°tokené›†åˆã€‚</li>
<li><code>num_return_sequences</code>
ï¼ŒæŒ‡å®šè¦ç”Ÿæˆçš„ç‹¬ç«‹åºåˆ—æ•°é‡ã€‚é»˜è®¤ä¸º1ï¼Œå³åªç”Ÿæˆ1ä¸ªåºåˆ—ã€‚</li>
<li><code>output_scores</code>æ˜¯å¦è¾“å‡ºæ¯ä¸ªtokençš„é¢„æµ‹åˆ†æ•°ã€‚</li>
<li><code>output_logits</code>æ˜¯å¦è¾“å‡ºæœªç»å¤„ç†çš„åŸå§‹é¢„æµ‹logitsã€‚</li>
<li><code>pad_token_id,bos_token_id,eos_token_id</code>ï¼Œéœ€è¦æ ¹æ®æ¨¡å‹çš„è¯è¡¨æ¥çœ‹ã€‚ä¸è®¾ç½®åˆ™ä¸ºNoneã€‚</li>
<li><code>max_length</code>:
æœ€å¤§è¾“å‡ºé•¿åº¦,åŒ…æ‹¬promptå’Œç”Ÿæˆçš„æ–°tokensã€‚é»˜è®¤æ˜¯20</li>
<li><code>max_new_tokens</code>:
æœ€å¤§ç”Ÿæˆæ–°tokensæ•°é‡,ä¸åŒ…æ‹¬prompté•¿åº¦ã€‚</li>
<li><code>min_length</code>:
æœ€å°è¾“å‡ºé•¿åº¦,åŒ…æ‹¬promptå’Œç”Ÿæˆçš„æ–°tokensã€‚</li>
<li><code>min_new_tokens</code>:
æœ€å°ç”Ÿæˆæ–°tokensæ•°é‡,ä¸åŒ…æ‹¬prompté•¿åº¦ã€‚</li>
<li><code>early_stopping</code>: æ§åˆ¶beam searchåœæ­¢çš„æ¡ä»¶ã€‚å¯é€‰å€¼ä¸º:
<ul>
<li><code>True</code>: å½“ç”Ÿæˆäº† <code>num_beams</code>
ä¸ªå®Œæ•´å€™é€‰åºåˆ—æ—¶ç«‹å³åœæ­¢ã€‚</li>
<li><code>False</code>:
æ ¹æ®å¯å‘å¼åœæ­¢,å³å½“å¾ˆéš¾æ‰¾åˆ°æ›´å¥½çš„å€™é€‰æ—¶åœæ­¢ã€‚</li>
<li><code>"never"</code>: ä¸€ç›´è¿è¡Œç›´åˆ°æ— æ³•æ‰¾åˆ°æ›´å¥½çš„å€™é€‰ä¸ºæ­¢ã€‚</li>
</ul></li>
</ul>
<p>æ¥ä¸‹æ¥è§£ægenerateå‡½æ•°ä¸»è¦åšäº†å“ªäº›ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>no_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">generate</span><span class="token punctuation">(</span>
    self<span class="token punctuation">,</span>
    inputs<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    generation_config<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>GenerationConfig<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    logits_processor<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>LogitsProcessorList<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    stopping_criteria<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>StoppingCriteriaList<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    prefix_allowed_tokens_fn<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Callable<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    synced_gpus<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    assistant_model<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token string">"PreTrainedModel"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    streamer<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token string">"BaseStreamer"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    negative_prompt_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    negative_prompt_attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Union<span class="token punctuation">[</span>GenerateOutput<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span><span class="token punctuation">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>inputsï¼Œä¸€èˆ¬æ˜¯ç»è¿‡tokenizerå¤„ç†çš„åºåˆ—ï¼ŒåŒ…å«attention_maskçš„ã€‚å¦‚æœæ˜¯è°ƒç”¨tokenizer.encode()ï¼Œé‚£ä¹ˆä¸ä¼šæœ‰attention_maskã€‚</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call</span>
<span class="token string">"å¤„ç† `generation_config` å’Œå¯èƒ½æ›´æ–°å®ƒçš„ `kwargs`ï¼Œå¹¶éªŒè¯ `.generate()` çš„è°ƒç”¨ï¼Œç•¥"</span>

<span class="token comment"># 2. Set generation parameters if not already defined</span>
<span class="token triple-quoted-string string">"""
ç•¥ï¼Œè®¾ç½®ç”Ÿæˆæ‰€éœ€çš„ä¸€äº›é»˜è®¤å‚æ•°
"""</span>
<span class="token keyword">if</span> generation_config<span class="token punctuation">.</span>pad_token_id <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">and</span> generation_config<span class="token punctuation">.</span>eos_token_id <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> model_kwargs<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"attention_mask"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span>
            <span class="token string">"The attention mask and the pad token id were not set. As a consequence, you may observe "</span>
            <span class="token string">"unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results."</span>
        <span class="token punctuation">)</span>
    eos_token_id <span class="token operator">=</span> generation_config<span class="token punctuation">.</span>eos_token_id
    <span class="token comment"># å¤šè¯­è¨€æ¨¡å‹çš„eoså¯èƒ½ä¼šæœ‰å¤šä¸ª</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>eos_token_id<span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        eos_token_id <span class="token operator">=</span> eos_token_id<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Setting `pad_token_id` to `eos_token_id`:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>eos_token_id<span class="token punctuation">&#125;</span></span><span class="token string"> for open-end generation."</span></span><span class="token punctuation">)</span>
    generation_config<span class="token punctuation">.</span>pad_token_id <span class="token operator">=</span> eos_token_id<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>è¿™ä¸€æ®µè¯´æ˜ç¡®è¯´æ˜å¦‚æœä½ æ²¡æœ‰ä¼ å…¥<code>pad_token_id</code>ï¼Œé‚£ä¹ˆä¼šä»¥<code>eos_token_id</code>æ›¿ä»£ã€‚è‹¥ä½ æ²¡æœ‰ä¼ å…¥<code>attention_mask</code>ï¼Œä¼šè­¦å‘Šä½ ä¼ å…¥ï¼ŒAttention
Maskä¸­å€¼ä¸º0çš„ä½ç½®å¯¹åº”çš„Attentionæƒé‡è®¾ä¸ºéå¸¸å°çš„è´Ÿå€¼ï¼Œé€šå¸¸æ˜¯-1e9ã€‚</p>
<p>æ¥ä¸‹æ¥å°±æ˜¯å¤„ç†æ¨¡å‹çš„è¾“å…¥ã€‚è·å–è¾“å…¥çš„tensorå’Œbatch_sizeã€‚<code>_prepare_model_inputs</code>æ–¹æ³•è¿‡æ»¤æ‰
model_kwargsä¸­éç©ºä¸”ä¸æ˜¯æ¨¡å‹ä¸»è¦è¾“å…¥çš„å‚æ•°ã€‚å¯¹äºæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œè¦çœ‹æ¨¡å‹çš„encoderæ˜¯å¦æ”¯æŒç›´æ¥è¾“å…¥embeddingï¼Œå¦åˆ™ä¸€å¾‹è®¾ç½®æˆinput_idsã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 3. Define model inputs</span>
<span class="token comment"># inputs_tensor has to be defined</span>
<span class="token comment"># model_input_name is defined if model-specific keyword input is passed</span>
<span class="token comment"># otherwise model_input_name is None</span>
<span class="token comment"># all model-specific keyword inputs are removed from `model_kwargs`</span>
inputs_tensor<span class="token punctuation">,</span> model_input_name<span class="token punctuation">,</span> model_kwargs <span class="token operator">=</span> self<span class="token punctuation">.</span>_prepare_model_inputs<span class="token punctuation">(</span>
    inputs<span class="token punctuation">,</span> generation_config<span class="token punctuation">.</span>bos_token_id<span class="token punctuation">,</span> model_kwargs
<span class="token punctuation">)</span>
batch_size <span class="token operator">=</span> inputs_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>decode-onlyçš„æ¨¡å‹åº”è¯¥ä½¿ç”¨å·¦å¯¹é½ã€‚ä½¿ç”¨å³å¯¹é½ä¼šè­¦å‘Šï¼Œåˆå§‹åŒ– tokenizer
æ—¶è®¾ç½®
<code>padding_side='left'</code>ä»¥ç¡®ä¿æ­£ç¡®çš„ç”Ÿæˆç»“æœã€‚æ¥ä¸‹æ¥çš„é€»è¾‘éƒ½ä¸çœ‹äº†ï¼Œæ— éå°±æ˜¯å¤„ç†ä¸€äº›æ¨¡å‹ç”Ÿæˆå‚æ•°ï¼Œå¦‚max_lengthã€‚æ ¹æ®ä¸åŒçš„ç”Ÿæˆç­–ç•¥ï¼Œä¼šè¿è¡Œä¸åŒçš„ç”Ÿæˆå‡½æ•°ï¼Œå°±çœ‹ä¸€ä¸ªç®€å•çš„è´ªå©ªè§£ç çš„éƒ¨åˆ†ä»£ç ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">        <span class="token keyword">if</span> generation_mode <span class="token operator">==</span> GenerationMode<span class="token punctuation">.</span>GREEDY_SEARCH<span class="token punctuation">:</span>
            <span class="token comment"># 11. run greedy search</span>
            result <span class="token operator">=</span> self<span class="token punctuation">.</span>_greedy_search<span class="token punctuation">(</span>
                input_ids<span class="token punctuation">,</span>
                logits_processor<span class="token operator">=</span>prepared_logits_processor<span class="token punctuation">,</span> <span class="token comment"># logitså¤„ç†å™¨ï¼Œmin_lengthä½œç”¨äºè¿™ä¸ªï¼Œåœ¨æ»¡è¶³å‰å‡å°eosçš„æ¦‚ç‡ã€‚</span>
                stopping_criteria<span class="token operator">=</span>prepared_stopping_criteria<span class="token punctuation">,</span> <span class="token comment"># åœæ­¢åˆ¤å®šå™¨ï¼Œmax_lengthå°±ä½œç”¨äºè¿™ä¸ª</span>
                pad_token_id<span class="token operator">=</span>generation_config<span class="token punctuation">.</span>pad_token_id<span class="token punctuation">,</span>
                eos_token_id<span class="token operator">=</span>generation_config<span class="token punctuation">.</span>eos_token_id<span class="token punctuation">,</span>
                output_attentions <span class="token operator">=</span> generation_config<span class="token punctuation">.</span>output_attentionsï¼Œ<span class="token comment"># æ˜¯å¦è¾“å‡ºæ³¨æ„åŠ›å±‚åˆ†æ•°</span>
                output_hidden_states <span class="token operator">=</span> generation_config<span class="token punctuation">.</span>output_hidden_states <span class="token comment"># æ˜¯å¦è¿”å›éšè—çŠ¶æ€</span>
                output_scores<span class="token operator">=</span>generation_config<span class="token punctuation">.</span>output_scores<span class="token punctuation">,</span>
                output_logits<span class="token operator">=</span>generation_config<span class="token punctuation">.</span>output_logits<span class="token punctuation">,</span>
                return_dict_in_generate<span class="token operator">=</span>generation_config<span class="token punctuation">.</span>return_dict_in_generate<span class="token punctuation">,</span>
                synced_gpus<span class="token operator">=</span>synced_gpus<span class="token punctuation">,</span>
                streamer<span class="token operator">=</span>streamer<span class="token punctuation">,</span>
                <span class="token operator">**</span>model_kwargs<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>

    <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>é¦–å…ˆæ‹¿åˆ°å…¨éƒ¨çš„è¾“å‡ºï¼Œå¹¶åªéœ€è¦ä¸‹ä¸€ä¸ªtokençš„å†…å®¹ã€‚<code>next_tokens</code>åœ¨åºåˆ—ç»“æŸçš„æƒ…å†µä¸‹ï¼Œä¸€å®šæ˜¯pad_idã€‚ç”Ÿæˆåæ›´æ–°<code>input_ids</code>ï¼Œè‹¥ç”Ÿæˆäº†eos_idï¼Œå°±è®¤ä¸ºåºåˆ—å·²ç»å®Œæˆã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">while</span> self<span class="token punctuation">.</span>_has_unfinished_sequences<span class="token punctuation">(</span>this_peer_finished<span class="token punctuation">,</span> synced_gpus<span class="token punctuation">,</span> device<span class="token operator">=</span>input_ids<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">:</span>            
  		outputs <span class="token operator">=</span> self<span class="token punctuation">(</span>
              <span class="token operator">**</span>model_inputs<span class="token punctuation">,</span>
              return_dict<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
              output_attentions<span class="token operator">=</span>output_attentions<span class="token punctuation">,</span>
              output_hidden_states<span class="token operator">=</span>output_hidden_states<span class="token punctuation">,</span>
          <span class="token punctuation">)</span>
          next_token_logits <span class="token operator">=</span> outputs<span class="token punctuation">.</span>logits<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment"># æœ€åä¸€ä¸ªæ—¶é—´æ­¥</span>
          next_tokens_scores <span class="token operator">=</span> logits_processor<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> next_token_logits<span class="token punctuation">)</span>
          next_tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>next_tokens_scores<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># æœ€å¤§å€¼ç´¢å¼•ï¼Œè´ªå©ªç­–ç•¥</span>
          <span class="token comment"># finished sentences should have their next token be a padding token</span>
          <span class="token keyword">if</span> eos_token_id <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
              <span class="token keyword">if</span> pad_token_id <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                  <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"If `eos_token_id` is defined, make sure that `pad_token_id` is defined."</span><span class="token punctuation">)</span>
                 	<span class="token comment"># unfinished_sequencesåˆå§‹åŒ–æ˜¯torch.ones(batch_size,dtype = torch.long)                   </span>
              next_tokens <span class="token operator">=</span> next_tokens <span class="token operator">*</span> unfinished_sequences <span class="token operator">+</span> pad_token_id <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> unfinished_sequences<span class="token punctuation">)</span>
          <span class="token comment"># update generated ids, model inputs, and length for next step</span>
          input_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>input_ids<span class="token punctuation">,</span> next_tokens<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
          <span class="token comment"># if eos_token was found in one sentence, set sentence to finished</span>
          <span class="token keyword">if</span> eos_token_id_tensor <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
              unfinished_sequences <span class="token operator">=</span> unfinished_sequences<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>
                  next_tokens<span class="token punctuation">.</span>tile<span class="token punctuation">(</span>eos_token_id_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>ne<span class="token punctuation">(</span>eos_token_id_tensor<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>prod<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
              <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>æœ€ågenerateæ–¹æ³•è¿”å›ä¸€ä¸ªUnion[GenerateOutput,torch.LongTensor]ã€‚ä¸€èˆ¬æ¥è¯´æ˜¯å‰è€…ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">GenerateNonBeamOutput <span class="token operator">=</span> Union<span class="token punctuation">[</span>GenerateDecoderOnlyOutput<span class="token punctuation">,</span> GenerateEncoderDecoderOutput<span class="token punctuation">]</span>
GenerateBeamOutput <span class="token operator">=</span> Union<span class="token punctuation">[</span>GenerateBeamDecoderOnlyOutput<span class="token punctuation">,</span> GenerateBeamEncoderDecoderOutput<span class="token punctuation">]</span>
GenerateOutput <span class="token operator">=</span> Union<span class="token punctuation">[</span>GenerateNonBeamOutput<span class="token punctuation">,</span> GenerateBeamOutput<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>æˆ‘ä»¬å°±æ‹¿<code>GenerateBeamDecoderOnlyOutput</code>æ¥çœ‹ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">sequences<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>LongTensor <span class="token operator">=</span> <span class="token boolean">None</span> <span class="token comment"># è¿”å›çš„åºåˆ—ï¼Œéœ€è¦è¿›è¡Œdecodeï¼Œä¸€èˆ¬åªç”¨è¿™ä¸ª</span>
sequences_scores<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span> <span class="token comment"># åºåˆ—beam_searchçš„åˆ†æ•°</span>
scores<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
logits<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
beam_indices<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
attentions<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
hidden_states<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
past_key_values<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>æœ¬æ–‡ä½œè€…ï¼š</strong>iroha</li><li class="post-copyright-link"><strong>æœ¬æ–‡é“¾æ¥ï¼š</strong><a href="http://example.com/post/model_load.html" title="Huggingfaceçš„æ¨¡å‹åŠ è½½æµç¨‹">http://example.com/post/model_load.html</a></li><li class="post-copyright-license"><strong>ç‰ˆæƒå£°æ˜ï¼š</strong>æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é»˜è®¤é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> è®¸å¯åè®®ã€‚</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/post/rl_surface.html" rel="prev" title="DRL introduce"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">DRL introduce</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/post/ft_survey.html" rel="next" title="å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ç»¼è¿°"><span class="post-nav-text">å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ç»¼è¿°</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 â€“ 2024 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> iroha</span></div><div class="powered"><span>ç”± <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> é©±åŠ¨ v6.3.0</span><span class="footer-separator">|</span><span>ä¸»é¢˜ - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.11</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>