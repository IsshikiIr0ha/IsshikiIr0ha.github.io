<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="iroha"><meta name="copyright" content="iroha"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>Huggingface的模型加载流程 | iroha</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...undefined?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"example.com","root":"/","title":"不想摆烂","version":"1.10.11","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js" defer></script><script src="/js/load-aplayer.js" defer></script><meta name="description" content="我们以下面的这一句语句作为开始，以从本地加载模型为例。 model &#x3D; AutoModelForSeq2SeqLM.from_pretrained(&quot;bigscience&#x2F;T0_3B&quot;) inputs &#x3D; tokenizer.encode(q.strip()+&quot; ? To answer this question, we need to know&quot;, return_tensors&#x3D;&quot;pt&quot;) o">
<meta property="og:type" content="article">
<meta property="og:title" content="Huggingface的模型加载流程">
<meta property="og:url" content="http://example.com/post/model_load.html">
<meta property="og:site_name" content="iroha">
<meta property="og:description" content="我们以下面的这一句语句作为开始，以从本地加载模型为例。 model &#x3D; AutoModelForSeq2SeqLM.from_pretrained(&quot;bigscience&#x2F;T0_3B&quot;) inputs &#x3D; tokenizer.encode(q.strip()+&quot; ? To answer this question, we need to know&quot;, return_tensors&#x3D;&quot;pt&quot;) o">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20240422014943359.png">
<meta property="article:published_time" content="2024-04-11T08:00:00.000Z">
<meta property="article:modified_time" content="2024-07-22T09:14:59.982Z">
<meta property="article:author" content="iroha">
<meta property="article:tag" content="大模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20240422014943359.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="iroha"><img width="96" loading="lazy" src="/yun.png" alt="iroha"><span class="site-author-status" title="永远相信美好的事情即将发生">😊</span></a><div class="site-author-name"><a href="/about/">iroha</a></div><span class="site-name">iroha</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">16</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">5</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">2</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="友链" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.</span> <span class="toc-text">加载</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E8%AF%8D"><span class="toc-number">2.</span> <span class="toc-text">分词</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%9F%E6%88%90"><span class="toc-number">3.</span> <span class="toc-text">生成</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://example.com/post/model_load.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="iroha"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="iroha"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Huggingface的模型加载流程</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2024-04-11 16:00:00" itemprop="dateCreated datePublished" datetime="2024-04-11T16:00:00+08:00">2024-04-11</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="修改时间：2024-07-22 17:14:59" itemprop="dateModified" datetime="2024-07-22T17:14:59+08:00">2024-07-22</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">NLP</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">大模型</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>我们以下面的这一句语句作为开始，以从本地加载模型为例。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> AutoModelForSeq2SeqLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"bigscience/T0_3B"</span><span class="token punctuation">)</span>
inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>q<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">" ? To answer this question, we need to know"</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>
outputs <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>inputs<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> max_new_tokens<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> do_sample<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> top_k<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h1 id="加载">加载</h1>
<p>AutoModelForSeq2SeqLM继承了_BaseAutoModelClass，这个类是所有AutoModel的基类，保存在transformers/models/auto/auto_factory.py中。调用的from_pretrained方法实际上就来自于这个基类。我们假设模型保存在本地，一些下载的逻辑不看，且kwargs和config为None。最终会得到模型的哈希值。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>config<span class="token punctuation">,</span> PretrainedConfig<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># We make a call to the config file first (which may be absent) to get the commit hash as soon as possible</span>
    resolved_config_file <span class="token operator">=</span> cached_file<span class="token punctuation">(</span>
        pretrained_model_name_or_path<span class="token punctuation">,</span>
        CONFIG_NAME<span class="token punctuation">,</span>
        _raise_exceptions_for_gated_repo<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        _raise_exceptions_for_missing_entries<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        _raise_exceptions_for_connection_errors<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        <span class="token operator">**</span>hub_kwargs<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    commit_hash <span class="token operator">=</span> extract_commit_hash<span class="token punctuation">(</span>resolved_config_file<span class="token punctuation">,</span> commit_hash<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>首先需要加载config，通过cached_file来加载。CONFIG_NAME默认为config.json，pretrained_model_name_or_path则是from_pretrained传入的字符串。再看具体实现。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">path_or_repo_id <span class="token operator">=</span> <span class="token builtin">str</span><span class="token punctuation">(</span>path_or_repo_id<span class="token punctuation">)</span> <span class="token comment"># "bigscience/T0_3B"</span>
    full_filename <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>subfolder<span class="token punctuation">,</span> filename<span class="token punctuation">)</span> <span class="token comment"># filename=config.json</span>
    <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span>path_or_repo_id<span class="token punctuation">)</span><span class="token punctuation">:</span>
        resolved_file <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>path_or_repo_id<span class="token punctuation">,</span> subfolder<span class="token punctuation">)</span><span class="token punctuation">,</span> filename<span class="token punctuation">)</span> <span class="token comment"># subfolder不指定=None</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isfile<span class="token punctuation">(</span>resolved_file<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> _raise_exceptions_for_missing_entries<span class="token punctuation">:</span>
                <span class="token keyword">raise</span> EnvironmentError<span class="token punctuation">(</span>
                    <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>path_or_repo_id<span class="token punctuation">&#125;</span></span><span class="token string"> does not appear to have a file named </span><span class="token interpolation"><span class="token punctuation">&#123;</span>full_filename<span class="token punctuation">&#125;</span></span><span class="token string">. Checkout "</span></span>
                    <span class="token string-interpolation"><span class="token string">f"'https://huggingface.co/</span><span class="token interpolation"><span class="token punctuation">&#123;</span>path_or_repo_id<span class="token punctuation">&#125;</span></span><span class="token string">/tree/</span><span class="token interpolation"><span class="token punctuation">&#123;</span>revision<span class="token punctuation">&#125;</span></span><span class="token string">' for available files."</span></span>
                <span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token keyword">return</span> <span class="token boolean">None</span>
        <span class="token keyword">return</span> resolved_file <span class="token comment"># 返回bigscience/T0_3B/config.json</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>此时对应的config.json已经被加载到内存中，之后需要加载到AutoConfig中。可以看到就是T0_3B/config.json</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">            config<span class="token punctuation">,</span> kwargs <span class="token operator">=</span> AutoConfig<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
                pretrained_model_name_or_path<span class="token punctuation">,</span>
                return_unused_kwargs<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                trust_remote_code<span class="token operator">=</span>trust_remote_code<span class="token punctuation">,</span>
                code_revision<span class="token operator">=</span>code_revision<span class="token punctuation">,</span>
                _commit_hash<span class="token operator">=</span>commit_hash<span class="token punctuation">,</span>
                <span class="token operator">**</span>hub_kwargs<span class="token punctuation">,</span>
                <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>config<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
T5Config <span class="token punctuation">&#123;</span>
  <span class="token string">"_name_or_path"</span><span class="token punctuation">:</span> <span class="token string">"/data2/wtf/model/bigscience/T0_3B"</span><span class="token punctuation">,</span>
  <span class="token string">"architectures"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>
    <span class="token string">"T5ForConditionalGeneration"</span>
  <span class="token punctuation">]</span><span class="token punctuation">,</span>
  <span class="token string">"classifier_dropout"</span><span class="token punctuation">:</span> <span class="token number">0.0</span><span class="token punctuation">,</span>
  <span class="token string">"d_ff"</span><span class="token punctuation">:</span> <span class="token number">5120</span><span class="token punctuation">,</span>
  <span class="token string">"d_kv"</span><span class="token punctuation">:</span> <span class="token number">64</span><span class="token punctuation">,</span>
  <span class="token string">"d_model"</span><span class="token punctuation">:</span> <span class="token number">2048</span><span class="token punctuation">,</span>
  <span class="token string">"decoder_start_token_id"</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
  <span class="token string">"dense_act_fn"</span><span class="token punctuation">:</span> <span class="token string">"gelu_new"</span><span class="token punctuation">,</span>
  <span class="token string">"dropout_rate"</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span>
  <span class="token string">"eos_token_id"</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
  <span class="token string">"feed_forward_proj"</span><span class="token punctuation">:</span> <span class="token string">"gated-gelu"</span><span class="token punctuation">,</span>
  <span class="token string">"gradient_checkpointing"</span><span class="token punctuation">:</span> false<span class="token punctuation">,</span>
  <span class="token string">"initializer_factor"</span><span class="token punctuation">:</span> <span class="token number">1.0</span><span class="token punctuation">,</span>
  <span class="token string">"is_encoder_decoder"</span><span class="token punctuation">:</span> true<span class="token punctuation">,</span>
  <span class="token string">"is_gated_act"</span><span class="token punctuation">:</span> true<span class="token punctuation">,</span>
  <span class="token string">"layer_norm_epsilon"</span><span class="token punctuation">:</span> <span class="token number">1e-06</span><span class="token punctuation">,</span>
  <span class="token string">"model_type"</span><span class="token punctuation">:</span> <span class="token string">"t5"</span><span class="token punctuation">,</span>
  <span class="token string">"num_decoder_layers"</span><span class="token punctuation">:</span> <span class="token number">24</span><span class="token punctuation">,</span>
  <span class="token string">"num_heads"</span><span class="token punctuation">:</span> <span class="token number">32</span><span class="token punctuation">,</span>
  <span class="token string">"num_layers"</span><span class="token punctuation">:</span> <span class="token number">24</span><span class="token punctuation">,</span>
  <span class="token string">"output_past"</span><span class="token punctuation">:</span> true<span class="token punctuation">,</span>
  <span class="token string">"pad_token_id"</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
  <span class="token string">"relative_attention_max_distance"</span><span class="token punctuation">:</span> <span class="token number">128</span><span class="token punctuation">,</span>
  <span class="token string">"relative_attention_num_buckets"</span><span class="token punctuation">:</span> <span class="token number">32</span><span class="token punctuation">,</span>
  <span class="token string">"tie_word_embeddings"</span><span class="token punctuation">:</span> false<span class="token punctuation">,</span>
  <span class="token string">"transformers_version"</span><span class="token punctuation">:</span> <span class="token string">"4.38.2"</span><span class="token punctuation">,</span>
  <span class="token string">"use_cache"</span><span class="token punctuation">:</span> true<span class="token punctuation">,</span>
  <span class="token string">"vocab_size"</span><span class="token punctuation">:</span> <span class="token number">32128</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>最终将模型加载，返回model实例。model_class就是json中的architectures对应值。_get_model_class方法就是得到模型的类型！</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">          model_class <span class="token operator">=</span> _get_model_class<span class="token punctuation">(</span>config<span class="token punctuation">,</span> cls<span class="token punctuation">.</span>_model_mapping<span class="token punctuation">)</span> <span class="token comment"># T5ForConditionalGeneration</span>
  		<span class="token triple-quoted-string string">"""
  		"architectures": [
  "T5ForConditionalGeneration"
]
			就是返回arch中的模型类型
  		"""</span>
          <span class="token keyword">return</span> model_class<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
              pretrained_model_name_or_path<span class="token punctuation">,</span> <span class="token operator">*</span>model_args<span class="token punctuation">,</span> config<span class="token operator">=</span>config<span class="token punctuation">,</span> <span class="token operator">**</span>hub_kwargs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs
          <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>上面的<code>cls._model_mapping</code>就是模型根据你的输入，得到当前模型类型的映射。</p>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">class AutoModelForSeq2SeqLM(_BaseAutoModelClass):
    _model_mapping &#x3D; MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING
------
MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING &#x3D; _LazyAutoMapping(
    CONFIG_MAPPING_NAMES, MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES
)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>CONFIG_MAPPING_NAMES</code>是根据你传入的路径来匹配，而<code>MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES</code>同理，也有和下图类似的字典。本例中<code>CONFIG_MAPPING_NAMES="T5Config"</code>，<code>MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES=T5ForConditionalGeneration</code>，和config.json中的一样。</p>
<figure>
<img src="../images/image-20240422014943359.png"
alt="image-20240422014943359" />
<figcaption aria-hidden="true">image-20240422014943359</figcaption>
</figure>
<p><code>model_class.from_pretrained</code>中的<code>from_pretrained</code>来自于PretrainedModel类，这是所有模型的基类(注意不是AutoModel)。这个函数是本篇的核心。返回的模型实例默认是开启model.eval()模式，若要微调or训练模型，需要手动指定model.train()。</p>
<p>这里顺便提一嘴model.train和eval下的区别：</p>
<ol type="1">
<li>Dropout 和 BatchNorm 行为。model.train()下 Dropout
层会随机丢弃一部分神经元, BatchNorm 层会计算当前 batch 的统计量。
model.eval()下Dropout 层会全部保留神经元, BatchNorm
层会使用训练好的滑动平均统计量。</li>
<li>梯度与优化器。model.train()会计算梯度并更新模型参数。model.eval()不会计算梯度,
也不会更新模型参数。</li>
<li>数据增强。model.train()通常会应用一些数据增强技术,
如翻转、旋转等。model.eval()一般不需要数据增强,
直接使用原始的输入数据。</li>
<li>内存与计算开销。model.train()需要保存中间激活值用于反向传播,
计算开销相对更大。model.eval()只需要前向传播, 不需要保存中间激活值,
计算开销相对更小。</li>
</ol>
<p>下面看几个比较关键的参数。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token decorator annotation punctuation">@classmethod</span>
<span class="token keyword">def</span> <span class="token function">from_pretrained</span><span class="token punctuation">(</span>
    cls<span class="token punctuation">,</span>
    pretrained_model_name_or_path<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> os<span class="token punctuation">.</span>PathLike<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token operator">*</span>model_args<span class="token punctuation">,</span>
    config<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span>PretrainedConfig<span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">,</span> os<span class="token punctuation">.</span>PathLike<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    cache_dir<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> os<span class="token punctuation">.</span>PathLike<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    ignore_mismatched_sizes<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    force_download<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    local_files_only<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    token<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Union<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> <span class="token builtin">bool</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    revision<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">"main"</span><span class="token punctuation">,</span>
    use_safetensors<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
<span class="token punctuation">)</span><span class="token punctuation">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><p>pretrained_model_name_or_path，模型的路径or在huggingface中的名字。</p></li>
<li><p>force_download，不论有没有下载好模型，都下载，若存在则覆盖。</p></li>
<li><p>torch_dtype，<code>torch.float16</code> or
<code>torch.bfloat16</code> or
<code>torch.float</code>，指定模型参数的载入精度，不指定则默认为<code>torch.float</code>。也就是说，<code>config.json</code>
中的 <code>torch_dtype</code> 设置拥有最高优先级。如果
<code>torch_dtype</code> 参数被设置为
<code>"auto"</code>，那么它会首先使用 <code>config.json</code>
中的设置。只有当 <code>config.json</code> 中没有找到
<code>torch_dtype</code> 且 <code>torch_dtype</code> 参数被设置为
<code>"auto"</code>时，它才会回退到使用权重checkpoint中的数据类型，查看第一个数据是什么类型就用什么类型。若根本没有设置该参数，则使用torch.float。</p></li>
<li><p>device_map，可以传入三种类型的参数。<strong>字符串类型</strong>，如果传入一个字符串类型的设备名称(例如
"cpu", "cuda:1", "mps")，那么整个模型会被分配到指定的设备上，如果传入
"auto"，Accelerate
库会自动计算出最优的设备分布。<strong>字典类型</strong>，这种情况下
device_map
是一个字典,键是模型的子模块名称,值是对应的设备编号或设备名称，这允许用户手动指定模型的各个子模块应该分布在哪些设备上，只需要指定到模块名称的级别,子模块会自动分配到同一设备，如</p>
<pre class="line-numbers language-none"><code class="language-none">device_map &#x3D; &#123;
    &quot;transformer.encoder&quot;: &quot;cuda:0&quot;,
    &quot;transformer.decoder&quot;: &quot;cuda:1&quot;,
    &quot;transformer.pooler&quot;: &quot;cuda:0&quot;,
    &quot;lm_head&quot;: &quot;cuda:1&quot;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>还可以传入整数或<code>torch.device</code>，代表将整个模型放在指定编号的
GPU
上。如<code>device = torch.device("cuda:1"),device_map = deveice</code>。只要指定了device_map，那么都会让
<code>low_cpu_mem_usage=True</code>。不指定就用cpu。</p></li>
<li><p>quantization_config，指定模型的量化策略。可以是一个字典或者继承自
<code>QuantizationConfigMixin</code>
的对象，它用于配置模型的量化参数。除了 <code>quantization_config</code>
之外,还可以使用 <code>load_in_4bit</code> 和 <code>load_in_8bit</code>
等参数来指定量化方式,但这种方式不被推荐，只量化了参数，并不量化梯度。但推理阶段无所谓。下面是一个例子。</p></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> bitsandbytes <span class="token keyword">as</span> bnb
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> QuantizationConfig

quantization_config <span class="token operator">=</span> QuantizationConfig<span class="token punctuation">(</span>
    quantization_method<span class="token operator">=</span>bnb<span class="token punctuation">.</span>QuantizationMethod<span class="token punctuation">.</span>DYNAMIC_QUANT<span class="token punctuation">,</span>
    weight_bits<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token comment"># 权重为INT8</span>
    grad_bits<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token comment"># 梯度也INT8</span>
    per_channel<span class="token operator">=</span><span class="token boolean">False</span>
<span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForSeq2SeqLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    <span class="token string">"bigscience/T0_3B"</span><span class="token punctuation">,</span>
    quantization_config<span class="token operator">=</span>quantization_config
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>local_files_only，如果是True，则不会从Hub上下载。</li>
<li>low_cpu_mem_usage，作用是尝试在加载模型时不使用超过模型大小 1 倍的
CPU 内存(包括峰值内存)。</li>
<li>attn_implementation，可以选择<code>flash_attention_2</code>,<code>sdpa(default)</code>,<code>eager(手动实现)</code></li>
</ul>
<p>之后看几处比较关键的源码。</p>
<p>从传入的路径中提取config。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Load config if we don't provide a configuration</span>
<span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>config<span class="token punctuation">,</span> PretrainedConfig<span class="token punctuation">)</span><span class="token punctuation">:</span>
    config_path <span class="token operator">=</span> config <span class="token keyword">if</span> config <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> pretrained_model_name_or_path
    config<span class="token punctuation">,</span> model_kwargs <span class="token operator">=</span> cls<span class="token punctuation">.</span>config_class<span class="token punctuation">.</span>from_pretrained<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>量化操作。注意到量化操作会强制开启low_cpu_mem_usage。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pre_quantized <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>config<span class="token punctuation">,</span> <span class="token string">"quantization_config"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
<span class="token keyword">if</span> pre_quantized <span class="token keyword">or</span> quantization_config <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> pre_quantized<span class="token punctuation">:</span>
        config<span class="token punctuation">.</span>quantization_config <span class="token operator">=</span> AutoHfQuantizer<span class="token punctuation">.</span>merge_quantization_configs<span class="token punctuation">(</span>
            config<span class="token punctuation">.</span>quantization_config<span class="token punctuation">,</span> quantization_config
        <span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        config<span class="token punctuation">.</span>quantization_config <span class="token operator">=</span> quantization_config
    hf_quantizer <span class="token operator">=</span> AutoHfQuantizer<span class="token punctuation">.</span>from_config<span class="token punctuation">(</span>config<span class="token punctuation">.</span>quantization_config<span class="token punctuation">,</span> pre_quantized<span class="token operator">=</span>pre_quantized<span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    hf_quantizer <span class="token operator">=</span> <span class="token boolean">None</span>

<span class="token keyword">if</span> hf_quantizer <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    hf_quantizer<span class="token punctuation">.</span>validate_environment<span class="token punctuation">(</span>
        torch_dtype<span class="token operator">=</span>torch_dtype<span class="token punctuation">,</span> from_tf<span class="token operator">=</span>from_tf<span class="token punctuation">,</span> from_flax<span class="token operator">=</span>from_flax<span class="token punctuation">,</span> device_map<span class="token operator">=</span>device_map
    <span class="token punctuation">)</span>
    torch_dtype <span class="token operator">=</span> hf_quantizer<span class="token punctuation">.</span>update_torch_dtype<span class="token punctuation">(</span>torch_dtype<span class="token punctuation">)</span>
    device_map <span class="token operator">=</span> hf_quantizer<span class="token punctuation">.</span>update_device_map<span class="token punctuation">(</span>device_map<span class="token punctuation">)</span>

    <span class="token comment"># Force-set to `True` for more mem efficiency</span>
    <span class="token keyword">if</span> low_cpu_mem_usage <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        low_cpu_mem_usage <span class="token operator">=</span> <span class="token boolean">True</span>
        logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span><span class="token string">"`low_cpu_mem_usage` was None, now set to True since model is quantized."</span><span class="token punctuation">)</span>
is_quantized <span class="token operator">=</span> hf_quantizer <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>加载权重，tf相关的就不看了。在加载pytorch权重中，会去你指定的文件夹中找<code>pytorch_model.bin</code>这个权重文件。<code>subfolder,variant</code>若不在参数中指定都为空字符。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">elif</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isfile<span class="token punctuation">(</span>
    os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>pretrained_model_name_or_path<span class="token punctuation">,</span> subfolder<span class="token punctuation">,</span> _add_variant<span class="token punctuation">(</span>WEIGHTS_NAME<span class="token punctuation">,</span> variant<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Load from a PyTorch checkpoint,会拼成model_path/pytorch_model.bin</span>
    archive_file <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>
        pretrained_model_name_or_path<span class="token punctuation">,</span> subfolder<span class="token punctuation">,</span> _add_variant<span class="token punctuation">(</span>WEIGHTS_NAME<span class="token punctuation">,</span> variant<span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>一些模型的权重可能以多个checkpoint文件来保存，这时候要求有一个<code>WEIGHTS_INDEX_NAME = "pytorch_model.bin.index.json"</code>文件来进行索引。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">                <span class="token keyword">elif</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isfile<span class="token punctuation">(</span>
                    os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>pretrained_model_name_or_path<span class="token punctuation">,</span> subfolder<span class="token punctuation">,</span> _add_variant<span class="token punctuation">(</span>WEIGHTS_INDEX_NAME<span class="token punctuation">,</span> variant<span class="token punctuation">)</span><span class="token punctuation">)</span>
                <span class="token punctuation">)</span><span class="token punctuation">:</span>
                    <span class="token comment"># Load from a sharded PyTorch checkpoint</span>
                    archive_file <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>
                        pretrained_model_name_or_path<span class="token punctuation">,</span> subfolder<span class="token punctuation">,</span> _add_variant<span class="token punctuation">(</span>WEIGHTS_INDEX_NAME<span class="token punctuation">,</span> variant<span class="token punctuation">)</span>
                    <span class="token punctuation">)</span>
                    is_sharded <span class="token operator">=</span> <span class="token boolean">True</span> <span class="token comment"># 注意这里</span>
<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
<span class="token punctuation">&#123;</span>
    <span class="token string">"checkpoint_files"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"pytorch_model.bin.0"</span><span class="token punctuation">,</span> <span class="token string">"pytorch_model.bin.1"</span><span class="token punctuation">,</span> <span class="token string">"pytorch_model.bin.2"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"num_checkpoint_files"</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span>
    <span class="token string">"size_checkpoint_files"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">100000</span><span class="token punctuation">,</span> <span class="token number">200000</span><span class="token punctuation">,</span> <span class="token number">50000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"weight_map"</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span>
        <span class="token string">"layer1.weight"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">"layer1.bias"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">50000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">"layer2.weight"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">"layer2.bias"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">100000</span><span class="token punctuation">]</span>
    <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>还有一种情况，就是指定的路径不是一个文件夹，而是权重文件本身，如<code>bigscience/T0_3B/pytorch_model.bin</code>，那也可以加载。因为最终都是让<code>archive_file = weight_file</code>。</p>
<pre class="line-numbers language-none"><code class="language-none">elif os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):
    archive_file &#x3D; pretrained_model_name_or_path
    is_local &#x3D; True<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>最终，<code>resolved_archive_file = archive_file</code>，获取权重文件路径。如果是分散的checkpoint，也就是<code>is_sharded</code>是True，还要进行额外的操作，这里就不深入了。</p>
<p>接下来就要加载权重了，首先判断是不是pytorch，若是，则加载权重文件。详细的加载源码就不赘述，最终会返回由torch.load加载模型结构和权重参数。</p>
<pre class="line-numbers language-none"><code class="language-none">if from_pt:
    if not is_sharded and state_dict is None:
        # Time to load the checkpoint
        state_dict &#x3D; load_state_dict(resolved_archive_file)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>接下来决定权重的数据类型，正如上面交代torch_dtype参数所说，先考虑<code>torch_dtype=auto</code>，也就是config.json中的数据类型。然后再考虑</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> torch_dtype <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>torch_dtype<span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> torch_dtype <span class="token operator">==</span> <span class="token string">"auto"</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>config<span class="token punctuation">,</span> <span class="token string">"torch_dtype"</span><span class="token punctuation">)</span> <span class="token keyword">and</span> config<span class="token punctuation">.</span>torch_dtype <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                torch_dtype <span class="token operator">=</span> config<span class="token punctuation">.</span>torch_dtype
                logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Will use torch_dtype=</span><span class="token interpolation"><span class="token punctuation">&#123;</span>torch_dtype<span class="token punctuation">&#125;</span></span><span class="token string"> as defined in model's config object"</span></span><span class="token punctuation">)</span>

        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
                <span class="token string-interpolation"><span class="token string">f'`torch_dtype` can be either `torch.dtype` or `"auto"`, but received </span><span class="token interpolation"><span class="token punctuation">&#123;</span>torch_dtype<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span>
            <span class="token punctuation">)</span>
    dtype_orig <span class="token operator">=</span> cls<span class="token punctuation">.</span>_set_default_torch_dtype<span class="token punctuation">(</span>torch_dtype<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>若是分片情况，则去分片json中找有没有指定。如果不是分片的情况，则按权重文件中第一个数据的类型。若不显式指定torch_dtype(None)，则使用float32。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">                    <span class="token keyword">else</span><span class="token punctuation">:</span>
                        <span class="token keyword">if</span> is_sharded <span class="token keyword">and</span> <span class="token string">"dtype"</span> <span class="token keyword">in</span> sharded_metadata<span class="token punctuation">:</span>
                            torch_dtype <span class="token operator">=</span> sharded_metadata<span class="token punctuation">[</span><span class="token string">"dtype"</span><span class="token punctuation">]</span>
                        <span class="token keyword">elif</span> <span class="token keyword">not</span> is_sharded<span class="token punctuation">:</span>
                            torch_dtype <span class="token operator">=</span> get_state_dict_dtype<span class="token punctuation">(</span>state_dict<span class="token punctuation">)</span>
                        <span class="token keyword">else</span><span class="token punctuation">:</span>
                            one_state_dict <span class="token operator">=</span> load_state_dict<span class="token punctuation">(</span>resolved_archive_file<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
                            torch_dtype <span class="token operator">=</span> get_state_dict_dtype<span class="token punctuation">(</span>one_state_dict<span class="token punctuation">)</span>
                            <span class="token keyword">del</span> one_state_dict  <span class="token comment"># free CPU memory</span>
                        logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span>
                            <span class="token string">"Since the `torch_dtype` attribute can't be found in model's config object, "</span>
                            <span class="token string">"will use torch_dtype=&#123;torch_dtype&#125; as derived from model's weights"</span>
                        <span class="token punctuation">)</span>    
<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>get_state_dict_dtype<span class="token punctuation">(</span>state_dict<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
<span class="token comment"># if no floating dtype was found return whatever the first dtype is</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token builtin">next</span><span class="token punctuation">(</span>state_dict<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>dtype<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>还有混合精度的情况，在初始nn.Module的时候可以设置单独设置_keep_in_fp32_modules哪些模块保持fp32精度。</p>
<pre class="line-numbers language-none"><code class="language-none"># Check if &#96;_keep_in_fp32_modules&#96; is not None
use_keep_in_fp32_modules &#x3D; (cls._keep_in_fp32_modules is not None) and (
    (torch_dtype &#x3D;&#x3D; torch.float16) or hasattr(hf_quantizer, &quot;use_keep_in_fp32_modules&quot;)
)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p>创建模型实例。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">with</span> ContextManagers<span class="token punctuation">(</span>init_contexts<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Let's make sure we don't run the init function of buffer modules</span>
    model <span class="token operator">=</span> cls<span class="token punctuation">(</span>config<span class="token punctuation">,</span> <span class="token operator">*</span>model_args<span class="token punctuation">,</span> <span class="token operator">**</span>model_kwargs<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>来看device_map的逻辑。首先是字符串的情况，必须要是<code>"auto", "balanced", "balanced_low_0", "sequential"</code>这几种，否则报错。"auto"会自动操作，尽可能均匀地分配计算负载。<code>balanced</code>则是平均分配模型层中的参数给不同的卡。<code>balanced_low_0</code>则是少给0分一些，因为0往往还有其他事情要做。<code>sequential</code>则是按模型层的顺序来分配给不同的卡，保持模型层的拓扑结构,减少跨设备的数据传输，如attention一张卡，MLP一张卡。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> device_map <span class="token keyword">not</span> <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"auto"</span><span class="token punctuation">,</span> <span class="token string">"balanced"</span><span class="token punctuation">,</span> <span class="token string">"balanced_low_0"</span><span class="token punctuation">,</span> <span class="token string">"sequential"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> ValueError
<span class="token keyword">if</span> device_map <span class="token operator">!=</span> <span class="token string">"sequential"</span><span class="token punctuation">:</span>
                max_memory <span class="token operator">=</span> get_balanced_memory<span class="token punctuation">(</span>
                    model<span class="token punctuation">,</span>
                    dtype<span class="token operator">=</span>target_dtype<span class="token punctuation">,</span>
                    low_zero<span class="token operator">=</span><span class="token punctuation">(</span>device_map <span class="token operator">==</span> <span class="token string">"balanced_low_0"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    max_memory<span class="token operator">=</span>max_memory<span class="token punctuation">,</span>
                    <span class="token operator">**</span>device_map_kwargs<span class="token punctuation">,</span>
                <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>实际上可以看到，<code>auto</code>就是<code>balanced</code>策略。</p>
<p>其他情况，输入的什么设备就绑定什么设备，若没有指定device_map，就加载到cpu。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>device_map<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">:</span>
     device_map <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">""</span><span class="token punctuation">:</span> device_map<span class="token punctuation">&#125;</span>
<span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>device_map<span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token keyword">and</span> device_map <span class="token keyword">not</span> <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"auto"</span><span class="token punctuation">,</span> <span class="token string">"balanced"</span><span class="token punctuation">,</span> <span class="token string">"balanced_low_0"</span><span class="token punctuation">,</span> <span class="token string">"sequential"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
      <span class="token keyword">try</span><span class="token punctuation">:</span>
          device_map <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">""</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span>device_map<span class="token punctuation">)</span><span class="token punctuation">&#125;</span>
<span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>device_map<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment"># 小于0报错</span>
	device_map <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">""</span><span class="token punctuation">:</span> device_map<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>这里有一个<code>tie_weights</code>函数，实现了参数的绑定操作，本质上就是默认让输入嵌入层和输出嵌入层的权重绑定在一起。若是在config中指定<code>is_encoder_decoder=True</code>且<code>tie_encoder_decoder=True</code>，那么Encoder和Decoder的参数也会共用(都使用Decoder的Weights)，不过T5中并不这么做，一般是<strong>BERT-based</strong>模型在微调成Encoder-Decoder模型的时候会这么做。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">tie_weights</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Tie the weights between the input embeddings and the output embeddings.

    If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the
    weights instead.
    """</span>
    <span class="token keyword">if</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">,</span> <span class="token string">"tie_word_embeddings"</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        output_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>get_output_embeddings<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> output_embeddings <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>_tie_or_clone_weights<span class="token punctuation">(</span>output_embeddings<span class="token punctuation">,</span> self<span class="token punctuation">.</span>get_input_embeddings<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">,</span> <span class="token string">"is_encoder_decoder"</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">,</span> <span class="token string">"tie_encoder_decoder"</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> self<span class="token punctuation">.</span>base_model_prefix<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> self<span class="token punctuation">.</span>base_model_prefix<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_tie_encoder_decoder_weights<span class="token punctuation">(</span>self<span class="token punctuation">.</span>encoder<span class="token punctuation">,</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">,</span> self<span class="token punctuation">.</span>base_model_prefix<span class="token punctuation">)</span>

    <span class="token keyword">for</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> <span class="token string">"_tie_weights"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            module<span class="token punctuation">.</span>_tie_weights<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>开启model.eval。</p>
<pre class="line-numbers language-none"><code class="language-none">model.eval()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>若模型是生成式模型，那么还需要配置生成的参数。<code>GenerationConfig</code>实际上就是model.generate()方法中所要用的参数。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> model<span class="token punctuation">.</span>can_generate<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">and</span> pretrained_model_name_or_path <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    <span class="token keyword">try</span><span class="token punctuation">:</span>
        model<span class="token punctuation">.</span>generation_config <span class="token operator">=</span> GenerationConfig<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
            pretrained_model_name_or_path<span class="token punctuation">,</span>
            cache_dir<span class="token operator">=</span>cache_dir<span class="token punctuation">,</span>
            force_download<span class="token operator">=</span>force_download<span class="token punctuation">,</span>
            resume_download<span class="token operator">=</span>resume_download<span class="token punctuation">,</span>
            proxies<span class="token operator">=</span>proxies<span class="token punctuation">,</span>
            local_files_only<span class="token operator">=</span>local_files_only<span class="token punctuation">,</span>
            token<span class="token operator">=</span>token<span class="token punctuation">,</span>
            revision<span class="token operator">=</span>revision<span class="token punctuation">,</span>
            subfolder<span class="token operator">=</span>subfolder<span class="token punctuation">,</span>
            _from_auto<span class="token operator">=</span>from_auto_class<span class="token punctuation">,</span>
            _from_pipeline<span class="token operator">=</span>from_pipeline<span class="token punctuation">,</span>
            <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
    <span class="token keyword">except</span> OSError<span class="token punctuation">:</span>
        logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span>
            <span class="token string">"Generation config file not found, using a generation config created from the model config."</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>最终输出一些加载参数时输出的信息，然后返回模型。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> output_loading_info<span class="token punctuation">:</span>
    <span class="token keyword">if</span> loading_info <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        loading_info <span class="token operator">=</span> <span class="token punctuation">&#123;</span>
            <span class="token string">"missing_keys"</span><span class="token punctuation">:</span> missing_keys<span class="token punctuation">,</span>
            <span class="token string">"unexpected_keys"</span><span class="token punctuation">:</span> unexpected_keys<span class="token punctuation">,</span>
            <span class="token string">"mismatched_keys"</span><span class="token punctuation">:</span> mismatched_keys<span class="token punctuation">,</span>
            <span class="token string">"error_msgs"</span><span class="token punctuation">:</span> error_msgs<span class="token punctuation">,</span>
        <span class="token punctuation">&#125;</span>
    <span class="token keyword">return</span> model<span class="token punctuation">,</span> loading_info<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>总结，根据最常用的方法，主要是做以下几个操作。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> AutoModelForSeq2SeqLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"bigscience/T0_3B"</span><span class="token punctuation">,</span>device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<ol type="1">
<li>根据输入路径拿到config.json，加载到Config。</li>
<li>根据输入路径拿到权重文件pytorch_model.bin，由torch.load加载模型结构和权重参数。。</li>
<li>决定权重的数据类型，未指定则是float32。</li>
<li>平均分配参数给各张卡。</li>
<li>绑定input和output的Embedding，让其使用同一份Embedding参数。</li>
<li>model.eval()。</li>
<li>若是生成式模型，配置生成参数。</li>
<li>返回模型实例。</li>
</ol>
<p>所以，实际上会调用两个不同的<code>from_pretrained</code>方法，第一个是AutoModel基类_BaseAutoModelClass的，在最后调用<code>get_model_class</code>方法得到模型本身的类，本例中是<code>T5ForConditionalGeneration</code>，然后再调用这个类的<code>from_pretrained</code>，而这个类的<code>from_pretrained</code>在其基类<code>PreTrainedModel</code>实现，所以再会调用<code>PreTrainedModel</code>的<code>from_pretrained</code>方法。分析完毕。</p>
<h1 id="分词">分词</h1>
<p>后续更新，挖坑</p>
<h1 id="生成">生成</h1>
<p>并不是每一个模型都可以使用.generate()进行序列生成，需要通过函数判断是否能够进行序列生成任务，所以每一个模型都需要重写<code>prepare_inputs_for_generation</code>方法。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PreTrainedModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">,</span> ModuleUtilsMixin<span class="token punctuation">,</span> GenerationMixin<span class="token punctuation">,</span> PushToHubMixin<span class="token punctuation">,</span> PeftAdapterMixin<span class="token punctuation">)</span><span class="token punctuation">:</span>
	    <span class="token decorator annotation punctuation">@classmethod</span>
    <span class="token keyword">def</span> <span class="token function">can_generate</span><span class="token punctuation">(</span>cls<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">bool</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Returns whether this model can generate sequences with `.generate()`.

        Returns:
            `bool`: Whether this model can generate sequences with `.generate()`.
        """</span>
        <span class="token comment"># Detects whether `prepare_inputs_for_generation` has been overwritten, which is a requirement for generation.</span>
        <span class="token comment"># Alternativelly, the model can also have a custom `generate` function.</span>
        <span class="token keyword">if</span> <span class="token string">"GenerationMixin"</span> <span class="token keyword">in</span> <span class="token builtin">str</span><span class="token punctuation">(</span>cls<span class="token punctuation">.</span>prepare_inputs_for_generation<span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token string">"GenerationMixin"</span> <span class="token keyword">in</span> <span class="token builtin">str</span><span class="token punctuation">(</span>cls<span class="token punctuation">.</span>generate<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> <span class="token boolean">False</span>
        <span class="token keyword">return</span> <span class="token boolean">True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>而generate方法本身就在<code>GenerationMixin</code>类中实现。</p>
<pre class="line-numbers language-none"><code class="language-none">Class that holds a configuration for a generation task. A &#96;generate&#96; call supports the following generation methods
for text-decoder, text-to-text, speech-to-text, and vision-to-text models:

    - *greedy decoding* by calling [&#96;~generation.GenerationMixin._greedy_search&#96;] if &#96;num_beams&#x3D;1&#96; and
        &#96;do_sample&#x3D;False&#96;
    - *contrastive search* by calling [&#96;~generation.GenerationMixin._contrastive_search&#96;] if &#96;penalty_alpha&gt;0.&#96;
        and &#96;top_k&gt;1&#96;
    - *multinomial sampling* by calling [&#96;~generation.GenerationMixin._sample&#96;] if &#96;num_beams&#x3D;1&#96; and
        &#96;do_sample&#x3D;True&#96;
    - *beam-search decoding* by calling [&#96;~generation.GenerationMixin._beam_search&#96;] if &#96;num_beams&gt;1&#96; and
        &#96;do_sample&#x3D;False&#96;
    - *beam-search multinomial sampling* by calling [&#96;~generation.GenerationMixin._beam_sample&#96;] if
        &#96;num_beams&gt;1&#96; and &#96;do_sample&#x3D;True&#96;
    - *diverse beam-search decoding* by calling [&#96;~generation.GenerationMixin._group_beam_search&#96;], if
        &#96;num_beams&gt;1&#96; and &#96;num_beam_groups&gt;1&#96;
    - *constrained beam-search decoding* by calling [&#96;~generation.GenerationMixin._constrained_beam_search&#96;], if
        &#96;constraints!&#x3D;None&#96; or &#96;force_words_ids!&#x3D;None&#96;
    - *assisted decoding* by calling [&#96;~generation.GenerationMixin._assisted_decoding&#96;], if
        &#96;assistant_model&#96; or &#96;prompt_lookup_num_tokens&#96; is passed to &#96;.generate()&#96;

You do not need to call any of the above methods directly. Pass custom parameter values to &#39;.generate()&#39;. To learn
more about decoding strategies refer to the [text generation strategies guide](..&#x2F;generation_strategies).<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>就不看详细的实现，先看<code>GenerationConfig</code>的生成策略。若不用GenerationConfig，也可以直接输入kwargs。</p>
<ul>
<li>greedy
decoding贪婪解码，<code>num_beams=1</code>且<code>do_sample=False</code>，会一直选择概率最高的token，一条路走到黑。</li>
<li>Contrastive
search对比搜索，在NIPS22被提出，能在保持流畅性的前提下，鼓励多样性生成，减少重复输出。需要<code>penalty_alpha&gt;0</code>
and
<code>top_k&gt;1</code>。一个候选token与当前token非常相似(相似度得分高)，那么它的概率就会被较多地降低。这样做的目的是鼓励生成更加多样化的文本，避免同类型的token过于集中出现。最后,算法在经过调整的
<code>scores</code>
向量上取Top-1。核心公式<code>scores = (1.0 - alpha) * next_top_k_probs - alpha * scores</code>。<code>next_top_k_probs</code>是当前token的Top-k概率分布，等式右边的<code>scores</code>是当前token和下一个token之间的相似度分数。所以当前token与next
token越相似，惩罚就越大。</li>
<li>multinomial sampling，<code>num_beams=1</code>
and<code>do_sample=True</code>。和贪婪解码的区别不一定选择概率最高的token，而是根据概率分布来采样。</li>
<li>beam-search，<code>num_beams&gt;1</code>
and<code>do_sample=False</code>，保留top-k个得分最高的候选序列，称为"beam"。这里选择不采样，是选择得分最高的2*num_beams个token。</li>
<li>diverse beam-search，<code>num_beams&gt;1</code> and
<code>num_beam_groups&gt;1</code>，通过分组机制，确保了不同beam之间的差异性。</li>
</ul>
<p>接下来介绍一些比较常用的参数。</p>
<ul>
<li><code>do_sample</code>，是否根据概率分布采样。</li>
<li><code>temperature</code>，默认1.0。小于1时，当
<code>temperature &lt; 1.0</code> 时,
生成概率分布会被"平滑"(峰值变得更陡峭)，使得模型更倾向于选择概率较高的token，生成的文本会更加集中和保守。当<code>temperature &gt; 1.0</code>时，
生成概率分布会被"拉平"(峰值变得更平缓)，使得模型会选择概率较低的token，生成的文本会更加多样和探索性。</li>
<li><code>top_k</code>，选择下一个token时，只保留概率最高的前
<code>top_k</code>
个token，有效地避免模型选择概率很低的不合理token。</li>
<li><code>top_p</code>，动态地选择概率总和达到 top_p
阈值的最小token集合。</li>
<li><code>num_return_sequences</code>
，指定要生成的独立序列数量。默认为1，即只生成1个序列。</li>
<li><code>output_scores</code>是否输出每个token的预测分数。</li>
<li><code>output_logits</code>是否输出未经处理的原始预测logits。</li>
<li><code>pad_token_id,bos_token_id,eos_token_id</code>，需要根据模型的词表来看。不设置则为None。</li>
<li><code>max_length</code>:
最大输出长度,包括prompt和生成的新tokens。默认是20</li>
<li><code>max_new_tokens</code>:
最大生成新tokens数量,不包括prompt长度。</li>
<li><code>min_length</code>:
最小输出长度,包括prompt和生成的新tokens。</li>
<li><code>min_new_tokens</code>:
最小生成新tokens数量,不包括prompt长度。</li>
<li><code>early_stopping</code>: 控制beam search停止的条件。可选值为:
<ul>
<li><code>True</code>: 当生成了 <code>num_beams</code>
个完整候选序列时立即停止。</li>
<li><code>False</code>:
根据启发式停止,即当很难找到更好的候选时停止。</li>
<li><code>"never"</code>: 一直运行直到无法找到更好的候选为止。</li>
</ul></li>
</ul>
<p>接下来解析generate函数主要做了哪些。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>no_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">generate</span><span class="token punctuation">(</span>
    self<span class="token punctuation">,</span>
    inputs<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    generation_config<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>GenerationConfig<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    logits_processor<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>LogitsProcessorList<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    stopping_criteria<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>StoppingCriteriaList<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    prefix_allowed_tokens_fn<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Callable<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    synced_gpus<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    assistant_model<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token string">"PreTrainedModel"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    streamer<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token string">"BaseStreamer"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    negative_prompt_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    negative_prompt_attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Union<span class="token punctuation">[</span>GenerateOutput<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span><span class="token punctuation">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>inputs，一般是经过tokenizer处理的序列，包含attention_mask的。如果是调用tokenizer.encode()，那么不会有attention_mask。</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call</span>
<span class="token string">"处理 `generation_config` 和可能更新它的 `kwargs`，并验证 `.generate()` 的调用，略"</span>

<span class="token comment"># 2. Set generation parameters if not already defined</span>
<span class="token triple-quoted-string string">"""
略，设置生成所需的一些默认参数
"""</span>
<span class="token keyword">if</span> generation_config<span class="token punctuation">.</span>pad_token_id <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">and</span> generation_config<span class="token punctuation">.</span>eos_token_id <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> model_kwargs<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"attention_mask"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span>
            <span class="token string">"The attention mask and the pad token id were not set. As a consequence, you may observe "</span>
            <span class="token string">"unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results."</span>
        <span class="token punctuation">)</span>
    eos_token_id <span class="token operator">=</span> generation_config<span class="token punctuation">.</span>eos_token_id
    <span class="token comment"># 多语言模型的eos可能会有多个</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>eos_token_id<span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        eos_token_id <span class="token operator">=</span> eos_token_id<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Setting `pad_token_id` to `eos_token_id`:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>eos_token_id<span class="token punctuation">&#125;</span></span><span class="token string"> for open-end generation."</span></span><span class="token punctuation">)</span>
    generation_config<span class="token punctuation">.</span>pad_token_id <span class="token operator">=</span> eos_token_id<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>这一段说明确说明如果你没有传入<code>pad_token_id</code>，那么会以<code>eos_token_id</code>替代。若你没有传入<code>attention_mask</code>，会警告你传入，Attention
Mask中值为0的位置对应的Attention权重设为非常小的负值，通常是-1e9。</p>
<p>接下来就是处理模型的输入。获取输入的tensor和batch_size。<code>_prepare_model_inputs</code>方法过滤掉
model_kwargs中非空且不是模型主要输入的参数。对于文本生成模型，要看模型的encoder是否支持直接输入embedding，否则一律设置成input_ids。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 3. Define model inputs</span>
<span class="token comment"># inputs_tensor has to be defined</span>
<span class="token comment"># model_input_name is defined if model-specific keyword input is passed</span>
<span class="token comment"># otherwise model_input_name is None</span>
<span class="token comment"># all model-specific keyword inputs are removed from `model_kwargs`</span>
inputs_tensor<span class="token punctuation">,</span> model_input_name<span class="token punctuation">,</span> model_kwargs <span class="token operator">=</span> self<span class="token punctuation">.</span>_prepare_model_inputs<span class="token punctuation">(</span>
    inputs<span class="token punctuation">,</span> generation_config<span class="token punctuation">.</span>bos_token_id<span class="token punctuation">,</span> model_kwargs
<span class="token punctuation">)</span>
batch_size <span class="token operator">=</span> inputs_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>decode-only的模型应该使用左对齐。使用右对齐会警告，初始化 tokenizer
时设置
<code>padding_side='left'</code>以确保正确的生成结果。接下来的逻辑都不看了，无非就是处理一些模型生成参数，如max_length。根据不同的生成策略，会运行不同的生成函数，就看一个简单的贪婪解码的部分代码。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">        <span class="token keyword">if</span> generation_mode <span class="token operator">==</span> GenerationMode<span class="token punctuation">.</span>GREEDY_SEARCH<span class="token punctuation">:</span>
            <span class="token comment"># 11. run greedy search</span>
            result <span class="token operator">=</span> self<span class="token punctuation">.</span>_greedy_search<span class="token punctuation">(</span>
                input_ids<span class="token punctuation">,</span>
                logits_processor<span class="token operator">=</span>prepared_logits_processor<span class="token punctuation">,</span> <span class="token comment"># logits处理器，min_length作用于这个，在满足前减小eos的概率。</span>
                stopping_criteria<span class="token operator">=</span>prepared_stopping_criteria<span class="token punctuation">,</span> <span class="token comment"># 停止判定器，max_length就作用于这个</span>
                pad_token_id<span class="token operator">=</span>generation_config<span class="token punctuation">.</span>pad_token_id<span class="token punctuation">,</span>
                eos_token_id<span class="token operator">=</span>generation_config<span class="token punctuation">.</span>eos_token_id<span class="token punctuation">,</span>
                output_attentions <span class="token operator">=</span> generation_config<span class="token punctuation">.</span>output_attentions，<span class="token comment"># 是否输出注意力层分数</span>
                output_hidden_states <span class="token operator">=</span> generation_config<span class="token punctuation">.</span>output_hidden_states <span class="token comment"># 是否返回隐藏状态</span>
                output_scores<span class="token operator">=</span>generation_config<span class="token punctuation">.</span>output_scores<span class="token punctuation">,</span>
                output_logits<span class="token operator">=</span>generation_config<span class="token punctuation">.</span>output_logits<span class="token punctuation">,</span>
                return_dict_in_generate<span class="token operator">=</span>generation_config<span class="token punctuation">.</span>return_dict_in_generate<span class="token punctuation">,</span>
                synced_gpus<span class="token operator">=</span>synced_gpus<span class="token punctuation">,</span>
                streamer<span class="token operator">=</span>streamer<span class="token punctuation">,</span>
                <span class="token operator">**</span>model_kwargs<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>

    <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>首先拿到全部的输出，并只需要下一个token的内容。<code>next_tokens</code>在序列结束的情况下，一定是pad_id。生成后更新<code>input_ids</code>，若生成了eos_id，就认为序列已经完成。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">while</span> self<span class="token punctuation">.</span>_has_unfinished_sequences<span class="token punctuation">(</span>this_peer_finished<span class="token punctuation">,</span> synced_gpus<span class="token punctuation">,</span> device<span class="token operator">=</span>input_ids<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">:</span>            
  		outputs <span class="token operator">=</span> self<span class="token punctuation">(</span>
              <span class="token operator">**</span>model_inputs<span class="token punctuation">,</span>
              return_dict<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
              output_attentions<span class="token operator">=</span>output_attentions<span class="token punctuation">,</span>
              output_hidden_states<span class="token operator">=</span>output_hidden_states<span class="token punctuation">,</span>
          <span class="token punctuation">)</span>
          next_token_logits <span class="token operator">=</span> outputs<span class="token punctuation">.</span>logits<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment"># 最后一个时间步</span>
          next_tokens_scores <span class="token operator">=</span> logits_processor<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> next_token_logits<span class="token punctuation">)</span>
          next_tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>next_tokens_scores<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 最大值索引，贪婪策略</span>
          <span class="token comment"># finished sentences should have their next token be a padding token</span>
          <span class="token keyword">if</span> eos_token_id <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
              <span class="token keyword">if</span> pad_token_id <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                  <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"If `eos_token_id` is defined, make sure that `pad_token_id` is defined."</span><span class="token punctuation">)</span>
                 	<span class="token comment"># unfinished_sequences初始化是torch.ones(batch_size,dtype = torch.long)                   </span>
              next_tokens <span class="token operator">=</span> next_tokens <span class="token operator">*</span> unfinished_sequences <span class="token operator">+</span> pad_token_id <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> unfinished_sequences<span class="token punctuation">)</span>
          <span class="token comment"># update generated ids, model inputs, and length for next step</span>
          input_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>input_ids<span class="token punctuation">,</span> next_tokens<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
          <span class="token comment"># if eos_token was found in one sentence, set sentence to finished</span>
          <span class="token keyword">if</span> eos_token_id_tensor <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
              unfinished_sequences <span class="token operator">=</span> unfinished_sequences<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>
                  next_tokens<span class="token punctuation">.</span>tile<span class="token punctuation">(</span>eos_token_id_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>ne<span class="token punctuation">(</span>eos_token_id_tensor<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>prod<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
              <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>最后generate方法返回一个Union[GenerateOutput,torch.LongTensor]。一般来说是前者。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">GenerateNonBeamOutput <span class="token operator">=</span> Union<span class="token punctuation">[</span>GenerateDecoderOnlyOutput<span class="token punctuation">,</span> GenerateEncoderDecoderOutput<span class="token punctuation">]</span>
GenerateBeamOutput <span class="token operator">=</span> Union<span class="token punctuation">[</span>GenerateBeamDecoderOnlyOutput<span class="token punctuation">,</span> GenerateBeamEncoderDecoderOutput<span class="token punctuation">]</span>
GenerateOutput <span class="token operator">=</span> Union<span class="token punctuation">[</span>GenerateNonBeamOutput<span class="token punctuation">,</span> GenerateBeamOutput<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>我们就拿<code>GenerateBeamDecoderOnlyOutput</code>来看。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">sequences<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>LongTensor <span class="token operator">=</span> <span class="token boolean">None</span> <span class="token comment"># 返回的序列，需要进行decode，一般只用这个</span>
sequences_scores<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span> <span class="token comment"># 序列beam_search的分数</span>
scores<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
logits<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
beam_indices<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
attentions<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
hidden_states<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
past_key_values<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>iroha</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://example.com/post/model_load.html" title="Huggingface的模型加载流程">http://example.com/post/model_load.html</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/post/rl_surface.html" rel="prev" title="DRL introduce"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">DRL introduce</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/post/ft_survey.html" rel="next" title="大模型微调方法综述"><span class="post-nav-text">大模型微调方法综述</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2024 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> iroha</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.11</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>