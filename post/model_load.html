<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="iroha"><meta name="copyright" content="iroha"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>Huggingface的模型加载流程 | iroha</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...undefined?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"example.com","root":"/","title":"不想摆烂","version":"1.10.9","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><meta name="description" content="我们以下面的这一句语句作为开始，以从本地加载模型为例。 model &#x3D; AutoModelForSeq2SeqLM.from_pretrained(&quot;bigscience&#x2F;T0_3B&quot;) inputs &#x3D; tokenizer.encode(q.strip()+&quot; ? To answer this question, we need to know&quot;, return_tensors&#x3D;&quot;pt&quot;) o">
<meta property="og:type" content="article">
<meta property="og:title" content="Huggingface的模型加载流程">
<meta property="og:url" content="http://example.com/post/model_load.html">
<meta property="og:site_name" content="iroha">
<meta property="og:description" content="我们以下面的这一句语句作为开始，以从本地加载模型为例。 model &#x3D; AutoModelForSeq2SeqLM.from_pretrained(&quot;bigscience&#x2F;T0_3B&quot;) inputs &#x3D; tokenizer.encode(q.strip()+&quot; ? To answer this question, we need to know&quot;, return_tensors&#x3D;&quot;pt&quot;) o">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20240422014943359.png">
<meta property="article:published_time" content="2024-04-11T08:00:00.000Z">
<meta property="article:modified_time" content="2024-07-22T09:14:59.982Z">
<meta property="article:author" content="iroha">
<meta property="article:tag" content="大模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20240422014943359.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="iroha"><img width="96" loading="lazy" src="/yun.png" alt="iroha"><span class="site-author-status" title="永远相信美好的事情即将发生">😊</span></a><div class="site-author-name"><a href="/about/">iroha</a></div><span class="site-name">iroha</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">14</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">4</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">2</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="友链" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.</span> <span class="toc-text">加载</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E8%AF%8D"><span class="toc-number">2.</span> <span class="toc-text">分词</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%9F%E6%88%90"><span class="toc-number">3.</span> <span class="toc-text">生成</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://example.com/post/model_load.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="iroha"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="iroha"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Huggingface的模型加载流程</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2024-04-11 16:00:00" itemprop="dateCreated datePublished" datetime="2024-04-11T16:00:00+08:00">2024-04-11</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="修改时间：2024-07-22 17:14:59" itemprop="dateModified" datetime="2024-07-22T17:14:59+08:00">2024-07-22</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">NLP</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">大模型</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>我们以下面的这一句语句作为开始，以从本地加载模型为例。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B")
inputs = tokenizer.encode(q.strip()+" ? To answer this question, we need to know", return_tensors="pt")
outputs = model.generate(inputs.cuda(), max_new_tokens=100, do_sample=False, top_k=50)</code></pre>
<h1 id="加载">加载</h1>
<p>AutoModelForSeq2SeqLM继承了_BaseAutoModelClass，这个类是所有AutoModel的基类，保存在transformers/models/auto/auto_factory.py中。调用的from_pretrained方法实际上就来自于这个基类。我们假设模型保存在本地，一些下载的逻辑不看，且kwargs和config为None。最终会得到模型的哈希值。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if not isinstance(config, PretrainedConfig):
    # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible
    resolved_config_file = cached_file(
        pretrained_model_name_or_path,
        CONFIG_NAME,
        _raise_exceptions_for_gated_repo=False,
        _raise_exceptions_for_missing_entries=False,
        _raise_exceptions_for_connection_errors=False,
        **hub_kwargs,
    )
    commit_hash = extract_commit_hash(resolved_config_file, commit_hash)</code></pre>
<p>首先需要加载config，通过cached_file来加载。CONFIG_NAME默认为config.json，pretrained_model_name_or_path则是from_pretrained传入的字符串。再看具体实现。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">path_or_repo_id = str(path_or_repo_id) # "bigscience/T0_3B"
    full_filename = os.path.join(subfolder, filename) # filename=config.json
    if os.path.isdir(path_or_repo_id):
        resolved_file = os.path.join(os.path.join(path_or_repo_id, subfolder), filename) # subfolder不指定=None
        if not os.path.isfile(resolved_file):
            if _raise_exceptions_for_missing_entries:
                raise EnvironmentError(
                    f"{path_or_repo_id} does not appear to have a file named {full_filename}. Checkout "
                    f"'https://huggingface.co/{path_or_repo_id}/tree/{revision}' for available files."
                )
            else:
                return None
        return resolved_file # 返回bigscience/T0_3B/config.json</code></pre>
<p>此时对应的config.json已经被加载到内存中，之后需要加载到AutoConfig中。可以看到就是T0_3B/config.json</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">            config, kwargs = AutoConfig.from_pretrained(
                pretrained_model_name_or_path,
                return_unused_kwargs=True,
                trust_remote_code=trust_remote_code,
                code_revision=code_revision,
                _commit_hash=commit_hash,
                **hub_kwargs,
                **kwargs,
            )
---------config---------
T5Config {
  "_name_or_path": "/data2/wtf/model/bigscience/T0_3B",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 5120,
  "d_kv": 64,
  "d_model": 2048,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 24,
  "num_heads": 32,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.38.2",
  "use_cache": true,
  "vocab_size": 32128
}</code></pre>
<p>最终将模型加载，返回model实例。model_class就是json中的architectures对应值。_get_model_class方法就是得到模型的类型！</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">          model_class = _get_model_class(config, cls._model_mapping) # T5ForConditionalGeneration
  		"""
  		"architectures": [
  "T5ForConditionalGeneration"
]
			就是返回arch中的模型类型
  		"""
          return model_class.from_pretrained(
              pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
          )</code></pre>
<p>上面的<code>cls._model_mapping</code>就是模型根据你的输入，得到当前模型类型的映射。</p>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">class AutoModelForSeq2SeqLM(_BaseAutoModelClass):
    _model_mapping = MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING
------
MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING = _LazyAutoMapping(
    CONFIG_MAPPING_NAMES, MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES
)
</code></pre>
<p><code>CONFIG_MAPPING_NAMES</code>是根据你传入的路径来匹配，而<code>MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES</code>同理，也有和下图类似的字典。本例中<code>CONFIG_MAPPING_NAMES="T5Config"</code>，<code>MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES=T5ForConditionalGeneration</code>，和config.json中的一样。</p>
<figure>
<img src="../images/image-20240422014943359.png" alt="image-20240422014943359" loading="lazy">
<figcaption aria-hidden="true">image-20240422014943359</figcaption>
</figure>
<p><code>model_class.from_pretrained</code>中的<code>from_pretrained</code>来自于PretrainedModel类，这是所有模型的基类(注意不是AutoModel)。这个函数是本篇的核心。返回的模型实例默认是开启model.eval()模式，若要微调or训练模型，需要手动指定model.train()。</p>
<p>这里顺便提一嘴model.train和eval下的区别：</p>
<ol type="1">
<li>Dropout 和 BatchNorm 行为。model.train()下 Dropout
层会随机丢弃一部分神经元, BatchNorm 层会计算当前 batch 的统计量。
model.eval()下Dropout 层会全部保留神经元, BatchNorm
层会使用训练好的滑动平均统计量。</li>
<li>梯度与优化器。model.train()会计算梯度并更新模型参数。model.eval()不会计算梯度,
也不会更新模型参数。</li>
<li>数据增强。model.train()通常会应用一些数据增强技术,
如翻转、旋转等。model.eval()一般不需要数据增强,
直接使用原始的输入数据。</li>
<li>内存与计算开销。model.train()需要保存中间激活值用于反向传播,
计算开销相对更大。model.eval()只需要前向传播, 不需要保存中间激活值,
计算开销相对更小。</li>
</ol>
<p>下面看几个比较关键的参数。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">@classmethod
def from_pretrained(
    cls,
    pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
    *model_args,
    config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
    cache_dir: Optional[Union[str, os.PathLike]] = None,
    ignore_mismatched_sizes: bool = False,
    force_download: bool = False,
    local_files_only: bool = False,
    token: Optional[Union[str, bool]] = None,
    revision: str = "main",
    use_safetensors: bool = None,
    **kwargs,
):</code></pre>
<ul>
<li><p>pretrained_model_name_or_path，模型的路径or在huggingface中的名字。</p></li>
<li><p>force_download，不论有没有下载好模型，都下载，若存在则覆盖。</p></li>
<li><p>torch_dtype，<code>torch.float16</code> or
<code>torch.bfloat16</code> or
<code>torch.float</code>，指定模型参数的载入精度，不指定则默认为<code>torch.float</code>。也就是说，<code>config.json</code>
中的 <code>torch_dtype</code> 设置拥有最高优先级。如果
<code>torch_dtype</code> 参数被设置为
<code>"auto"</code>，那么它会首先使用 <code>config.json</code>
中的设置。只有当 <code>config.json</code> 中没有找到
<code>torch_dtype</code> 且 <code>torch_dtype</code> 参数被设置为
<code>"auto"</code>时，它才会回退到使用权重checkpoint中的数据类型，查看第一个数据是什么类型就用什么类型。若根本没有设置该参数，则使用torch.float。</p></li>
<li><p>device_map，可以传入三种类型的参数。<strong>字符串类型</strong>，如果传入一个字符串类型的设备名称(例如
"cpu", "cuda:1", "mps")，那么整个模型会被分配到指定的设备上，如果传入
"auto"，Accelerate
库会自动计算出最优的设备分布。<strong>字典类型</strong>，这种情况下
device_map
是一个字典,键是模型的子模块名称,值是对应的设备编号或设备名称，这允许用户手动指定模型的各个子模块应该分布在哪些设备上，只需要指定到模块名称的级别,子模块会自动分配到同一设备，如</p>
<pre class="line-numbers language-none"><code class="language-none">device_map = {
    "transformer.encoder": "cuda:0",
    "transformer.decoder": "cuda:1",
    "transformer.pooler": "cuda:0",
    "lm_head": "cuda:1"
}</code></pre>
<p>还可以传入整数或<code>torch.device</code>，代表将整个模型放在指定编号的
GPU
上。如<code>device = torch.device("cuda:1"),device_map = deveice</code>。只要指定了device_map，那么都会让
<code>low_cpu_mem_usage=True</code>。不指定就用cpu。</p></li>
<li><p>quantization_config，指定模型的量化策略。可以是一个字典或者继承自
<code>QuantizationConfigMixin</code>
的对象，它用于配置模型的量化参数。除了 <code>quantization_config</code>
之外,还可以使用 <code>load_in_4bit</code> 和 <code>load_in_8bit</code>
等参数来指定量化方式,但这种方式不被推荐，只量化了参数，并不量化梯度。但推理阶段无所谓。下面是一个例子。</p></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import bitsandbytes as bnb
from transformers import QuantizationConfig

quantization_config = QuantizationConfig(
    quantization_method=bnb.QuantizationMethod.DYNAMIC_QUANT,
    weight_bits=8,# 权重为INT8
    grad_bits=8,# 梯度也INT8
    per_channel=False
)
model = AutoModelForSeq2SeqLM.from_pretrained(
    "bigscience/T0_3B",
    quantization_config=quantization_config
)</code></pre>
<ul>
<li>local_files_only，如果是True，则不会从Hub上下载。</li>
<li>low_cpu_mem_usage，作用是尝试在加载模型时不使用超过模型大小 1 倍的
CPU 内存(包括峰值内存)。</li>
<li>attn_implementation，可以选择<code>flash_attention_2</code>,<code>sdpa(default)</code>,<code>eager(手动实现)</code></li>
</ul>
<p>之后看几处比较关键的源码。</p>
<p>从传入的路径中提取config。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># Load config if we don't provide a configuration
if not isinstance(config, PretrainedConfig):
    config_path = config if config is not None else pretrained_model_name_or_path
    config, model_kwargs = cls.config_class.from_pretrained</code></pre>
<p>量化操作。注意到量化操作会强制开启low_cpu_mem_usage。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pre_quantized = getattr(config, "quantization_config", None) is not None
if pre_quantized or quantization_config is not None:
    if pre_quantized:
        config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
            config.quantization_config, quantization_config
        )
    else:
        config.quantization_config = quantization_config
    hf_quantizer = AutoHfQuantizer.from_config(config.quantization_config, pre_quantized=pre_quantized)
else:
    hf_quantizer = None

if hf_quantizer is not None:
    hf_quantizer.validate_environment(
        torch_dtype=torch_dtype, from_tf=from_tf, from_flax=from_flax, device_map=device_map
    )
    torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)
    device_map = hf_quantizer.update_device_map(device_map)

    # Force-set to `True` for more mem efficiency
    if low_cpu_mem_usage is None:
        low_cpu_mem_usage = True
        logger.warning("`low_cpu_mem_usage` was None, now set to True since model is quantized.")
is_quantized = hf_quantizer is not None</code></pre>
<p>加载权重，tf相关的就不看了。在加载pytorch权重中，会去你指定的文件夹中找<code>pytorch_model.bin</code>这个权重文件。<code>subfolder,variant</code>若不在参数中指定都为空字符。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">elif os.path.isfile(
    os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_NAME, variant))
):
    # Load from a PyTorch checkpoint,会拼成model_path/pytorch_model.bin
    archive_file = os.path.join(
        pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_NAME, variant)
    )
</code></pre>
<p>一些模型的权重可能以多个checkpoint文件来保存，这时候要求有一个<code>WEIGHTS_INDEX_NAME = "pytorch_model.bin.index.json"</code>文件来进行索引。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">                elif os.path.isfile(
                    os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_INDEX_NAME, variant))
                ):
                    # Load from a sharded PyTorch checkpoint
                    archive_file = os.path.join(
                        pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_INDEX_NAME, variant)
                    )
                    is_sharded = True # 注意这里
----
{
    "checkpoint_files": ["pytorch_model.bin.0", "pytorch_model.bin.1", "pytorch_model.bin.2"],
    "num_checkpoint_files": 3,
    "size_checkpoint_files": [100000, 200000, 50000],
    "weight_map": {
        "layer1.weight": [0, 0],
        "layer1.bias": [0, 50000],
        "layer2.weight": [1, 0],
        "layer2.bias": [1, 100000]
    }
}</code></pre>
<p>还有一种情况，就是指定的路径不是一个文件夹，而是权重文件本身，如<code>bigscience/T0_3B/pytorch_model.bin</code>，那也可以加载。因为最终都是让<code>archive_file = weight_file</code>。</p>
<pre class="line-numbers language-none"><code class="language-none">elif os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):
    archive_file = pretrained_model_name_or_path
    is_local = True</code></pre>
<p>最终，<code>resolved_archive_file = archive_file</code>，获取权重文件路径。如果是分散的checkpoint，也就是<code>is_sharded</code>是True，还要进行额外的操作，这里就不深入了。</p>
<p>接下来就要加载权重了，首先判断是不是pytorch，若是，则加载权重文件。详细的加载源码就不赘述，最终会返回由torch.load加载模型结构和权重参数。</p>
<pre class="line-numbers language-none"><code class="language-none">if from_pt:
    if not is_sharded and state_dict is None:
        # Time to load the checkpoint
        state_dict = load_state_dict(resolved_archive_file)</code></pre>
<p>接下来决定权重的数据类型，正如上面交代torch_dtype参数所说，先考虑<code>torch_dtype=auto</code>，也就是config.json中的数据类型。然后再考虑</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if torch_dtype is not None:
    if isinstance(torch_dtype, str):
        if torch_dtype == "auto":
            if hasattr(config, "torch_dtype") and config.torch_dtype is not None:
                torch_dtype = config.torch_dtype
                logger.info(f"Will use torch_dtype={torch_dtype} as defined in model's config object")

        else:
            raise ValueError(
                f'`torch_dtype` can be either `torch.dtype` or `"auto"`, but received {torch_dtype}'
            )
    dtype_orig = cls._set_default_torch_dtype(torch_dtype)</code></pre>
<p>若是分片情况，则去分片json中找有没有指定。如果不是分片的情况，则按权重文件中第一个数据的类型。若不显式指定torch_dtype(None)，则使用float32。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">                    else:
                        if is_sharded and "dtype" in sharded_metadata:
                            torch_dtype = sharded_metadata["dtype"]
                        elif not is_sharded:
                            torch_dtype = get_state_dict_dtype(state_dict)
                        else:
                            one_state_dict = load_state_dict(resolved_archive_file[0])
                            torch_dtype = get_state_dict_dtype(one_state_dict)
                            del one_state_dict  # free CPU memory
                        logger.info(
                            "Since the `torch_dtype` attribute can't be found in model's config object, "
                            "will use torch_dtype={torch_dtype} as derived from model's weights"
                        )    
-------------get_state_dict_dtype(state_dict)---------
# if no floating dtype was found return whatever the first dtype is
else:
    return next(state_dict.values()).dtype</code></pre>
<p>还有混合精度的情况，在初始nn.Module的时候可以设置单独设置_keep_in_fp32_modules哪些模块保持fp32精度。</p>
<pre class="line-numbers language-none"><code class="language-none"># Check if `_keep_in_fp32_modules` is not None
use_keep_in_fp32_modules = (cls._keep_in_fp32_modules is not None) and (
    (torch_dtype == torch.float16) or hasattr(hf_quantizer, "use_keep_in_fp32_modules")
)</code></pre>
<p>创建模型实例。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">with ContextManagers(init_contexts):
    # Let's make sure we don't run the init function of buffer modules
    model = cls(config, *model_args, **model_kwargs)</code></pre>
<p>来看device_map的逻辑。首先是字符串的情况，必须要是<code>"auto", "balanced", "balanced_low_0", "sequential"</code>这几种，否则报错。"auto"会自动操作，尽可能均匀地分配计算负载。<code>balanced</code>则是平均分配模型层中的参数给不同的卡。<code>balanced_low_0</code>则是少给0分一些，因为0往往还有其他事情要做。<code>sequential</code>则是按模型层的顺序来分配给不同的卡，保持模型层的拓扑结构,减少跨设备的数据传输，如attention一张卡，MLP一张卡。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if device_map not in ["auto", "balanced", "balanced_low_0", "sequential"]:
        raise ValueError
if device_map != "sequential":
                max_memory = get_balanced_memory(
                    model,
                    dtype=target_dtype,
                    low_zero=(device_map == "balanced_low_0"),
                    max_memory=max_memory,
                    **device_map_kwargs,
                )</code></pre>
<p>实际上可以看到，<code>auto</code>就是<code>balanced</code>策略。</p>
<p>其他情况，输入的什么设备就绑定什么设备，若没有指定device_map，就加载到cpu。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if isinstance(device_map, torch.device):
     device_map = {"": device_map}
elif isinstance(device_map, str) and device_map not in ["auto", "balanced", "balanced_low_0", "sequential"]:
      try:
          device_map = {"": torch.device(device_map)}
elif isinstance(device_map, int):# 小于0报错
	device_map = {"": device_map}</code></pre>
<p>这里有一个<code>tie_weights</code>函数，实现了参数的绑定操作，本质上就是默认让输入嵌入层和输出嵌入层的权重绑定在一起。若是在config中指定<code>is_encoder_decoder=True</code>且<code>tie_encoder_decoder=True</code>，那么Encoder和Decoder的参数也会共用(都使用Decoder的Weights)，不过T5中并不这么做，一般是<strong>BERT-based</strong>模型在微调成Encoder-Decoder模型的时候会这么做。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def tie_weights(self):
    """
    Tie the weights between the input embeddings and the output embeddings.

    If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the
    weights instead.
    """
    if getattr(self.config, "tie_word_embeddings", True):
        output_embeddings = self.get_output_embeddings()
        if output_embeddings is not None:
            self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())

    if getattr(self.config, "is_encoder_decoder", False) and getattr(self.config, "tie_encoder_decoder", False):
        if hasattr(self, self.base_model_prefix):
            self = getattr(self, self.base_model_prefix)
        self._tie_encoder_decoder_weights(self.encoder, self.decoder, self.base_model_prefix)

    for module in self.modules():
        if hasattr(module, "_tie_weights"):
            module._tie_weights()</code></pre>
<p>开启model.eval。</p>
<pre class="line-numbers language-none"><code class="language-none">model.eval()</code></pre>
<p>若模型是生成式模型，那么还需要配置生成的参数。<code>GenerationConfig</code>实际上就是model.generate()方法中所要用的参数。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if model.can_generate() and pretrained_model_name_or_path is not None:
    try:
        model.generation_config = GenerationConfig.from_pretrained(
            pretrained_model_name_or_path,
            cache_dir=cache_dir,
            force_download=force_download,
            resume_download=resume_download,
            proxies=proxies,
            local_files_only=local_files_only,
            token=token,
            revision=revision,
            subfolder=subfolder,
            _from_auto=from_auto_class,
            _from_pipeline=from_pipeline,
            **kwargs,
        )
    except OSError:
        logger.info(
            "Generation config file not found, using a generation config created from the model config."
        )
        pass</code></pre>
<p>最终输出一些加载参数时输出的信息，然后返回模型。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if output_loading_info:
    if loading_info is None:
        loading_info = {
            "missing_keys": missing_keys,
            "unexpected_keys": unexpected_keys,
            "mismatched_keys": mismatched_keys,
            "error_msgs": error_msgs,
        }
    return model, loading_info</code></pre>
<p>总结，根据最常用的方法，主要是做以下几个操作。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B",device_map="auto")</code></pre>
<ol type="1">
<li>根据输入路径拿到config.json，加载到Config。</li>
<li>根据输入路径拿到权重文件pytorch_model.bin，由torch.load加载模型结构和权重参数。。</li>
<li>决定权重的数据类型，未指定则是float32。</li>
<li>平均分配参数给各张卡。</li>
<li>绑定input和output的Embedding，让其使用同一份Embedding参数。</li>
<li>model.eval()。</li>
<li>若是生成式模型，配置生成参数。</li>
<li>返回模型实例。</li>
</ol>
<p>所以，实际上会调用两个不同的<code>from_pretrained</code>方法，第一个是AutoModel基类_BaseAutoModelClass的，在最后调用<code>get_model_class</code>方法得到模型本身的类，本例中是<code>T5ForConditionalGeneration</code>，然后再调用这个类的<code>from_pretrained</code>，而这个类的<code>from_pretrained</code>在其基类<code>PreTrainedModel</code>实现，所以再会调用<code>PreTrainedModel</code>的<code>from_pretrained</code>方法。分析完毕。</p>
<h1 id="分词">分词</h1>
<p>后续更新，挖坑</p>
<h1 id="生成">生成</h1>
<p>并不是每一个模型都可以使用.generate()进行序列生成，需要通过函数判断是否能够进行序列生成任务，所以每一个模型都需要重写<code>prepare_inputs_for_generation</code>方法。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin, PeftAdapterMixin):
	    @classmethod
    def can_generate(cls) -&gt; bool:
        """
        Returns whether this model can generate sequences with `.generate()`.

        Returns:
            `bool`: Whether this model can generate sequences with `.generate()`.
        """
        # Detects whether `prepare_inputs_for_generation` has been overwritten, which is a requirement for generation.
        # Alternativelly, the model can also have a custom `generate` function.
        if "GenerationMixin" in str(cls.prepare_inputs_for_generation) and "GenerationMixin" in str(cls.generate):
            return False
        return True</code></pre>
<p>而generate方法本身就在<code>GenerationMixin</code>类中实现。</p>
<pre class="line-numbers language-none"><code class="language-none">Class that holds a configuration for a generation task. A `generate` call supports the following generation methods
for text-decoder, text-to-text, speech-to-text, and vision-to-text models:

    - *greedy decoding* by calling [`~generation.GenerationMixin._greedy_search`] if `num_beams=1` and
        `do_sample=False`
    - *contrastive search* by calling [`~generation.GenerationMixin._contrastive_search`] if `penalty_alpha&gt;0.`
        and `top_k&gt;1`
    - *multinomial sampling* by calling [`~generation.GenerationMixin._sample`] if `num_beams=1` and
        `do_sample=True`
    - *beam-search decoding* by calling [`~generation.GenerationMixin._beam_search`] if `num_beams&gt;1` and
        `do_sample=False`
    - *beam-search multinomial sampling* by calling [`~generation.GenerationMixin._beam_sample`] if
        `num_beams&gt;1` and `do_sample=True`
    - *diverse beam-search decoding* by calling [`~generation.GenerationMixin._group_beam_search`], if
        `num_beams&gt;1` and `num_beam_groups&gt;1`
    - *constrained beam-search decoding* by calling [`~generation.GenerationMixin._constrained_beam_search`], if
        `constraints!=None` or `force_words_ids!=None`
    - *assisted decoding* by calling [`~generation.GenerationMixin._assisted_decoding`], if
        `assistant_model` or `prompt_lookup_num_tokens` is passed to `.generate()`

You do not need to call any of the above methods directly. Pass custom parameter values to '.generate()'. To learn
more about decoding strategies refer to the [text generation strategies guide](../generation_strategies).</code></pre>
<p>就不看详细的实现，先看<code>GenerationConfig</code>的生成策略。若不用GenerationConfig，也可以直接输入kwargs。</p>
<ul>
<li>greedy
decoding贪婪解码，<code>num_beams=1</code>且<code>do_sample=False</code>，会一直选择概率最高的token，一条路走到黑。</li>
<li>Contrastive
search对比搜索，在NIPS22被提出，能在保持流畅性的前提下，鼓励多样性生成，减少重复输出。需要<code>penalty_alpha&gt;0</code>
and
<code>top_k&gt;1</code>。一个候选token与当前token非常相似(相似度得分高)，那么它的概率就会被较多地降低。这样做的目的是鼓励生成更加多样化的文本，避免同类型的token过于集中出现。最后,算法在经过调整的
<code>scores</code>
向量上取Top-1。核心公式<code>scores = (1.0 - alpha) * next_top_k_probs - alpha * scores</code>。<code>next_top_k_probs</code>是当前token的Top-k概率分布，等式右边的<code>scores</code>是当前token和下一个token之间的相似度分数。所以当前token与next
token越相似，惩罚就越大。</li>
<li>multinomial sampling，<code>num_beams=1</code>
and<code>do_sample=True</code>。和贪婪解码的区别不一定选择概率最高的token，而是根据概率分布来采样。</li>
<li>beam-search，<code>num_beams&gt;1</code>
and<code>do_sample=False</code>，保留top-k个得分最高的候选序列，称为"beam"。这里选择不采样，是选择得分最高的2*num_beams个token。</li>
<li>diverse beam-search，<code>num_beams&gt;1</code> and
<code>num_beam_groups&gt;1</code>，通过分组机制，确保了不同beam之间的差异性。</li>
</ul>
<p>接下来介绍一些比较常用的参数。</p>
<ul>
<li><code>do_sample</code>，是否根据概率分布采样。</li>
<li><code>temperature</code>，默认1.0。小于1时，当
<code>temperature &lt; 1.0</code> 时,
生成概率分布会被"平滑"(峰值变得更陡峭)，使得模型更倾向于选择概率较高的token，生成的文本会更加集中和保守。当<code>temperature &gt; 1.0</code>时，
生成概率分布会被"拉平"(峰值变得更平缓)，使得模型会选择概率较低的token，生成的文本会更加多样和探索性。</li>
<li><code>top_k</code>，选择下一个token时，只保留概率最高的前
<code>top_k</code>
个token，有效地避免模型选择概率很低的不合理token。</li>
<li><code>top_p</code>，动态地选择概率总和达到 top_p
阈值的最小token集合。</li>
<li><code>num_return_sequences</code>
，指定要生成的独立序列数量。默认为1，即只生成1个序列。</li>
<li><code>output_scores</code>是否输出每个token的预测分数。</li>
<li><code>output_logits</code>是否输出未经处理的原始预测logits。</li>
<li><code>pad_token_id,bos_token_id,eos_token_id</code>，需要根据模型的词表来看。不设置则为None。</li>
<li><code>max_length</code>:
最大输出长度,包括prompt和生成的新tokens。默认是20</li>
<li><code>max_new_tokens</code>:
最大生成新tokens数量,不包括prompt长度。</li>
<li><code>min_length</code>:
最小输出长度,包括prompt和生成的新tokens。</li>
<li><code>min_new_tokens</code>:
最小生成新tokens数量,不包括prompt长度。</li>
<li><code>early_stopping</code>: 控制beam search停止的条件。可选值为:
<ul>
<li><code>True</code>: 当生成了 <code>num_beams</code>
个完整候选序列时立即停止。</li>
<li><code>False</code>:
根据启发式停止,即当很难找到更好的候选时停止。</li>
<li><code>"never"</code>: 一直运行直到无法找到更好的候选为止。</li>
</ul></li>
</ul>
<p>接下来解析generate函数主要做了哪些。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">@torch.no_grad()
def generate(
    self,
    inputs: Optional[torch.Tensor] = None,
    generation_config: Optional[GenerationConfig] = None,
    logits_processor: Optional[LogitsProcessorList] = None,
    stopping_criteria: Optional[StoppingCriteriaList] = None,
    prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,
    synced_gpus: Optional[bool] = None,
    assistant_model: Optional["PreTrainedModel"] = None,
    streamer: Optional["BaseStreamer"] = None,
    negative_prompt_ids: Optional[torch.Tensor] = None,
    negative_prompt_attention_mask: Optional[torch.Tensor] = None,
    **kwargs,
) -&gt; Union[GenerateOutput, torch.LongTensor]:</code></pre>
<ul>
<li>inputs，一般是经过tokenizer处理的序列，包含attention_mask的。如果是调用tokenizer.encode()，那么不会有attention_mask。</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call
"处理 `generation_config` 和可能更新它的 `kwargs`，并验证 `.generate()` 的调用，略"

# 2. Set generation parameters if not already defined
"""
略，设置生成所需的一些默认参数
"""
if generation_config.pad_token_id is None and generation_config.eos_token_id is not None:
    if model_kwargs.get("attention_mask", None) is None:
        logger.warning(
            "The attention mask and the pad token id were not set. As a consequence, you may observe "
            "unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results."
        )
    eos_token_id = generation_config.eos_token_id
    # 多语言模型的eos可能会有多个
    if isinstance(eos_token_id, list):
        eos_token_id = eos_token_id[0]
    logger.warning(f"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.")
    generation_config.pad_token_id = eos_token_id</code></pre>
<p>这一段说明确说明如果你没有传入<code>pad_token_id</code>，那么会以<code>eos_token_id</code>替代。若你没有传入<code>attention_mask</code>，会警告你传入，Attention
Mask中值为0的位置对应的Attention权重设为非常小的负值，通常是-1e9。</p>
<p>接下来就是处理模型的输入。获取输入的tensor和batch_size。<code>_prepare_model_inputs</code>方法过滤掉
model_kwargs中非空且不是模型主要输入的参数。对于文本生成模型，要看模型的encoder是否支持直接输入embedding，否则一律设置成input_ids。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 3. Define model inputs
# inputs_tensor has to be defined
# model_input_name is defined if model-specific keyword input is passed
# otherwise model_input_name is None
# all model-specific keyword inputs are removed from `model_kwargs`
inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(
    inputs, generation_config.bos_token_id, model_kwargs
)
batch_size = inputs_tensor.shape[0]</code></pre>
<p>decode-only的模型应该使用左对齐。使用右对齐会警告，初始化 tokenizer
时设置
<code>padding_side='left'</code>以确保正确的生成结果。接下来的逻辑都不看了，无非就是处理一些模型生成参数，如max_length。根据不同的生成策略，会运行不同的生成函数，就看一个简单的贪婪解码的部分代码。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">        if generation_mode == GenerationMode.GREEDY_SEARCH:
            # 11. run greedy search
            result = self._greedy_search(
                input_ids,
                logits_processor=prepared_logits_processor, # logits处理器，min_length作用于这个，在满足前减小eos的概率。
                stopping_criteria=prepared_stopping_criteria, # 停止判定器，max_length就作用于这个
                pad_token_id=generation_config.pad_token_id,
                eos_token_id=generation_config.eos_token_id,
                output_attentions = generation_config.output_attentions，# 是否输出注意力层分数
                output_hidden_states = generation_config.output_hidden_states # 是否返回隐藏状态
                output_scores=generation_config.output_scores,
                output_logits=generation_config.output_logits,
                return_dict_in_generate=generation_config.return_dict_in_generate,
                synced_gpus=synced_gpus,
                streamer=streamer,
                **model_kwargs,
            )
-----------------------------------------------------------------------------------

    </code></pre>
<p>首先拿到全部的输出，并只需要下一个token的内容。<code>next_tokens</code>在序列结束的情况下，一定是pad_id。生成后更新<code>input_ids</code>，若生成了eos_id，就认为序列已经完成。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):            
  		outputs = self(
              **model_inputs,
              return_dict=True,
              output_attentions=output_attentions,
              output_hidden_states=output_hidden_states,
          )
          next_token_logits = outputs.logits[:, -1, :] # 最后一个时间步
          next_tokens_scores = logits_processor(input_ids, next_token_logits)
          next_tokens = torch.argmax(next_tokens_scores, dim=-1) # 最大值索引，贪婪策略
          # finished sentences should have their next token be a padding token
          if eos_token_id is not None:
              if pad_token_id is None:
                  raise ValueError("If `eos_token_id` is defined, make sure that `pad_token_id` is defined.")
                 	# unfinished_sequences初始化是torch.ones(batch_size,dtype = torch.long)                   
              next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)
          # update generated ids, model inputs, and length for next step
          input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
          # if eos_token was found in one sentence, set sentence to finished
          if eos_token_id_tensor is not None:
              unfinished_sequences = unfinished_sequences.mul(
                  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)
              )</code></pre>
<p>最后generate方法返回一个Union[GenerateOutput,torch.LongTensor]。一般来说是前者。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">GenerateNonBeamOutput = Union[GenerateDecoderOnlyOutput, GenerateEncoderDecoderOutput]
GenerateBeamOutput = Union[GenerateBeamDecoderOnlyOutput, GenerateBeamEncoderDecoderOutput]
GenerateOutput = Union[GenerateNonBeamOutput, GenerateBeamOutput]</code></pre>
<p>我们就拿<code>GenerateBeamDecoderOnlyOutput</code>来看。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">sequences: torch.LongTensor = None # 返回的序列，需要进行decode，一般只用这个
sequences_scores: Optional[torch.FloatTensor] = None # 序列beam_search的分数
scores: Optional[Tuple[torch.FloatTensor]] = None
logits: Optional[Tuple[torch.FloatTensor]] = None
beam_indices: Optional[torch.LongTensor] = None
attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None</code></pre>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>iroha</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://example.com/post/model_load.html" title="Huggingface的模型加载流程">http://example.com/post/model_load.html</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/post/rl_surface.html" rel="prev" title="DRL introduce"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">DRL introduce</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/post/ft_survey.html" rel="next" title="大模型微调方法综述"><span class="post-nav-text">大模型微调方法综述</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2024 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> iroha</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.9</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>