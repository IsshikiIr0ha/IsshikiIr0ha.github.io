<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="iroha"><meta name="copyright" content="iroha"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>Huggingface的模型加载流程 | iroha</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...undefined?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"example.com","root":"/","title":"不想摆烂","version":"1.10.9","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><meta name="description" content="我们以下面的这一句语句作为开始，以从本地加载模型为例。 model &#x3D; AutoModelForSeq2SeqLM.from_pretrained(&quot;bigscience&#x2F;T0_3B&quot;) inputs &#x3D; tokenizer.encode(q.strip()+&quot; ? To answer this question, we need to">
<meta property="og:type" content="article">
<meta property="og:title" content="Huggingface的模型加载流程">
<meta property="og:url" content="http://example.com/post/model_load.html">
<meta property="og:site_name" content="iroha">
<meta property="og:description" content="我们以下面的这一句语句作为开始，以从本地加载模型为例。 model &#x3D; AutoModelForSeq2SeqLM.from_pretrained(&quot;bigscience&#x2F;T0_3B&quot;) inputs &#x3D; tokenizer.encode(q.strip()+&quot; ? To answer this question, we need to">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20240422014943359.png">
<meta property="article:published_time" content="2024-04-11T08:00:00.000Z">
<meta property="article:modified_time" content="2024-07-22T09:14:59.982Z">
<meta property="article:author" content="iroha">
<meta property="article:tag" content="大模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20240422014943359.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="iroha"><img width="96" loading="lazy" src="/yun.png" alt="iroha"><span class="site-author-status" title="永远相信美好的事情即将发生">😊</span></a><div class="site-author-name"><a href="/about/">iroha</a></div><span class="site-name">iroha</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">14</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">4</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">2</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="友链" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.</span> <span class="toc-text">加载</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E8%AF%8D"><span class="toc-number">2.</span> <span class="toc-text">分词</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%9F%E6%88%90"><span class="toc-number">3.</span> <span class="toc-text">生成</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://example.com/post/model_load.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="iroha"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="iroha"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Huggingface的模型加载流程</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2024-04-11 16:00:00" itemprop="dateCreated datePublished" datetime="2024-04-11T16:00:00+08:00">2024-04-11</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="修改时间：2024-07-22 17:14:59" itemprop="dateModified" datetime="2024-07-22T17:14:59+08:00">2024-07-22</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">NLP</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">大模型</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>我们以下面的这一句语句作为开始，以从本地加载模型为例。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model &#x3D; AutoModelForSeq2SeqLM.from_pretrained(&quot;bigscience&#x2F;T0_3B&quot;)
inputs &#x3D; tokenizer.encode(q.strip()+&quot; ? To answer this question, we need to know&quot;, return_tensors&#x3D;&quot;pt&quot;)
outputs &#x3D; model.generate(inputs.cuda(), max_new_tokens&#x3D;100, do_sample&#x3D;False, top_k&#x3D;50)</code></pre>
<h1 id="加载">加载</h1>
<p>AutoModelForSeq2SeqLM继承了_BaseAutoModelClass，这个类是所有AutoModel的基类，保存在transformers/models/auto/auto_factory.py中。调用的from_pretrained方法实际上就来自于这个基类。我们假设模型保存在本地，一些下载的逻辑不看，且kwargs和config为None。最终会得到模型的哈希值。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if not isinstance(config, PretrainedConfig):
    # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible
    resolved_config_file &#x3D; cached_file(
        pretrained_model_name_or_path,
        CONFIG_NAME,
        _raise_exceptions_for_gated_repo&#x3D;False,
        _raise_exceptions_for_missing_entries&#x3D;False,
        _raise_exceptions_for_connection_errors&#x3D;False,
        **hub_kwargs,
    )
    commit_hash &#x3D; extract_commit_hash(resolved_config_file, commit_hash)</code></pre>
<p>首先需要加载config，通过cached_file来加载。CONFIG_NAME默认为config.json，pretrained_model_name_or_path则是from_pretrained传入的字符串。再看具体实现。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">path_or_repo_id &#x3D; str(path_or_repo_id) # &quot;bigscience&#x2F;T0_3B&quot;
    full_filename &#x3D; os.path.join(subfolder, filename) # filename&#x3D;config.json
    if os.path.isdir(path_or_repo_id):
        resolved_file &#x3D; os.path.join(os.path.join(path_or_repo_id, subfolder), filename) # subfolder不指定&#x3D;None
        if not os.path.isfile(resolved_file):
            if _raise_exceptions_for_missing_entries:
                raise EnvironmentError(
                    f&quot;&#123;path_or_repo_id&#125; does not appear to have a file named &#123;full_filename&#125;. Checkout &quot;
                    f&quot;&#39;https:&#x2F;&#x2F;huggingface.co&#x2F;&#123;path_or_repo_id&#125;&#x2F;tree&#x2F;&#123;revision&#125;&#39; for available files.&quot;
                )
            else:
                return None
        return resolved_file # 返回bigscience&#x2F;T0_3B&#x2F;config.json</code></pre>
<p>此时对应的config.json已经被加载到内存中，之后需要加载到AutoConfig中。可以看到就是T0_3B/config.json</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">            config, kwargs &#x3D; AutoConfig.from_pretrained(
                pretrained_model_name_or_path,
                return_unused_kwargs&#x3D;True,
                trust_remote_code&#x3D;trust_remote_code,
                code_revision&#x3D;code_revision,
                _commit_hash&#x3D;commit_hash,
                **hub_kwargs,
                **kwargs,
            )
---------config---------
T5Config &#123;
  &quot;_name_or_path&quot;: &quot;&#x2F;data2&#x2F;wtf&#x2F;model&#x2F;bigscience&#x2F;T0_3B&quot;,
  &quot;architectures&quot;: [
    &quot;T5ForConditionalGeneration&quot;
  ],
  &quot;classifier_dropout&quot;: 0.0,
  &quot;d_ff&quot;: 5120,
  &quot;d_kv&quot;: 64,
  &quot;d_model&quot;: 2048,
  &quot;decoder_start_token_id&quot;: 0,
  &quot;dense_act_fn&quot;: &quot;gelu_new&quot;,
  &quot;dropout_rate&quot;: 0.1,
  &quot;eos_token_id&quot;: 1,
  &quot;feed_forward_proj&quot;: &quot;gated-gelu&quot;,
  &quot;gradient_checkpointing&quot;: false,
  &quot;initializer_factor&quot;: 1.0,
  &quot;is_encoder_decoder&quot;: true,
  &quot;is_gated_act&quot;: true,
  &quot;layer_norm_epsilon&quot;: 1e-06,
  &quot;model_type&quot;: &quot;t5&quot;,
  &quot;num_decoder_layers&quot;: 24,
  &quot;num_heads&quot;: 32,
  &quot;num_layers&quot;: 24,
  &quot;output_past&quot;: true,
  &quot;pad_token_id&quot;: 0,
  &quot;relative_attention_max_distance&quot;: 128,
  &quot;relative_attention_num_buckets&quot;: 32,
  &quot;tie_word_embeddings&quot;: false,
  &quot;transformers_version&quot;: &quot;4.38.2&quot;,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 32128
&#125;</code></pre>
<p>最终将模型加载，返回model实例。model_class就是json中的architectures对应值。_get_model_class方法就是得到模型的类型！</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">          model_class &#x3D; _get_model_class(config, cls._model_mapping) # T5ForConditionalGeneration
  		&quot;&quot;&quot;
  		&quot;architectures&quot;: [
  &quot;T5ForConditionalGeneration&quot;
]
			就是返回arch中的模型类型
  		&quot;&quot;&quot;
          return model_class.from_pretrained(
              pretrained_model_name_or_path, *model_args, config&#x3D;config, **hub_kwargs, **kwargs
          )</code></pre>
<p>上面的<code>cls._model_mapping</code>就是模型根据你的输入，得到当前模型类型的映射。</p>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">class AutoModelForSeq2SeqLM(_BaseAutoModelClass):
    _model_mapping &#x3D; MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING
------
MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING &#x3D; _LazyAutoMapping(
    CONFIG_MAPPING_NAMES, MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES
)
</code></pre>
<p><code>CONFIG_MAPPING_NAMES</code>是根据你传入的路径来匹配，而<code>MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES</code>同理，也有和下图类似的字典。本例中<code>CONFIG_MAPPING_NAMES="T5Config"</code>，<code>MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES=T5ForConditionalGeneration</code>，和config.json中的一样。</p>
<figure>
<img src="../images/image-20240422014943359.png"
alt="image-20240422014943359" />
<figcaption aria-hidden="true">image-20240422014943359</figcaption>
</figure>
<p><code>model_class.from_pretrained</code>中的<code>from_pretrained</code>来自于PretrainedModel类，这是所有模型的基类(注意不是AutoModel)。这个函数是本篇的核心。返回的模型实例默认是开启model.eval()模式，若要微调or训练模型，需要手动指定model.train()。</p>
<p>这里顺便提一嘴model.train和eval下的区别：</p>
<ol type="1">
<li>Dropout 和 BatchNorm 行为。model.train()下 Dropout
层会随机丢弃一部分神经元, BatchNorm 层会计算当前 batch 的统计量。
model.eval()下Dropout 层会全部保留神经元, BatchNorm
层会使用训练好的滑动平均统计量。</li>
<li>梯度与优化器。model.train()会计算梯度并更新模型参数。model.eval()不会计算梯度,
也不会更新模型参数。</li>
<li>数据增强。model.train()通常会应用一些数据增强技术,
如翻转、旋转等。model.eval()一般不需要数据增强,
直接使用原始的输入数据。</li>
<li>内存与计算开销。model.train()需要保存中间激活值用于反向传播,
计算开销相对更大。model.eval()只需要前向传播, 不需要保存中间激活值,
计算开销相对更小。</li>
</ol>
<p>下面看几个比较关键的参数。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">@classmethod
def from_pretrained(
    cls,
    pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
    *model_args,
    config: Optional[Union[PretrainedConfig, str, os.PathLike]] &#x3D; None,
    cache_dir: Optional[Union[str, os.PathLike]] &#x3D; None,
    ignore_mismatched_sizes: bool &#x3D; False,
    force_download: bool &#x3D; False,
    local_files_only: bool &#x3D; False,
    token: Optional[Union[str, bool]] &#x3D; None,
    revision: str &#x3D; &quot;main&quot;,
    use_safetensors: bool &#x3D; None,
    **kwargs,
):</code></pre>
<ul>
<li><p>pretrained_model_name_or_path，模型的路径or在huggingface中的名字。</p></li>
<li><p>force_download，不论有没有下载好模型，都下载，若存在则覆盖。</p></li>
<li><p>torch_dtype，<code>torch.float16</code> or
<code>torch.bfloat16</code> or
<code>torch.float</code>，指定模型参数的载入精度，不指定则默认为<code>torch.float</code>。也就是说，<code>config.json</code>
中的 <code>torch_dtype</code> 设置拥有最高优先级。如果
<code>torch_dtype</code> 参数被设置为
<code>"auto"</code>，那么它会首先使用 <code>config.json</code>
中的设置。只有当 <code>config.json</code> 中没有找到
<code>torch_dtype</code> 且 <code>torch_dtype</code> 参数被设置为
<code>"auto"</code>时，它才会回退到使用权重checkpoint中的数据类型，查看第一个数据是什么类型就用什么类型。若根本没有设置该参数，则使用torch.float。</p></li>
<li><p>device_map，可以传入三种类型的参数。<strong>字符串类型</strong>，如果传入一个字符串类型的设备名称(例如
"cpu", "cuda:1", "mps")，那么整个模型会被分配到指定的设备上，如果传入
"auto"，Accelerate
库会自动计算出最优的设备分布。<strong>字典类型</strong>，这种情况下
device_map
是一个字典,键是模型的子模块名称,值是对应的设备编号或设备名称，这允许用户手动指定模型的各个子模块应该分布在哪些设备上，只需要指定到模块名称的级别,子模块会自动分配到同一设备，如</p>
<pre class="line-numbers language-none"><code class="language-none">device_map &#x3D; &#123;
    &quot;transformer.encoder&quot;: &quot;cuda:0&quot;,
    &quot;transformer.decoder&quot;: &quot;cuda:1&quot;,
    &quot;transformer.pooler&quot;: &quot;cuda:0&quot;,
    &quot;lm_head&quot;: &quot;cuda:1&quot;
&#125;</code></pre>
<p>还可以传入整数或<code>torch.device</code>，代表将整个模型放在指定编号的
GPU
上。如<code>device = torch.device("cuda:1"),device_map = deveice</code>。只要指定了device_map，那么都会让
<code>low_cpu_mem_usage=True</code>。不指定就用cpu。</p></li>
<li><p>quantization_config，指定模型的量化策略。可以是一个字典或者继承自
<code>QuantizationConfigMixin</code>
的对象，它用于配置模型的量化参数。除了 <code>quantization_config</code>
之外,还可以使用 <code>load_in_4bit</code> 和 <code>load_in_8bit</code>
等参数来指定量化方式,但这种方式不被推荐，只量化了参数，并不量化梯度。但推理阶段无所谓。下面是一个例子。</p></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import bitsandbytes as bnb
from transformers import QuantizationConfig

quantization_config &#x3D; QuantizationConfig(
    quantization_method&#x3D;bnb.QuantizationMethod.DYNAMIC_QUANT,
    weight_bits&#x3D;8,# 权重为INT8
    grad_bits&#x3D;8,# 梯度也INT8
    per_channel&#x3D;False
)
model &#x3D; AutoModelForSeq2SeqLM.from_pretrained(
    &quot;bigscience&#x2F;T0_3B&quot;,
    quantization_config&#x3D;quantization_config
)</code></pre>
<ul>
<li>local_files_only，如果是True，则不会从Hub上下载。</li>
<li>low_cpu_mem_usage，作用是尝试在加载模型时不使用超过模型大小 1 倍的
CPU 内存(包括峰值内存)。</li>
<li>attn_implementation，可以选择<code>flash_attention_2</code>,<code>sdpa(default)</code>,<code>eager(手动实现)</code></li>
</ul>
<p>之后看几处比较关键的源码。</p>
<p>从传入的路径中提取config。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># Load config if we don&#39;t provide a configuration
if not isinstance(config, PretrainedConfig):
    config_path &#x3D; config if config is not None else pretrained_model_name_or_path
    config, model_kwargs &#x3D; cls.config_class.from_pretrained</code></pre>
<p>量化操作。注意到量化操作会强制开启low_cpu_mem_usage。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pre_quantized &#x3D; getattr(config, &quot;quantization_config&quot;, None) is not None
if pre_quantized or quantization_config is not None:
    if pre_quantized:
        config.quantization_config &#x3D; AutoHfQuantizer.merge_quantization_configs(
            config.quantization_config, quantization_config
        )
    else:
        config.quantization_config &#x3D; quantization_config
    hf_quantizer &#x3D; AutoHfQuantizer.from_config(config.quantization_config, pre_quantized&#x3D;pre_quantized)
else:
    hf_quantizer &#x3D; None

if hf_quantizer is not None:
    hf_quantizer.validate_environment(
        torch_dtype&#x3D;torch_dtype, from_tf&#x3D;from_tf, from_flax&#x3D;from_flax, device_map&#x3D;device_map
    )
    torch_dtype &#x3D; hf_quantizer.update_torch_dtype(torch_dtype)
    device_map &#x3D; hf_quantizer.update_device_map(device_map)

    # Force-set to &#96;True&#96; for more mem efficiency
    if low_cpu_mem_usage is None:
        low_cpu_mem_usage &#x3D; True
        logger.warning(&quot;&#96;low_cpu_mem_usage&#96; was None, now set to True since model is quantized.&quot;)
is_quantized &#x3D; hf_quantizer is not None</code></pre>
<p>加载权重，tf相关的就不看了。在加载pytorch权重中，会去你指定的文件夹中找<code>pytorch_model.bin</code>这个权重文件。<code>subfolder,variant</code>若不在参数中指定都为空字符。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">elif os.path.isfile(
    os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_NAME, variant))
):
    # Load from a PyTorch checkpoint,会拼成model_path&#x2F;pytorch_model.bin
    archive_file &#x3D; os.path.join(
        pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_NAME, variant)
    )
</code></pre>
<p>一些模型的权重可能以多个checkpoint文件来保存，这时候要求有一个<code>WEIGHTS_INDEX_NAME = "pytorch_model.bin.index.json"</code>文件来进行索引。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">                elif os.path.isfile(
                    os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_INDEX_NAME, variant))
                ):
                    # Load from a sharded PyTorch checkpoint
                    archive_file &#x3D; os.path.join(
                        pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_INDEX_NAME, variant)
                    )
                    is_sharded &#x3D; True # 注意这里
----
&#123;
    &quot;checkpoint_files&quot;: [&quot;pytorch_model.bin.0&quot;, &quot;pytorch_model.bin.1&quot;, &quot;pytorch_model.bin.2&quot;],
    &quot;num_checkpoint_files&quot;: 3,
    &quot;size_checkpoint_files&quot;: [100000, 200000, 50000],
    &quot;weight_map&quot;: &#123;
        &quot;layer1.weight&quot;: [0, 0],
        &quot;layer1.bias&quot;: [0, 50000],
        &quot;layer2.weight&quot;: [1, 0],
        &quot;layer2.bias&quot;: [1, 100000]
    &#125;
&#125;</code></pre>
<p>还有一种情况，就是指定的路径不是一个文件夹，而是权重文件本身，如<code>bigscience/T0_3B/pytorch_model.bin</code>，那也可以加载。因为最终都是让<code>archive_file = weight_file</code>。</p>
<pre class="line-numbers language-none"><code class="language-none">elif os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):
    archive_file &#x3D; pretrained_model_name_or_path
    is_local &#x3D; True</code></pre>
<p>最终，<code>resolved_archive_file = archive_file</code>，获取权重文件路径。如果是分散的checkpoint，也就是<code>is_sharded</code>是True，还要进行额外的操作，这里就不深入了。</p>
<p>接下来就要加载权重了，首先判断是不是pytorch，若是，则加载权重文件。详细的加载源码就不赘述，最终会返回由torch.load加载模型结构和权重参数。</p>
<pre class="line-numbers language-none"><code class="language-none">if from_pt:
    if not is_sharded and state_dict is None:
        # Time to load the checkpoint
        state_dict &#x3D; load_state_dict(resolved_archive_file)</code></pre>
<p>接下来决定权重的数据类型，正如上面交代torch_dtype参数所说，先考虑<code>torch_dtype=auto</code>，也就是config.json中的数据类型。然后再考虑</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if torch_dtype is not None:
    if isinstance(torch_dtype, str):
        if torch_dtype &#x3D;&#x3D; &quot;auto&quot;:
            if hasattr(config, &quot;torch_dtype&quot;) and config.torch_dtype is not None:
                torch_dtype &#x3D; config.torch_dtype
                logger.info(f&quot;Will use torch_dtype&#x3D;&#123;torch_dtype&#125; as defined in model&#39;s config object&quot;)

        else:
            raise ValueError(
                f&#39;&#96;torch_dtype&#96; can be either &#96;torch.dtype&#96; or &#96;&quot;auto&quot;&#96;, but received &#123;torch_dtype&#125;&#39;
            )
    dtype_orig &#x3D; cls._set_default_torch_dtype(torch_dtype)</code></pre>
<p>若是分片情况，则去分片json中找有没有指定。如果不是分片的情况，则按权重文件中第一个数据的类型。若不显式指定torch_dtype(None)，则使用float32。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">                    else:
                        if is_sharded and &quot;dtype&quot; in sharded_metadata:
                            torch_dtype &#x3D; sharded_metadata[&quot;dtype&quot;]
                        elif not is_sharded:
                            torch_dtype &#x3D; get_state_dict_dtype(state_dict)
                        else:
                            one_state_dict &#x3D; load_state_dict(resolved_archive_file[0])
                            torch_dtype &#x3D; get_state_dict_dtype(one_state_dict)
                            del one_state_dict  # free CPU memory
                        logger.info(
                            &quot;Since the &#96;torch_dtype&#96; attribute can&#39;t be found in model&#39;s config object, &quot;
                            &quot;will use torch_dtype&#x3D;&#123;torch_dtype&#125; as derived from model&#39;s weights&quot;
                        )    
-------------get_state_dict_dtype(state_dict)---------
# if no floating dtype was found return whatever the first dtype is
else:
    return next(state_dict.values()).dtype</code></pre>
<p>还有混合精度的情况，在初始nn.Module的时候可以设置单独设置_keep_in_fp32_modules哪些模块保持fp32精度。</p>
<pre class="line-numbers language-none"><code class="language-none"># Check if &#96;_keep_in_fp32_modules&#96; is not None
use_keep_in_fp32_modules &#x3D; (cls._keep_in_fp32_modules is not None) and (
    (torch_dtype &#x3D;&#x3D; torch.float16) or hasattr(hf_quantizer, &quot;use_keep_in_fp32_modules&quot;)
)</code></pre>
<p>创建模型实例。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">with ContextManagers(init_contexts):
    # Let&#39;s make sure we don&#39;t run the init function of buffer modules
    model &#x3D; cls(config, *model_args, **model_kwargs)</code></pre>
<p>来看device_map的逻辑。首先是字符串的情况，必须要是<code>"auto", "balanced", "balanced_low_0", "sequential"</code>这几种，否则报错。"auto"会自动操作，尽可能均匀地分配计算负载。<code>balanced</code>则是平均分配模型层中的参数给不同的卡。<code>balanced_low_0</code>则是少给0分一些，因为0往往还有其他事情要做。<code>sequential</code>则是按模型层的顺序来分配给不同的卡，保持模型层的拓扑结构,减少跨设备的数据传输，如attention一张卡，MLP一张卡。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if device_map not in [&quot;auto&quot;, &quot;balanced&quot;, &quot;balanced_low_0&quot;, &quot;sequential&quot;]:
        raise ValueError
if device_map !&#x3D; &quot;sequential&quot;:
                max_memory &#x3D; get_balanced_memory(
                    model,
                    dtype&#x3D;target_dtype,
                    low_zero&#x3D;(device_map &#x3D;&#x3D; &quot;balanced_low_0&quot;),
                    max_memory&#x3D;max_memory,
                    **device_map_kwargs,
                )</code></pre>
<p>实际上可以看到，<code>auto</code>就是<code>balanced</code>策略。</p>
<p>其他情况，输入的什么设备就绑定什么设备，若没有指定device_map，就加载到cpu。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if isinstance(device_map, torch.device):
     device_map &#x3D; &#123;&quot;&quot;: device_map&#125;
elif isinstance(device_map, str) and device_map not in [&quot;auto&quot;, &quot;balanced&quot;, &quot;balanced_low_0&quot;, &quot;sequential&quot;]:
      try:
          device_map &#x3D; &#123;&quot;&quot;: torch.device(device_map)&#125;
elif isinstance(device_map, int):# 小于0报错
	device_map &#x3D; &#123;&quot;&quot;: device_map&#125;</code></pre>
<p>这里有一个<code>tie_weights</code>函数，实现了参数的绑定操作，本质上就是默认让输入嵌入层和输出嵌入层的权重绑定在一起。若是在config中指定<code>is_encoder_decoder=True</code>且<code>tie_encoder_decoder=True</code>，那么Encoder和Decoder的参数也会共用(都使用Decoder的Weights)，不过T5中并不这么做，一般是<strong>BERT-based</strong>模型在微调成Encoder-Decoder模型的时候会这么做。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def tie_weights(self):
    &quot;&quot;&quot;
    Tie the weights between the input embeddings and the output embeddings.

    If the &#96;torchscript&#96; flag is set in the configuration, can&#39;t handle parameter sharing so we are cloning the
    weights instead.
    &quot;&quot;&quot;
    if getattr(self.config, &quot;tie_word_embeddings&quot;, True):
        output_embeddings &#x3D; self.get_output_embeddings()
        if output_embeddings is not None:
            self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())

    if getattr(self.config, &quot;is_encoder_decoder&quot;, False) and getattr(self.config, &quot;tie_encoder_decoder&quot;, False):
        if hasattr(self, self.base_model_prefix):
            self &#x3D; getattr(self, self.base_model_prefix)
        self._tie_encoder_decoder_weights(self.encoder, self.decoder, self.base_model_prefix)

    for module in self.modules():
        if hasattr(module, &quot;_tie_weights&quot;):
            module._tie_weights()</code></pre>
<p>开启model.eval。</p>
<pre class="line-numbers language-none"><code class="language-none">model.eval()</code></pre>
<p>若模型是生成式模型，那么还需要配置生成的参数。<code>GenerationConfig</code>实际上就是model.generate()方法中所要用的参数。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if model.can_generate() and pretrained_model_name_or_path is not None:
    try:
        model.generation_config &#x3D; GenerationConfig.from_pretrained(
            pretrained_model_name_or_path,
            cache_dir&#x3D;cache_dir,
            force_download&#x3D;force_download,
            resume_download&#x3D;resume_download,
            proxies&#x3D;proxies,
            local_files_only&#x3D;local_files_only,
            token&#x3D;token,
            revision&#x3D;revision,
            subfolder&#x3D;subfolder,
            _from_auto&#x3D;from_auto_class,
            _from_pipeline&#x3D;from_pipeline,
            **kwargs,
        )
    except OSError:
        logger.info(
            &quot;Generation config file not found, using a generation config created from the model config.&quot;
        )
        pass</code></pre>
<p>最终输出一些加载参数时输出的信息，然后返回模型。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if output_loading_info:
    if loading_info is None:
        loading_info &#x3D; &#123;
            &quot;missing_keys&quot;: missing_keys,
            &quot;unexpected_keys&quot;: unexpected_keys,
            &quot;mismatched_keys&quot;: mismatched_keys,
            &quot;error_msgs&quot;: error_msgs,
        &#125;
    return model, loading_info</code></pre>
<p>总结，根据最常用的方法，主要是做以下几个操作。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model &#x3D; AutoModelForSeq2SeqLM.from_pretrained(&quot;bigscience&#x2F;T0_3B&quot;,device_map&#x3D;&quot;auto&quot;)</code></pre>
<ol type="1">
<li>根据输入路径拿到config.json，加载到Config。</li>
<li>根据输入路径拿到权重文件pytorch_model.bin，由torch.load加载模型结构和权重参数。。</li>
<li>决定权重的数据类型，未指定则是float32。</li>
<li>平均分配参数给各张卡。</li>
<li>绑定input和output的Embedding，让其使用同一份Embedding参数。</li>
<li>model.eval()。</li>
<li>若是生成式模型，配置生成参数。</li>
<li>返回模型实例。</li>
</ol>
<p>所以，实际上会调用两个不同的<code>from_pretrained</code>方法，第一个是AutoModel基类_BaseAutoModelClass的，在最后调用<code>get_model_class</code>方法得到模型本身的类，本例中是<code>T5ForConditionalGeneration</code>，然后再调用这个类的<code>from_pretrained</code>，而这个类的<code>from_pretrained</code>在其基类<code>PreTrainedModel</code>实现，所以再会调用<code>PreTrainedModel</code>的<code>from_pretrained</code>方法。分析完毕。</p>
<h1 id="分词">分词</h1>
<p>后续更新，挖坑</p>
<h1 id="生成">生成</h1>
<p>并不是每一个模型都可以使用.generate()进行序列生成，需要通过函数判断是否能够进行序列生成任务，所以每一个模型都需要重写<code>prepare_inputs_for_generation</code>方法。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin, PeftAdapterMixin):
	    @classmethod
    def can_generate(cls) -&gt; bool:
        &quot;&quot;&quot;
        Returns whether this model can generate sequences with &#96;.generate()&#96;.

        Returns:
            &#96;bool&#96;: Whether this model can generate sequences with &#96;.generate()&#96;.
        &quot;&quot;&quot;
        # Detects whether &#96;prepare_inputs_for_generation&#96; has been overwritten, which is a requirement for generation.
        # Alternativelly, the model can also have a custom &#96;generate&#96; function.
        if &quot;GenerationMixin&quot; in str(cls.prepare_inputs_for_generation) and &quot;GenerationMixin&quot; in str(cls.generate):
            return False
        return True</code></pre>
<p>而generate方法本身就在<code>GenerationMixin</code>类中实现。</p>
<pre class="line-numbers language-none"><code class="language-none">Class that holds a configuration for a generation task. A &#96;generate&#96; call supports the following generation methods
for text-decoder, text-to-text, speech-to-text, and vision-to-text models:

    - *greedy decoding* by calling [&#96;~generation.GenerationMixin._greedy_search&#96;] if &#96;num_beams&#x3D;1&#96; and
        &#96;do_sample&#x3D;False&#96;
    - *contrastive search* by calling [&#96;~generation.GenerationMixin._contrastive_search&#96;] if &#96;penalty_alpha&gt;0.&#96;
        and &#96;top_k&gt;1&#96;
    - *multinomial sampling* by calling [&#96;~generation.GenerationMixin._sample&#96;] if &#96;num_beams&#x3D;1&#96; and
        &#96;do_sample&#x3D;True&#96;
    - *beam-search decoding* by calling [&#96;~generation.GenerationMixin._beam_search&#96;] if &#96;num_beams&gt;1&#96; and
        &#96;do_sample&#x3D;False&#96;
    - *beam-search multinomial sampling* by calling [&#96;~generation.GenerationMixin._beam_sample&#96;] if
        &#96;num_beams&gt;1&#96; and &#96;do_sample&#x3D;True&#96;
    - *diverse beam-search decoding* by calling [&#96;~generation.GenerationMixin._group_beam_search&#96;], if
        &#96;num_beams&gt;1&#96; and &#96;num_beam_groups&gt;1&#96;
    - *constrained beam-search decoding* by calling [&#96;~generation.GenerationMixin._constrained_beam_search&#96;], if
        &#96;constraints!&#x3D;None&#96; or &#96;force_words_ids!&#x3D;None&#96;
    - *assisted decoding* by calling [&#96;~generation.GenerationMixin._assisted_decoding&#96;], if
        &#96;assistant_model&#96; or &#96;prompt_lookup_num_tokens&#96; is passed to &#96;.generate()&#96;

You do not need to call any of the above methods directly. Pass custom parameter values to &#39;.generate()&#39;. To learn
more about decoding strategies refer to the [text generation strategies guide](..&#x2F;generation_strategies).</code></pre>
<p>就不看详细的实现，先看<code>GenerationConfig</code>的生成策略。若不用GenerationConfig，也可以直接输入kwargs。</p>
<ul>
<li>greedy
decoding贪婪解码，<code>num_beams=1</code>且<code>do_sample=False</code>，会一直选择概率最高的token，一条路走到黑。</li>
<li>Contrastive
search对比搜索，在NIPS22被提出，能在保持流畅性的前提下，鼓励多样性生成，减少重复输出。需要<code>penalty_alpha&gt;0</code>
and
<code>top_k&gt;1</code>。一个候选token与当前token非常相似(相似度得分高)，那么它的概率就会被较多地降低。这样做的目的是鼓励生成更加多样化的文本，避免同类型的token过于集中出现。最后,算法在经过调整的
<code>scores</code>
向量上取Top-1。核心公式<code>scores = (1.0 - alpha) * next_top_k_probs - alpha * scores</code>。<code>next_top_k_probs</code>是当前token的Top-k概率分布，等式右边的<code>scores</code>是当前token和下一个token之间的相似度分数。所以当前token与next
token越相似，惩罚就越大。</li>
<li>multinomial sampling，<code>num_beams=1</code>
and<code>do_sample=True</code>。和贪婪解码的区别不一定选择概率最高的token，而是根据概率分布来采样。</li>
<li>beam-search，<code>num_beams&gt;1</code>
and<code>do_sample=False</code>，保留top-k个得分最高的候选序列，称为"beam"。这里选择不采样，是选择得分最高的2*num_beams个token。</li>
<li>diverse beam-search，<code>num_beams&gt;1</code> and
<code>num_beam_groups&gt;1</code>，通过分组机制，确保了不同beam之间的差异性。</li>
</ul>
<p>接下来介绍一些比较常用的参数。</p>
<ul>
<li><code>do_sample</code>，是否根据概率分布采样。</li>
<li><code>temperature</code>，默认1.0。小于1时，当
<code>temperature &lt; 1.0</code> 时,
生成概率分布会被"平滑"(峰值变得更陡峭)，使得模型更倾向于选择概率较高的token，生成的文本会更加集中和保守。当<code>temperature &gt; 1.0</code>时，
生成概率分布会被"拉平"(峰值变得更平缓)，使得模型会选择概率较低的token，生成的文本会更加多样和探索性。</li>
<li><code>top_k</code>，选择下一个token时，只保留概率最高的前
<code>top_k</code>
个token，有效地避免模型选择概率很低的不合理token。</li>
<li><code>top_p</code>，动态地选择概率总和达到 top_p
阈值的最小token集合。</li>
<li><code>num_return_sequences</code>
，指定要生成的独立序列数量。默认为1，即只生成1个序列。</li>
<li><code>output_scores</code>是否输出每个token的预测分数。</li>
<li><code>output_logits</code>是否输出未经处理的原始预测logits。</li>
<li><code>pad_token_id,bos_token_id,eos_token_id</code>，需要根据模型的词表来看。不设置则为None。</li>
<li><code>max_length</code>:
最大输出长度,包括prompt和生成的新tokens。默认是20</li>
<li><code>max_new_tokens</code>:
最大生成新tokens数量,不包括prompt长度。</li>
<li><code>min_length</code>:
最小输出长度,包括prompt和生成的新tokens。</li>
<li><code>min_new_tokens</code>:
最小生成新tokens数量,不包括prompt长度。</li>
<li><code>early_stopping</code>: 控制beam search停止的条件。可选值为:
<ul>
<li><code>True</code>: 当生成了 <code>num_beams</code>
个完整候选序列时立即停止。</li>
<li><code>False</code>:
根据启发式停止,即当很难找到更好的候选时停止。</li>
<li><code>"never"</code>: 一直运行直到无法找到更好的候选为止。</li>
</ul></li>
</ul>
<p>接下来解析generate函数主要做了哪些。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">@torch.no_grad()
def generate(
    self,
    inputs: Optional[torch.Tensor] &#x3D; None,
    generation_config: Optional[GenerationConfig] &#x3D; None,
    logits_processor: Optional[LogitsProcessorList] &#x3D; None,
    stopping_criteria: Optional[StoppingCriteriaList] &#x3D; None,
    prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] &#x3D; None,
    synced_gpus: Optional[bool] &#x3D; None,
    assistant_model: Optional[&quot;PreTrainedModel&quot;] &#x3D; None,
    streamer: Optional[&quot;BaseStreamer&quot;] &#x3D; None,
    negative_prompt_ids: Optional[torch.Tensor] &#x3D; None,
    negative_prompt_attention_mask: Optional[torch.Tensor] &#x3D; None,
    **kwargs,
) -&gt; Union[GenerateOutput, torch.LongTensor]:</code></pre>
<ul>
<li>inputs，一般是经过tokenizer处理的序列，包含attention_mask的。如果是调用tokenizer.encode()，那么不会有attention_mask。</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 1. Handle &#96;generation_config&#96; and kwargs that might update it, and validate the &#96;.generate()&#96; call
&quot;处理 &#96;generation_config&#96; 和可能更新它的 &#96;kwargs&#96;，并验证 &#96;.generate()&#96; 的调用，略&quot;

# 2. Set generation parameters if not already defined
&quot;&quot;&quot;
略，设置生成所需的一些默认参数
&quot;&quot;&quot;
if generation_config.pad_token_id is None and generation_config.eos_token_id is not None:
    if model_kwargs.get(&quot;attention_mask&quot;, None) is None:
        logger.warning(
            &quot;The attention mask and the pad token id were not set. As a consequence, you may observe &quot;
            &quot;unexpected behavior. Please pass your input&#39;s &#96;attention_mask&#96; to obtain reliable results.&quot;
        )
    eos_token_id &#x3D; generation_config.eos_token_id
    # 多语言模型的eos可能会有多个
    if isinstance(eos_token_id, list):
        eos_token_id &#x3D; eos_token_id[0]
    logger.warning(f&quot;Setting &#96;pad_token_id&#96; to &#96;eos_token_id&#96;:&#123;eos_token_id&#125; for open-end generation.&quot;)
    generation_config.pad_token_id &#x3D; eos_token_id</code></pre>
<p>这一段说明确说明如果你没有传入<code>pad_token_id</code>，那么会以<code>eos_token_id</code>替代。若你没有传入<code>attention_mask</code>，会警告你传入，Attention
Mask中值为0的位置对应的Attention权重设为非常小的负值，通常是-1e9。</p>
<p>接下来就是处理模型的输入。获取输入的tensor和batch_size。<code>_prepare_model_inputs</code>方法过滤掉
model_kwargs中非空且不是模型主要输入的参数。对于文本生成模型，要看模型的encoder是否支持直接输入embedding，否则一律设置成input_ids。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 3. Define model inputs
# inputs_tensor has to be defined
# model_input_name is defined if model-specific keyword input is passed
# otherwise model_input_name is None
# all model-specific keyword inputs are removed from &#96;model_kwargs&#96;
inputs_tensor, model_input_name, model_kwargs &#x3D; self._prepare_model_inputs(
    inputs, generation_config.bos_token_id, model_kwargs
)
batch_size &#x3D; inputs_tensor.shape[0]</code></pre>
<p>decode-only的模型应该使用左对齐。使用右对齐会警告，初始化 tokenizer
时设置
<code>padding_side='left'</code>以确保正确的生成结果。接下来的逻辑都不看了，无非就是处理一些模型生成参数，如max_length。根据不同的生成策略，会运行不同的生成函数，就看一个简单的贪婪解码的部分代码。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">        if generation_mode &#x3D;&#x3D; GenerationMode.GREEDY_SEARCH:
            # 11. run greedy search
            result &#x3D; self._greedy_search(
                input_ids,
                logits_processor&#x3D;prepared_logits_processor, # logits处理器，min_length作用于这个，在满足前减小eos的概率。
                stopping_criteria&#x3D;prepared_stopping_criteria, # 停止判定器，max_length就作用于这个
                pad_token_id&#x3D;generation_config.pad_token_id,
                eos_token_id&#x3D;generation_config.eos_token_id,
                output_attentions &#x3D; generation_config.output_attentions，# 是否输出注意力层分数
                output_hidden_states &#x3D; generation_config.output_hidden_states # 是否返回隐藏状态
                output_scores&#x3D;generation_config.output_scores,
                output_logits&#x3D;generation_config.output_logits,
                return_dict_in_generate&#x3D;generation_config.return_dict_in_generate,
                synced_gpus&#x3D;synced_gpus,
                streamer&#x3D;streamer,
                **model_kwargs,
            )
-----------------------------------------------------------------------------------

    </code></pre>
<p>首先拿到全部的输出，并只需要下一个token的内容。<code>next_tokens</code>在序列结束的情况下，一定是pad_id。生成后更新<code>input_ids</code>，若生成了eos_id，就认为序列已经完成。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device&#x3D;input_ids.device):            
  		outputs &#x3D; self(
              **model_inputs,
              return_dict&#x3D;True,
              output_attentions&#x3D;output_attentions,
              output_hidden_states&#x3D;output_hidden_states,
          )
          next_token_logits &#x3D; outputs.logits[:, -1, :] # 最后一个时间步
          next_tokens_scores &#x3D; logits_processor(input_ids, next_token_logits)
          next_tokens &#x3D; torch.argmax(next_tokens_scores, dim&#x3D;-1) # 最大值索引，贪婪策略
          # finished sentences should have their next token be a padding token
          if eos_token_id is not None:
              if pad_token_id is None:
                  raise ValueError(&quot;If &#96;eos_token_id&#96; is defined, make sure that &#96;pad_token_id&#96; is defined.&quot;)
                 	# unfinished_sequences初始化是torch.ones(batch_size,dtype &#x3D; torch.long)                   
              next_tokens &#x3D; next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)
          # update generated ids, model inputs, and length for next step
          input_ids &#x3D; torch.cat([input_ids, next_tokens[:, None]], dim&#x3D;-1)
          # if eos_token was found in one sentence, set sentence to finished
          if eos_token_id_tensor is not None:
              unfinished_sequences &#x3D; unfinished_sequences.mul(
                  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim&#x3D;0)
              )</code></pre>
<p>最后generate方法返回一个Union[GenerateOutput,torch.LongTensor]。一般来说是前者。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">GenerateNonBeamOutput &#x3D; Union[GenerateDecoderOnlyOutput, GenerateEncoderDecoderOutput]
GenerateBeamOutput &#x3D; Union[GenerateBeamDecoderOnlyOutput, GenerateBeamEncoderDecoderOutput]
GenerateOutput &#x3D; Union[GenerateNonBeamOutput, GenerateBeamOutput]</code></pre>
<p>我们就拿<code>GenerateBeamDecoderOnlyOutput</code>来看。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">sequences: torch.LongTensor &#x3D; None # 返回的序列，需要进行decode，一般只用这个
sequences_scores: Optional[torch.FloatTensor] &#x3D; None # 序列beam_search的分数
scores: Optional[Tuple[torch.FloatTensor]] &#x3D; None
logits: Optional[Tuple[torch.FloatTensor]] &#x3D; None
beam_indices: Optional[torch.LongTensor] &#x3D; None
attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] &#x3D; None
hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] &#x3D; None
past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] &#x3D; None</code></pre>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>iroha</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://example.com/post/model_load.html" title="Huggingface的模型加载流程">http://example.com/post/model_load.html</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/post/rl_surface.html" rel="prev" title="DRL introduce"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">DRL introduce</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/post/ft_survey.html" rel="next" title="大模型微调方法综述"><span class="post-nav-text">大模型微调方法综述</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2024 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> iroha</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.9</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>