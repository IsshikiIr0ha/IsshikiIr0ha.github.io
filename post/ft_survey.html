<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="iroha"><meta name="copyright" content="iroha"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>大模型微调方法综述 | iroha</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...undefined?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"example.com","root":"/","title":"不想摆烂","version":"1.10.9","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><meta name="description" content="背景 ​ 随着模型的参数越来越大，为每一个任务从头训练一个模型越来越困难，而参数量的增大也让模型理解了更深层的语义信息。随着BERT的出现，预训练+微调的方式走上历史舞台。 ​ 为什么不选择全参数微调呢？最主要的原因还是成本太高，在消费级硬件上愈发困难。以GPT2-1.5B为例，若参数与梯度以fp16保存，显存需要保存3+3 &#x3D; 6B的数据，若使用Adam优化器，那么还需要保存fp32">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型微调方法综述">
<meta property="og:url" content="http://example.com/post/ft_survey.html">
<meta property="og:site_name" content="iroha">
<meta property="og:description" content="背景 ​ 随着模型的参数越来越大，为每一个任务从头训练一个模型越来越困难，而参数量的增大也让模型理解了更深层的语义信息。随着BERT的出现，预训练+微调的方式走上历史舞台。 ​ 为什么不选择全参数微调呢？最主要的原因还是成本太高，在消费级硬件上愈发困难。以GPT2-1.5B为例，若参数与梯度以fp16保存，显存需要保存3+3 &#x3D; 6B的数据，若使用Adam优化器，那么还需要保存fp32">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20240408231535224.png">
<meta property="og:image" content="http://example.com/images/image-20240410215838916.png">
<meta property="og:image" content="http://example.com/images/image-20240411005445372.png">
<meta property="article:published_time" content="2024-04-08T04:00:00.000Z">
<meta property="article:modified_time" content="2024-04-12T19:32:20.406Z">
<meta property="article:author" content="iroha">
<meta property="article:tag" content="大模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20240408231535224.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="iroha"><img width="96" loading="lazy" src="/yun.png" alt="iroha"><span class="site-author-status" title="永远相信美好的事情即将发生">😊</span></a><div class="site-author-name"><a href="/about/">iroha</a></div><a class="site-name" href="/about/site.html">iroha</a><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">14</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">4</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">2</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="友链" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#freeze"><span class="toc-number">2.</span> <span class="toc-text">Freeze</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#bitfit"><span class="toc-number">3.</span> <span class="toc-text">Bitfit</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#prompt-tuning"><span class="toc-number">4.</span> <span class="toc-text">Prompt-tuning</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#lora"><span class="toc-number">5.</span> <span class="toc-text">Lora</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-number">5.1.</span> <span class="toc-text">原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E9%80%9F%E8%AF%BB"><span class="toc-number">5.2.</span> <span class="toc-text">源码速读</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#get_peft_model"><span class="toc-number">5.2.1.</span> <span class="toc-text">get_peft_model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#peftmodel"><span class="toc-number">5.2.2.</span> <span class="toc-text">PeftModel</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#loramodel"><span class="toc-number">5.2.3.</span> <span class="toc-text">LoraModel</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#loralayer"><span class="toc-number">5.2.4.</span> <span class="toc-text">LoraLayer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#linear"><span class="toc-number">5.2.5.</span> <span class="toc-text">Linear</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%88%E5%B9%B6%E6%93%8D%E4%BD%9C"><span class="toc-number">5.2.6.</span> <span class="toc-text">合并操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">6.</span> <span class="toc-text">参考资料</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://example.com/post/ft_survey.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="iroha"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="iroha"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">大模型微调方法综述</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2024-04-08 12:00:00" itemprop="dateCreated datePublished" datetime="2024-04-08T12:00:00+08:00">2024-04-08</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="修改时间：2024-04-13 03:32:20" itemprop="dateModified" datetime="2024-04-13T03:32:20+08:00">2024-04-13</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">NLP</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">大模型</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><h1 id="背景">背景</h1>
<p>​
随着模型的参数越来越大，为每一个任务从头训练一个模型越来越困难，而参数量的增大也让模型理解了更深层的语义信息。随着BERT的出现，预训练+微调的方式走上历史舞台。</p>
<p>​
为什么不选择全参数微调呢？最主要的原因还是成本太高，在消费级硬件上愈发困难。以GPT2-1.5B为例，若参数与梯度以fp16保存，显存需要保存3+3
=
6B的数据，若使用Adam优化器，那么还需要保存fp32的模型参数备份，momentum与variance，又需要6+6+6
=
18G，总共需要24G。再加上其他状态，如activation，buffer，还有显存碎片无法利用，实际上需要的显存是大于24G的。</p>
<p>​
全参数微调还有一个问题，当LLM尝试学习多个连续任务时，容易忘记之前学到的东西，也就是“灾难性遗忘”。如何在保留先前知识的基础上增量地增强LLM，即进行持续学习，至关重要。简单来说，全量微调有overfitting，灾难性遗忘，成本高的问题。因此，高效微调模型的参数就成了一个新方向，PEFT(Parameter-Efficient
Fine-tuning)应运而生。</p>
<p>​
微调一般只更改模型的部分参数，调整哪些参数，如何调整则对应了不同方法。一般从下游任务出发，来决定下游应该如何添加参数。</p>
<p>​
下面我们以transformers库中的BERT为例。我们都知道，BERT的预训练是MLM与NSP，所以下游任务自然有这两者，也就是<strong>下一句预测</strong>和<strong>完形填空</strong>。所以这才应该是BERT的原生下游任务。</p>
<p>​ 首先看NSP，模型的结构如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># assert logits[0, 0] &lt; logits[0, 1]  # next sentence was random
class BertForNextSentencePrediction(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)

        self.bert &#x3D; BertModel(config)
        self.cls &#x3D; BertOnlyNSPHead(config)

        # Initialize weights and apply final processing
        self.post_init()

        
class BertOnlyNSPHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.seq_relationship &#x3D; nn.Linear(config.hidden_size, 2)

    def forward(self, pooled_output):
        seq_relationship_score &#x3D; self.seq_relationship(pooled_output)
        return seq_relationship_score</code></pre>
<p>​
可以看到，对于NSP任务，就是在Model的后面拼一个Linear层，将768维映射到2维，本质上是一个二分类问题。</p>
<p>​
再看SequenceClassification任务，也就是序列分类。实际上也是在最后填充了一层Linear层用于分类。与NSP的区别在于pooler层会先经过一次dropout，dropout的概率可以在config中设置，默认为0.1。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class BertForSequenceClassification(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels &#x3D; config.num_labels
        self.config &#x3D; config

        self.bert &#x3D; BertModel(config)
        classifier_dropout &#x3D; (
            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob
        )
        self.dropout &#x3D; nn.Dropout(classifier_dropout)
        self.classifier &#x3D; nn.Linear(config.hidden_size, config.num_labels)</code></pre>
<p>​ <img src="../images/image-20240408231535224.png"
alt="image-20240408231535224" /></p>
<p>​ PEFT按核心思想可以分为三类：</p>
<ol type="1">
<li>添加一些参数量小的层，只微调这些层，最典型的就是LST。</li>
<li>选择某些层，或层中的某一部分进行微调，最典型的是BitFit，只对bias进行微调。</li>
<li>重参数化，也算是增加一部分参数，但最后加回原参数的对应部分</li>
</ol>
<h1 id="freeze">Freeze</h1>
<p>​
冻结某些参数是最容易想到的方法，仅仅调整某些未被冻结的参数就可以减少大量显存占用，但freeze方法大大降低了模型的灵活性。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">for name ,param in model.parameters():
	if not any (n in name for n in [layers_name1,layer_name2...])
		param.requires_grad &#x3D; False</code></pre>
<h1 id="bitfit">Bitfit</h1>
<p>出自<strong>BitFit: Simple Parameter-efficient Fine-tuning or
Transformer-based Masked Language-models</strong>。</p>
<p>pass</p>
<h1 id="prompt-tuning">Prompt-tuning</h1>
<p>pass</p>
<h1 id="lora">Lora</h1>
<h2 id="原理">原理</h2>
<figure>
<img src="../images/image-20240410215838916.png"
alt="image-20240410215838916" />
<figcaption aria-hidden="true">image-20240410215838916</figcaption>
</figure>
<p>​ 在原始权重中并上一个旁路的分支，以Linear层为例子，原本<span
class="math inline">\(h = W_{d×k}X\)</span>，<span
class="math inline">\(W\)</span>是权重，旁路有两个低秩的矩阵，其中<span
class="math inline">\(A_{r×k} =
N(0,σ^2)\)</span>，也就是以高斯分布初始化，而<span
class="math inline">\(B_{d×r}\)</span>矩阵则以全0初始化，其中维度r远小于d和k。<span
class="math inline">\(ΔW = BA\)</span> ,最终的权重为$W+ΔW <span
class="math inline">\(。对于\)</span>ΔW<span
class="math inline">\(可以使用一个α参数来控制倍数，即\)</span>W+ΔW$。B为升维矩阵，A为降维矩阵。实际上，LoRA一般用于Dense层。</p>
<p>​ 对于梯度计算，借用知乎上CodeLearner答主的一张图。</p>
<p>​ <img src="../images/image-20240411005445372.png"
alt="image-20240411005445372" /></p>
<p>​
所以，在微调时，某一Lora层反向传播中的梯度计算量是要更多的，但由于r远小于原权重的维度d和k，所以相对于全量微调保存的梯度值就少。同时，原论文中也只对Transformer
Block中的Q,K,V进行了Lora处理。若r等于k，那么此时可以等价于全参数微调。</p>
<p>​ 对于初始化问题，首先，我们需要保证最开始的<span
class="math inline">\(ΔW=0\)</span>，所以需要AB的其中一者为0。其次，我们看上图带上LoRA后的反向传播梯度计算，若A为0，那么梯度就会一直为0，<span
class="math inline">\(ΔW\)</span>就不会更新。</p>
<p>​
原文中提到了LoRA的Limitation，如果选择将A和B矩阵吸收(合并)到W原始权重矩阵中,以消除额外的推理延迟,那么在单个前向传递中为不同任务的不同A和B批量输入数据就变得不直观了。我想原文的意思是一个batch里可能有不同的任务，那么不同的任务应该用不同的权重，那么最好是不将<span
class="math inline">\(ΔW\)</span>合并到原始权重，针对不同任务来动态选择<span
class="math inline">\(ΔW\)</span>，这需要在推理速度上做取舍。</p>
<h2 id="源码速读">源码速读</h2>
<p>​ 为了方便理解，我们从peft0.10.0的官方示例出发。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from transformers import AutoModelForSeq2SeqLM
from peft import get_peft_config, get_peft_model, LoraConfig, TaskType
model_name_or_path &#x3D; &quot;bigscience&#x2F;mt0-large&quot;
tokenizer_name_or_path &#x3D; &quot;bigscience&#x2F;mt0-large&quot;

peft_config &#x3D; LoraConfig(
    task_type&#x3D;TaskType.SEQ_2_SEQ_LM, inference_mode&#x3D;False, r&#x3D;8, lora_alpha&#x3D;32, lora_dropout&#x3D;0.1
)

model &#x3D; AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
#等价于 model &#x3D; PeftModelForSeq2SeqLM(model,peft_config)
#也等价于model &#x3D; lora_model &#x3D; LoraModel(model, config, &quot;default&quot;)
model &#x3D; get_peft_model(model, peft_config) 

model.print_trainable_parameters()
&quot;trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282&quot;</code></pre>
<p>​
首先从LoraConfig出发，这里的task_type是PeftConfig的参数，其余则是LoraConfig的参数，这些参数在上文的原理中都有提到。主要还是根据不同的下游任务返回不同的模型。这里也可以直接用get_peft_config函数来读取。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">peft_config &#x3D; LoraConfig(
    task_type&#x3D;TaskType.SEQ_2_SEQ_LM, inference_mode&#x3D;False, r&#x3D;8, lora_alpha&#x3D;32, lora_dropout&#x3D;0.1
)
----
config &#x3D; &#123;
    &quot;task_type&quot;:&quot;SEQ_2_SEQ_LM&quot;,
    &quot;peft_type&quot;:&quot;LORA&quot;,
    &quot;inference_mode&quot;:False,
    &quot;r&quot;:8,
    &quot;lora_alpha&quot;:32,
    &quot;lora_dropout&quot;:0.1
&#125;

another_config &#x3D; get_peft_config(config)</code></pre>
<h3 id="get_peft_model">get_peft_model</h3>
<p>​
接下来看get_peft_model。传入的参数有模型和对应的config，最后会返回一个PeftModel实例。返回的实例类型会根据传入的config来确定。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model &#x3D; get_peft_model(model, peft_config)
---------mapping.py----------------
def get_peft_model(
    model: PreTrainedModel, peft_config: PeftConfig, adapter_name: str &#x3D; &quot;default&quot;, mixed: bool &#x3D; False
) -&gt; PeftModel | PeftMixedModel:
    &quot;&quot;&quot;
    Returns a Peft model object from a model and a config.

    Args:
        model ([&#96;transformers.PreTrainedModel&#96;]):
            Model to be wrapped.
        peft_config ([&#96;PeftConfig&#96;]):
            Configuration object containing the parameters of the Peft model.
        adapter_name (&#96;str&#96;, &#96;optional&#96;, defaults to &#96;&quot;default&quot;&#96;):
            The name of the adapter to be injected, if not provided, the default adapter name is used (&quot;default&quot;).
        mixed (&#96;bool&#96;, &#96;optional&#96;, defaults to &#96;False&#96;):
            Whether to allow mixing different (compatible) adapter types.
    &quot;&quot;&quot;
    model_config &#x3D; getattr(model, &quot;config&quot;, &#123;&quot;model_type&quot;: &quot;custom&quot;&#125;)
    if hasattr(model_config, &quot;to_dict&quot;):
        model_config &#x3D; model_config.to_dict()

    peft_config.base_model_name_or_path &#x3D; model.__dict__.get(&quot;name_or_path&quot;, None)

    if mixed:
        return PeftMixedModel(model, peft_config, adapter_name&#x3D;adapter_name)

    if peft_config.task_type not in MODEL_TYPE_TO_PEFT_MODEL_MAPPING.keys() and not peft_config.is_prompt_learning:
        return PeftModel(model, peft_config, adapter_name&#x3D;adapter_name)

    if peft_config.is_prompt_learning:
        peft_config &#x3D; _prepare_prompt_learning_config(peft_config, model_config)
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name&#x3D;adapter_name)
----------------------
MODEL_TYPE_TO_PEFT_MODEL_MAPPING: dict[str, PeftModel] &#x3D; &#123;
    &quot;SEQ_CLS&quot;: PeftModelForSequenceClassification,
    &quot;SEQ_2_SEQ_LM&quot;: PeftModelForSeq2SeqLM,
    &quot;CAUSAL_LM&quot;: PeftModelForCausalLM,
    &quot;TOKEN_CLS&quot;: PeftModelForTokenClassification,
    &quot;QUESTION_ANS&quot;: PeftModelForQuestionAnswering,
    &quot;FEATURE_EXTRACTION&quot;: PeftModelForFeatureExtraction,
&#125;
</code></pre>
<p>​
关键在于这两句。如果任务类型不在支持的特定任务中，返回PeftModel，否则返回对应任务类型的Model，这些Model继承了PeftModel。若是提示学习类型的，如prompt-tuning，则需要额外的config信息，如隐藏层的数量,可能的键包括<code>num_hidden_layers</code>、<code>num_layers</code>、<code>n_layer</code>，
如果无法找到,则需要在peft_config中手动指定num_layers，这个参数指定了prompt将被注入到模型的哪些层。还需要注意力的头数，encoder隐藏层的大小和tokenembedding的维度等等。这些参数若从模型的config中找不到，则需要在peftconfig中自行指定。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">    if peft_config.task_type not in MODEL_TYPE_TO_PEFT_MODEL_MAPPING.keys() and not peft_config.is_prompt_learning:
        return PeftModel(model, peft_config, adapter_name&#x3D;adapter_name)

    if peft_config.is_prompt_learning:
        peft_config &#x3D; _prepare_prompt_learning_config(peft_config, model_config)
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name&#x3D;adapter_name)
-------------------
def _prepare_prompt_learning_config(peft_config, model_config):
    if peft_config.num_layers is None:
        if &quot;num_hidden_layers&quot; in model_config:
            num_layers &#x3D; model_config[&quot;num_hidden_layers&quot;]
        elif &quot;num_layers&quot; in model_config:
            num_layers &#x3D; model_config[&quot;num_layers&quot;]
        elif &quot;n_layer&quot; in model_config:
            num_layers &#x3D; model_config[&quot;n_layer&quot;]
        else:
            raise ValueError(&quot;Please specify &#96;num_layers&#96; in &#96;peft_config&#96;&quot;)
        peft_config.num_layers &#x3D; num_layers

    if peft_config.token_dim is None:
        if &quot;hidden_size&quot; in model_config:
            token_dim &#x3D; model_config[&quot;hidden_size&quot;]
        elif &quot;n_embd&quot; in model_config:
            token_dim &#x3D; model_config[&quot;n_embd&quot;]
        elif &quot;d_model&quot; in model_config:
            token_dim &#x3D; model_config[&quot;d_model&quot;]
        else:
            raise ValueError(&quot;Please specify &#96;token_dim&#96; in &#96;peft_config&#96;&quot;)
        peft_config.token_dim &#x3D; token_dim

    if peft_config.num_attention_heads is None:
        if &quot;num_attention_heads&quot; in model_config:
            num_attention_heads &#x3D; model_config[&quot;num_attention_heads&quot;]
        elif &quot;n_head&quot; in model_config:
            num_attention_heads &#x3D; model_config[&quot;n_head&quot;]
        elif &quot;num_heads&quot; in model_config:
            num_attention_heads &#x3D; model_config[&quot;num_heads&quot;]
        elif &quot;encoder_attention_heads&quot; in model_config:
            num_attention_heads &#x3D; model_config[&quot;encoder_attention_heads&quot;]
        else:
            raise ValueError(&quot;Please specify &#96;num_attention_heads&#96; in &#96;peft_config&#96;&quot;)
        peft_config.num_attention_heads &#x3D; num_attention_heads

    if getattr(peft_config, &quot;encoder_hidden_size&quot;, None) is None:
        setattr(peft_config, &quot;encoder_hidden_size&quot;, peft_config.token_dim)

    return peft_config</code></pre>
<p>​
回到重点，我们以Seq2SeqModel的代码为例，相对于PeftModel，整体结构一样，只是多了两个参数变量，其中prepare_inputs_for_generation函数需要生成模型在generate方法中自行实现，base_model_prepare_encoder_decoder_kwargs_for_generation变量则从base_model中提取出一些生成时的参数，如是否要使用cache的use_cahe，将encoder的参数封装进model_kwargs返回。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class PeftModelForSeq2SeqLM(PeftModel):
	def __init__(self, model: torch.nn.Module, peft_config: PeftConfig, adapter_name: str &#x3D; &quot;default&quot;) -&gt; None:
        super().__init__(model, peft_config, adapter_name)
        self.base_model_prepare_inputs_for_generation &#x3D; self.base_model.prepare_inputs_for_generation
        self.base_model_prepare_encoder_decoder_kwargs_for_generation &#x3D; (
            self.base_model._prepare_encoder_decoder_kwargs_for_generation
        )
</code></pre>
<h3 id="peftmodel">PeftModel</h3>
<p>​ 还是先贴出整体的初始化源码。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class PeftModel(PushToHubMixin, torch.nn.Module):
    
    def __init__(self, model: PreTrainedModel, peft_config: PeftConfig, adapter_name: str &#x3D; &quot;default&quot;) -&gt; None:
        super().__init__()
        self.modules_to_save &#x3D; None
        self.active_adapter &#x3D; adapter_name
        self.peft_type &#x3D; peft_config.peft_type
        # These args are special PEFT arguments that users can pass. They need to be removed before passing them to
        # forward.
        self.special_peft_forward_args &#x3D; &#123;&quot;adapter_names&quot;&#125;

        self._is_prompt_learning &#x3D; peft_config.is_prompt_learning
        if self._is_prompt_learning:
            self._peft_config &#x3D; &#123;adapter_name: peft_config&#125;
            self.base_model &#x3D; model
            self.add_adapter(adapter_name, peft_config)
        else:
            self._peft_config &#x3D; None
            cls &#x3D; PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type]
            self.base_model &#x3D; cls(model, &#123;adapter_name: peft_config&#125;, adapter_name)
            self.set_additional_trainable_modules(peft_config, adapter_name)

        if getattr(model, &quot;is_gradient_checkpointing&quot;, True):
            model &#x3D; self._prepare_model_for_gradient_checkpointing(model)

        # the &#96;pretraining_tp&#96; is set for some models to simulate Tensor Parallelism during inference to avoid
        # numerical differences, https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;issues&#x2F;76232 - to avoid any unexpected
        # behavior we disable that in this line.
        if hasattr(self.base_model, &quot;config&quot;) and hasattr(self.base_model.config, &quot;pretraining_tp&quot;):
            self.base_model.config.pretraining_tp &#x3D; 1</code></pre>
<p>​
PeftModel中的源码很大部分服务于提示学习，先不看。我们只看_is_prompt_learning是False的情况。</p>
<pre class="line-numbers language-none"><code class="language-none">else:
    self._peft_config &#x3D; None
    cls &#x3D; PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type]
    self.base_model &#x3D; cls(model, &#123;adapter_name: peft_config&#125;, adapter_name)
    self.set_additional_trainable_modules(peft_config, adapter_name)</code></pre>
<p>​
可以看到，cls最终映射到对应peft方案的模型实例，若是LORA，则返回一个LoraModel实例，看来关键就在于LoraModel中。</p>
<h3 id="loramodel">LoraModel</h3>
<p>​ LoraModel继承了BaseTuner，而BaseTuner继承nn.Module。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LoraModel(BaseTuner):
    prefix: str &#x3D; &quot;lora_&quot;

    def __init__(self, model, config, adapter_name) -&gt; None:
        super().__init__(model, config, adapter_name)</code></pre>
<p>​
由于代码量非常大，所以只贴出核心的代码，这一段是LoRA的核心逻辑。首先判断target是不是Lora层，若是，则根据loraconfig来更新这一层。若不是，则创建一个新的Lora层来替换原来的层。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def _create_and_replace(
    self,
    lora_config,
    adapter_name,
    target,
    target_name,
    parent,
    current_key,
):
    &quot;&quot;&quot;
    此处省略部分代码
    &quot;&quot;&quot;
    from peft.tuners.adalora import AdaLoraLayer

    if isinstance(target, LoraLayer) and not isinstance(target, AdaLoraLayer):
        target.update_layer(
            adapter_name,
            r,
            lora_alpha&#x3D;alpha,
            lora_dropout&#x3D;lora_config.lora_dropout,
            init_lora_weights&#x3D;lora_config.init_lora_weights,
            use_rslora&#x3D;lora_config.use_rslora,
            use_dora&#x3D;lora_config.use_dora,
        )
    else:
        new_module &#x3D; self._create_new_module(lora_config, adapter_name, target, **kwargs)
        if adapter_name !&#x3D; self.active_adapter:
            # adding an additional adapter: it is not automatically trainable
            new_module.requires_grad_(False)
        self._replace_module(parent, target_name, new_module, target)</code></pre>
<p>​
下面是<strong>update_layer</strong>函数的逻辑，首先保证秩大于0，然后创建dropout层。</p>
<p>​
再创建AB矩阵，可以看到A是降维矩阵，B是升维矩阵。之后初始化这两个矩阵，可以看到A矩阵可以选择kaiming
uniform或高斯分布初始化。若是用于Embedding层，那么A初始化为0，B用高斯分布。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  def update_layer(
      self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights, use_rslora, use_dora: bool &#x3D; False
  ):
      # This code works for linear layers, override for other layer types
      if r &lt;&#x3D; 0:
          raise ValueError(f&quot;&#96;r&#96; should be a positive integer value but the value passed is &#123;r&#125;&quot;)

      self.r[adapter_name] &#x3D; r
      self.lora_alpha[adapter_name] &#x3D; lora_alpha
      if lora_dropout &gt; 0.0:
          lora_dropout_layer &#x3D; nn.Dropout(p&#x3D;lora_dropout)
      else:
          lora_dropout_layer &#x3D; nn.Identity()

      self.lora_dropout.update(nn.ModuleDict(&#123;adapter_name: lora_dropout_layer&#125;))
      # Actual trainable parameters
      self.lora_A[adapter_name] &#x3D; nn.Linear(self.in_features, r, bias&#x3D;False)
      self.lora_B[adapter_name] &#x3D; nn.Linear(r, self.out_features, bias&#x3D;False)
      if use_rslora:
          self.scaling[adapter_name] &#x3D; lora_alpha &#x2F; math.sqrt(r)
      else:
          self.scaling[adapter_name] &#x3D; lora_alpha &#x2F; r

      if init_lora_weights &#x3D;&#x3D; &quot;loftq&quot;:
          self.loftq_init(adapter_name)
      elif init_lora_weights:
          self.reset_lora_parameters(adapter_name, init_lora_weights)
&quot;&quot;&quot;
下略
&quot;&quot;&quot;

      self.set_adapter(self.active_adapters)
    -----------------------------------------------------
  def reset_lora_parameters(self, adapter_name, init_lora_weights):
      if init_lora_weights is False:
          return

      if adapter_name in self.lora_A.keys():
          if init_lora_weights is True:
              # initialize A the same way as the default for nn.Linear and B to zero
              # https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;LoRA&#x2F;blob&#x2F;a0a92e0f26c067cf94747bdbf1ce73793fa44d19&#x2F;loralib&#x2F;layers.py#L124
              nn.init.kaiming_uniform_(self.lora_A[adapter_name].weight, a&#x3D;math.sqrt(5))
          elif init_lora_weights.lower() &#x3D;&#x3D; &quot;gaussian&quot;:
              nn.init.normal_(self.lora_A[adapter_name].weight, std&#x3D;1 &#x2F; self.r[adapter_name])
          else:
              raise ValueError(f&quot;Unknown initialization &#123;init_lora_weights&#x3D;&#125;&quot;)
          nn.init.zeros_(self.lora_B[adapter_name].weight)
      if adapter_name in self.lora_embedding_A.keys():
          # initialize a the same way as the default for nn.linear and b to zero
          nn.init.zeros_(self.lora_embedding_A[adapter_name])
          nn.init.normal_(self.lora_embedding_B[adapter_name])</code></pre>
<h3 id="loralayer">LoraLayer</h3>
<p>​
可以看到layer需要是LoraLayer的实例才会被更新，下面是LoraLayer的代码。最初的peft0.1.0，Lora只能用于Linear层，后来则将一些dense层统一抽象成为了LoraLayer，可以看到有Linear，Embedding，Conv1D和Conv2D。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LoraLayer(BaseTunerLayer):
    # All names of layers that may contain (trainable) adapter weights
    adapter_layer_names &#x3D; (&quot;lora_A&quot;, &quot;lora_B&quot;, &quot;lora_embedding_A&quot;, &quot;lora_embedding_B&quot;)
    # All names of other parameters that may contain adapter-related parameters
    other_param_names &#x3D; (&quot;r&quot;, &quot;lora_alpha&quot;, &quot;scaling&quot;, &quot;lora_dropout&quot;)

    def __init__(self, base_layer: nn.Module, **kwargs) -&gt; None:
        self.base_layer &#x3D; base_layer
        self.r &#x3D; &#123;&#125;
        self.lora_alpha &#x3D; &#123;&#125;
        self.scaling &#x3D; &#123;&#125;
        self.lora_dropout &#x3D; nn.ModuleDict(&#123;&#125;)
        self.lora_A &#x3D; nn.ModuleDict(&#123;&#125;)
        self.lora_B &#x3D; nn.ModuleDict(&#123;&#125;)
        # For Embedding layer
        self.lora_embedding_A &#x3D; nn.ParameterDict(&#123;&#125;)
        self.lora_embedding_B &#x3D; nn.ParameterDict(&#123;&#125;)
        # Mark the weight as unmerged
        self._disable_adapters &#x3D; False
        self.merged_adapters &#x3D; []
        self.use_dora: dict[str, bool] &#x3D; &#123;&#125;
        self.lora_magnitude_vector: Optional[torch.nn.ParameterDict] &#x3D; None  # for DoRA
        self._caches: dict[str, Any] &#x3D; &#123;&#125;
        self.kwargs &#x3D; kwargs

        base_layer &#x3D; self.get_base_layer()
        if isinstance(base_layer, nn.Linear):
            in_features, out_features &#x3D; base_layer.in_features, base_layer.out_features
        elif isinstance(base_layer, nn.Conv2d):
            in_features, out_features &#x3D; base_layer.in_channels, base_layer.out_channels
        elif isinstance(base_layer, nn.Embedding):
            in_features, out_features &#x3D; base_layer.num_embeddings, base_layer.embedding_dim
        elif isinstance(base_layer, Conv1D):
            in_features, out_features &#x3D; (
                base_layer.weight.ds_shape if hasattr(base_layer.weight, &quot;ds_shape&quot;) else base_layer.weight.shape
            )
        elif hasattr(base_layer, &quot;infeatures&quot;) and hasattr(base_layer, &quot;outfeatures&quot;):
            # QuantLinear
            in_features, out_features &#x3D; base_layer.infeatures, base_layer.outfeatures
        elif hasattr(base_layer, &quot;input_size&quot;) and hasattr(base_layer, &quot;output_size&quot;):
            # Megatron ColumnParallelLinear,RowParallelLinear
            in_features, out_features &#x3D; base_layer.input_size, base_layer.output_size
        elif hasattr(base_layer, &quot;codebooks&quot;) and base_layer.__class__.__name__ &#x3D;&#x3D; &quot;QuantizedLinear&quot;:
            # AQLM QuantLinear
            in_features, out_features &#x3D; base_layer.in_features, base_layer.out_features
        elif hasattr(base_layer, &quot;w_bit&quot;) and base_layer.__class__.__name__ &#x3D;&#x3D; &quot;WQLinear_GEMM&quot;:
            # Awq layers
            in_features, out_features &#x3D; base_layer.in_features, base_layer.out_features
        else:
            raise ValueError(f&quot;Unsupported layer type &#123;type(base_layer)&#125;&quot;)

        self.in_features &#x3D; in_features
        self.out_features &#x3D; out_features</code></pre>
<p>​
在LoraModel中的**_create_new_module**函数中我们也可以看到抛出的异常，应证了当target不是LoraLayer的时候，如果想创建一个新Lora模块将旧模块替换，那么需要原模块是torch.nn.Linear,
torch.nn.Embedding, torch.nn.Conv2d,
transformers.pytorch_utils.Conv1D这几种类型，最终这几个类在LoraLayer.py中都被重写了。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if new_module is None:
    # no module could be matched
    raise ValueError(
        f&quot;Target module &#123;target&#125; is not supported. Currently, only the following modules are supported: &quot;
        &quot;&#96;torch.nn.Linear&#96;, &#96;torch.nn.Embedding&#96;, &#96;torch.nn.Conv2d&#96;, &#96;transformers.pytorch_utils.Conv1D&#96;.&quot;
    )</code></pre>
<p>​
同时，对于不同的模型，支持的Lora策略也不同。Transformer库中列举了哪些模型的哪些层能够使用官方Lora方案。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING &#x3D; &#123;
    &quot;t5&quot;: [&quot;q&quot;, &quot;v&quot;],
    &quot;mt5&quot;: [&quot;q&quot;, &quot;v&quot;],
    &quot;bart&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;gpt2&quot;: [&quot;c_attn&quot;],
    &quot;bloom&quot;: [&quot;query_key_value&quot;],
    &quot;blip-2&quot;: [&quot;q&quot;, &quot;v&quot;, &quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;opt&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;gptj&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;gpt_neox&quot;: [&quot;query_key_value&quot;],
    &quot;gpt_neo&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;bert&quot;: [&quot;query&quot;, &quot;value&quot;],
    &quot;roberta&quot;: [&quot;query&quot;, &quot;value&quot;],
    &quot;xlm-roberta&quot;: [&quot;query&quot;, &quot;value&quot;],
    &quot;electra&quot;: [&quot;query&quot;, &quot;value&quot;],
    &quot;deberta-v2&quot;: [&quot;query_proj&quot;, &quot;value_proj&quot;],
    &quot;deberta&quot;: [&quot;in_proj&quot;],
    &quot;layoutlm&quot;: [&quot;query&quot;, &quot;value&quot;],
    &quot;llama&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;chatglm&quot;: [&quot;query_key_value&quot;],
    &quot;gpt_bigcode&quot;: [&quot;c_attn&quot;],
    &quot;mpt&quot;: [&quot;Wqkv&quot;],
    &quot;RefinedWebModel&quot;: [&quot;query_key_value&quot;],
    &quot;RefinedWeb&quot;: [&quot;query_key_value&quot;],
    &quot;falcon&quot;: [&quot;query_key_value&quot;],
    &quot;btlm&quot;: [&quot;c_proj&quot;, &quot;c_attn&quot;],
    &quot;codegen&quot;: [&quot;qkv_proj&quot;],
    &quot;mistral&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;mixtral&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;stablelm&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;phi&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;, &quot;fc1&quot;, &quot;fc2&quot;],
    &quot;gemma&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
&#125;</code></pre>
<h3 id="linear">Linear</h3>
<p>​
我们只需要知道Linear最终被重写了，继承了LoraLayer，然后通过LoraLayer的<strong>update_layer</strong>方法根据config的参数进行了初始化。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class Linear(nn.Module, LoraLayer):
    # Lora implemented in a dense layer
    def __init__(
        self,
        base_layer,
        adapter_name: str,
        r: int &#x3D; 0,
        lora_alpha: int &#x3D; 1,
        lora_dropout: float &#x3D; 0.0,
        fan_in_fan_out: bool &#x3D; False,  # Set this to True if the layer to replace stores weight like (fan_in, fan_out)
        is_target_conv_1d_layer: bool &#x3D; False,
        init_lora_weights: Union[bool, str] &#x3D; True,
        use_rslora: bool &#x3D; False,
        use_dora: bool &#x3D; False,
        **kwargs,
    ) -&gt; None:
        super().__init__()
        LoraLayer.__init__(self, base_layer, **kwargs)
        self.fan_in_fan_out &#x3D; fan_in_fan_out

        self._active_adapter &#x3D; adapter_name
        self.update_layer(
            adapter_name,
            r,
            lora_alpha&#x3D;lora_alpha,
            lora_dropout&#x3D;lora_dropout,
            init_lora_weights&#x3D;init_lora_weights,
            use_rslora&#x3D;use_rslora,
            use_dora&#x3D;use_dora,
        )
        self.is_target_conv_1d_layer &#x3D; is_target_conv_1d_layer</code></pre>
<p>​
我们直奔forward方法。对于不合并的adapter，若已经合并，把已经加入到baselayer的权重移出来。可以合并的adapter若已经合并了就直接用这个权重。最终Lora中的Linear层的前向传播结果是：
<span class="math display">\[
X = XW + scale × \frac{α}{r}(dropout(X))AB
\]</span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  def forward(self, x: torch.Tensor, *args: Any, **kwargs: Any) -&gt; torch.Tensor:
      self._check_forward_args(x, *args, **kwargs)
      adapter_names &#x3D; kwargs.pop(&quot;adapter_names&quot;, None)
# 若之前把该adapter设置为不合并但合并了，马上把加进去的权重拿出来
      if self.disable_adapters:
          if self.merged:
              self.unmerge()
          result &#x3D; self.base_layer(x, *args, **kwargs)
      elif adapter_names is not None:
          result &#x3D; self._mixed_batch_forward(x, *args, adapter_names&#x3D;adapter_names, **kwargs)
      # 已经合并了就直接返回
      elif self.merged:
          result &#x3D; self.base_layer(x, *args, **kwargs)
      else:
          result &#x3D; self.base_layer(x, *args, **kwargs)
          torch_result_dtype &#x3D; result.dtype
          for active_adapter in self.active_adapters:
              if active_adapter not in self.lora_A.keys():
                  continue
              lora_A &#x3D; self.lora_A[active_adapter]
              lora_B &#x3D; self.lora_B[active_adapter]
              dropout &#x3D; self.lora_dropout[active_adapter]
              # self.scaling[adapter] &#x3D; scale * self.lora_alpha[adapter] &#x2F; self.r[adapter]
              scaling &#x3D; self.scaling[active_adapter]
              x &#x3D; x.to(lora_A.weight.dtype)

              if not self.use_dora[active_adapter]:
                  result &#x3D; result + lora_B(lora_A(dropout(x))) * scaling
              else:
                  x &#x3D; dropout(x)
                  result &#x3D; result + self._apply_dora(x, lora_A, lora_B, scaling, active_adapter)

          result &#x3D; result.to(torch_result_dtype)

      return result</code></pre>
<p>​
当然还有少不了的merge和unmerge函数，最终微调完毕的权重是可以选择加回原权重来消除额外的推理延迟。</p>
<p>​ 先看unmerge方法，其实就是把加到baselayer上的权重减回去。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def unmerge(self) -&gt; None:
    &quot;&quot;&quot;
    This method unmerges all merged adapter layers from the base weights.
    &quot;&quot;&quot;
    if not self.merged:
        warnings.warn(&quot;Already unmerged. Nothing to do.&quot;)
        return
    while len(self.merged_adapters) &gt; 0:
        active_adapter &#x3D; self.merged_adapters.pop()
        if active_adapter in self.lora_A.keys():
            weight &#x3D; self.get_base_layer().weight
            delta_weight &#x3D; self.get_delta_weight(active_adapter)
            if not self.use_dora[active_adapter]:
                # 只看这一句就行了
                weight.data -&#x3D; delta_weight
            else:
                weight_norm &#x3D; self._cache_pop(f&quot;&#123;active_adapter&#125;-weight_norm&quot;)
                dora_factor &#x3D; self.lora_magnitude_vector[active_adapter] &#x2F; weight_norm
                weight_orig &#x3D; weight.data &#x2F; dora_factor.view(-1, 1) - delta_weight
                weight.data &#x3D; weight_orig</code></pre>
<p>​
再看merge操作，首先需要拿到<strong>delta_weight</strong>，具体的内容就不复制了，如果是fp16的权重在cpu上计算，那么由于cpu原生不支持该类型，所以需要转换成fp32再转换回去。最终的<span
class="math inline">\(W =
W+scaling×BA\)</span>，可以看到从头到尾是没有原来的W参与前向传播的，原来的W直接被冻结。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def merge(self, safe_merge: bool &#x3D; False, adapter_names: Optional[list[str]] &#x3D; None) -&gt; None:
    adapter_names &#x3D; check_adapters_to_merge(self, adapter_names)
    if not adapter_names:
        # no adapter to merge
        return

    for active_adapter in adapter_names:
        if active_adapter in self.lora_A.keys():
            base_layer &#x3D; self.get_base_layer()
            if safe_merge:
                # Note that safe_merge will be slower than the normal merge
                # because of the copy operation.
	&quot;&quot;&quot;
	safe_merge其实就是查看是否有NaN值，有的话抛出异常。
	并且由于多复制了一次原权重，所以效率会更低，代码略
	&quot;&quot;&quot;
            else:
                # 该操作核心就一句，返回scaling B@A
                # output_tensor &#x3D; transpose(weight_B @ weight_A, self.fan_in_fan_out) * self.scaling[adapter]
                delta_weight &#x3D; self.get_delta_weight(active_adapter)
                if not self.use_dora[active_adapter]:
                    base_layer.weight.data &#x3D; base_layer.weight.data + delta_weight
                else:
                    # handle dora，此处略
            self.merged_adapters.append(active_adapter)</code></pre>
<p>​
剩下的Embedding，一维卷积和二维卷积操作差不多，这里就不多赘述了。</p>
<h3 id="合并操作">合并操作</h3>
<p>​
首先需要判断是否能够合并。有两种情况下不能合并，一种是当前使用gptq量化模型(QLoRA可以)，另一种则是开启了peftconfig中的layer_replication操作。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def _check_merge_allowed(self):
    &quot;&quot;&quot;Verify that the configuration supports merging.

    Currently gptq quantization and replicated layers do not support merging.
    &quot;&quot;&quot;
    if getattr(self.model, &quot;quantization_method&quot;, None) &#x3D;&#x3D; &quot;gptq&quot;:
        raise ValueError(&quot;Cannot merge LORA layers when the model is gptq quantized&quot;)
    if self.peft_config.get(&quot;layer_replication&quot;):
        raise ValueError(&quot;Cannot merge LORA layers when base model layers are replicated&quot;)</code></pre>
<p>​
最终调用的方法是<strong>merge_and_unload</strong>方法，<code>progressbar</code>就是是否用tqdm显示进行，<code>safe_merge</code>会检查tensor中是否有NaN，<code>adapter_names</code>用于指定哪些层要合并，默认是所有都合并。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  def merge_and_unload(
      self, progressbar: bool &#x3D; False, safe_merge: bool &#x3D; False, adapter_names: Optional[list[str]] &#x3D; None
  ) -&gt; torch.nn.Module:
# progressbar就是是否用tqdm显示进行
      return self._unload_and_optionally_merge(
          progressbar&#x3D;progressbar, safe_merge&#x3D;safe_merge, adapter_names&#x3D;adapter_names
      )</code></pre>
<p>​
下面是核心方法**_unload_and_optionally_merge**。首先会判断是否满足要求，也就是不使用gptq且不采取replicated策略的情况下才能合并。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def _unload_and_optionally_merge(
    self,
    merge&#x3D;True,
    progressbar: bool &#x3D; False,
    safe_merge: bool &#x3D; False,
    adapter_names: Optional[list[str]] &#x3D; None,
):
    if merge:
        self._check_merge_allowed()</code></pre>
<p>​
然后拿到targetlayer进行合并操作后，替换掉原来的module。这里的target.merge是LoraLayer的子类都实现的merge操作，上面已经给出。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if hasattr(target, &quot;base_layer&quot;):
    if merge:
        target.merge(safe_merge&#x3D;safe_merge, adapter_names&#x3D;adapter_names)
    self._replace_module(parent, target_name, target.get_base_layer(), target)
</code></pre>
<h1 id="参考资料">参考资料</h1>
<p>[1] <a target="_blank" rel="noopener" href='https://arxiv.org/abs/2303.15647'>Scaling Down to Scale
Up: A Guide to Parameter-Efficient Fine-Tuning</a></p>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>iroha</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://example.com/post/ft_survey.html" title="大模型微调方法综述">http://example.com/post/ft_survey.html</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/post/model_load.html" rel="prev" title="Huggingface的模型加载流程"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">Huggingface的模型加载流程</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/post/backtrace.html" rel="next" title="回溯之组合问题"><span class="post-nav-text">回溯之组合问题</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2024 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> iroha</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.9</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>