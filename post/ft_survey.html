<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="iroha"><meta name="copyright" content="iroha"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ç»¼è¿° | iroha</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...undefined?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"example.com","root":"/","title":"ä¸æƒ³æ‘†çƒ‚","version":"1.10.9","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"æœç´¢...","empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹: ${query}","hits":"æ‰¾åˆ° ${hits} æ¡ç»“æœ","hits_time":"æ‰¾åˆ° ${hits} æ¡ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><meta name="description" content="èƒŒæ™¯ â€‹ éšç€æ¨¡å‹çš„å‚æ•°è¶Šæ¥è¶Šå¤§ï¼Œä¸ºæ¯ä¸€ä¸ªä»»åŠ¡ä»å¤´è®­ç»ƒä¸€ä¸ªæ¨¡å‹è¶Šæ¥è¶Šå›°éš¾ï¼Œè€Œå‚æ•°é‡çš„å¢å¤§ä¹Ÿè®©æ¨¡å‹ç†è§£äº†æ›´æ·±å±‚çš„è¯­ä¹‰ä¿¡æ¯ã€‚éšç€BERTçš„å‡ºç°ï¼Œé¢„è®­ç»ƒ+å¾®è°ƒçš„æ–¹å¼èµ°ä¸Šå†å²èˆå°ã€‚ â€‹ ä¸ºä»€ä¹ˆä¸é€‰æ‹©å…¨å‚æ•°å¾®è°ƒå‘¢ï¼Ÿæœ€ä¸»è¦çš„åŸå› è¿˜æ˜¯æˆæœ¬å¤ªé«˜ï¼Œåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šæ„ˆå‘å›°éš¾ã€‚ä»¥GPT2-1.5Bä¸ºä¾‹ï¼Œè‹¥å‚æ•°ä¸æ¢¯åº¦ä»¥fp16ä¿å­˜ï¼Œæ˜¾å­˜éœ€è¦ä¿å­˜3+3 &#x3D; 6Bçš„æ•°æ®ï¼Œè‹¥ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œé‚£ä¹ˆè¿˜éœ€è¦ä¿å­˜fp32">
<meta property="og:type" content="article">
<meta property="og:title" content="å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ç»¼è¿°">
<meta property="og:url" content="http://example.com/post/ft_survey.html">
<meta property="og:site_name" content="iroha">
<meta property="og:description" content="èƒŒæ™¯ â€‹ éšç€æ¨¡å‹çš„å‚æ•°è¶Šæ¥è¶Šå¤§ï¼Œä¸ºæ¯ä¸€ä¸ªä»»åŠ¡ä»å¤´è®­ç»ƒä¸€ä¸ªæ¨¡å‹è¶Šæ¥è¶Šå›°éš¾ï¼Œè€Œå‚æ•°é‡çš„å¢å¤§ä¹Ÿè®©æ¨¡å‹ç†è§£äº†æ›´æ·±å±‚çš„è¯­ä¹‰ä¿¡æ¯ã€‚éšç€BERTçš„å‡ºç°ï¼Œé¢„è®­ç»ƒ+å¾®è°ƒçš„æ–¹å¼èµ°ä¸Šå†å²èˆå°ã€‚ â€‹ ä¸ºä»€ä¹ˆä¸é€‰æ‹©å…¨å‚æ•°å¾®è°ƒå‘¢ï¼Ÿæœ€ä¸»è¦çš„åŸå› è¿˜æ˜¯æˆæœ¬å¤ªé«˜ï¼Œåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šæ„ˆå‘å›°éš¾ã€‚ä»¥GPT2-1.5Bä¸ºä¾‹ï¼Œè‹¥å‚æ•°ä¸æ¢¯åº¦ä»¥fp16ä¿å­˜ï¼Œæ˜¾å­˜éœ€è¦ä¿å­˜3+3 &#x3D; 6Bçš„æ•°æ®ï¼Œè‹¥ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œé‚£ä¹ˆè¿˜éœ€è¦ä¿å­˜fp32">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20240408231535224.png">
<meta property="og:image" content="http://example.com/images/image-20240410215838916.png">
<meta property="og:image" content="http://example.com/images/image-20240411005445372.png">
<meta property="article:published_time" content="2024-04-08T04:00:00.000Z">
<meta property="article:modified_time" content="2024-04-12T19:32:20.406Z">
<meta property="article:author" content="iroha">
<meta property="article:tag" content="å¤§æ¨¡å‹">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20240408231535224.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="æ–‡ç« ç›®å½•"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="ç«™ç‚¹æ¦‚è§ˆ"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="iroha"><img width="96" loading="lazy" src="/yun.png" alt="iroha"><span class="site-author-status" title="æ°¸è¿œç›¸ä¿¡ç¾å¥½çš„äº‹æƒ…å³å°†å‘ç”Ÿ">ğŸ˜Š</span></a><div class="site-author-name"><a href="/about/">iroha</a></div><a class="site-name" href="/about/site.html">iroha</a><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="é¦–é¡µ"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="å½’æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">14</span></a></div><div class="site-state-item"><a href="/categories/" title="åˆ†ç±»"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">4</span></a></div><div class="site-state-item"><a href="/tags/" title="æ ‡ç­¾"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">2</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="æ–‡æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="å‹é“¾" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">1.</span> <span class="toc-text">èƒŒæ™¯</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#freeze"><span class="toc-number">2.</span> <span class="toc-text">Freeze</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#bitfit"><span class="toc-number">3.</span> <span class="toc-text">Bitfit</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#prompt-tuning"><span class="toc-number">4.</span> <span class="toc-text">Prompt-tuning</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#lora"><span class="toc-number">5.</span> <span class="toc-text">Lora</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-number">5.1.</span> <span class="toc-text">åŸç†</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E9%80%9F%E8%AF%BB"><span class="toc-number">5.2.</span> <span class="toc-text">æºç é€Ÿè¯»</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#get_peft_model"><span class="toc-number">5.2.1.</span> <span class="toc-text">get_peft_model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#peftmodel"><span class="toc-number">5.2.2.</span> <span class="toc-text">PeftModel</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#loramodel"><span class="toc-number">5.2.3.</span> <span class="toc-text">LoraModel</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#loralayer"><span class="toc-number">5.2.4.</span> <span class="toc-text">LoraLayer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#linear"><span class="toc-number">5.2.5.</span> <span class="toc-text">Linear</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%88%E5%B9%B6%E6%93%8D%E4%BD%9C"><span class="toc-number">5.2.6.</span> <span class="toc-text">åˆå¹¶æ“ä½œ</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">6.</span> <span class="toc-text">å‚è€ƒèµ„æ–™</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://example.com/post/ft_survey.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="iroha"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="iroha"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ç»¼è¿°</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="åˆ›å»ºæ—¶é—´ï¼š2024-04-08 12:00:00" itemprop="dateCreated datePublished" datetime="2024-04-08T12:00:00+08:00">2024-04-08</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="ä¿®æ”¹æ—¶é—´ï¼š2024-04-13 03:32:20" itemprop="dateModified" datetime="2024-04-13T03:32:20+08:00">2024-04-13</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">NLP</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">å¤§æ¨¡å‹</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><h1 id="èƒŒæ™¯">èƒŒæ™¯</h1>
<p>â€‹
éšç€æ¨¡å‹çš„å‚æ•°è¶Šæ¥è¶Šå¤§ï¼Œä¸ºæ¯ä¸€ä¸ªä»»åŠ¡ä»å¤´è®­ç»ƒä¸€ä¸ªæ¨¡å‹è¶Šæ¥è¶Šå›°éš¾ï¼Œè€Œå‚æ•°é‡çš„å¢å¤§ä¹Ÿè®©æ¨¡å‹ç†è§£äº†æ›´æ·±å±‚çš„è¯­ä¹‰ä¿¡æ¯ã€‚éšç€BERTçš„å‡ºç°ï¼Œé¢„è®­ç»ƒ+å¾®è°ƒçš„æ–¹å¼èµ°ä¸Šå†å²èˆå°ã€‚</p>
<p>â€‹
ä¸ºä»€ä¹ˆä¸é€‰æ‹©å…¨å‚æ•°å¾®è°ƒå‘¢ï¼Ÿæœ€ä¸»è¦çš„åŸå› è¿˜æ˜¯æˆæœ¬å¤ªé«˜ï¼Œåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šæ„ˆå‘å›°éš¾ã€‚ä»¥GPT2-1.5Bä¸ºä¾‹ï¼Œè‹¥å‚æ•°ä¸æ¢¯åº¦ä»¥fp16ä¿å­˜ï¼Œæ˜¾å­˜éœ€è¦ä¿å­˜3+3
=
6Bçš„æ•°æ®ï¼Œè‹¥ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œé‚£ä¹ˆè¿˜éœ€è¦ä¿å­˜fp32çš„æ¨¡å‹å‚æ•°å¤‡ä»½ï¼Œmomentumä¸varianceï¼Œåˆéœ€è¦6+6+6
=
18Gï¼Œæ€»å…±éœ€è¦24Gã€‚å†åŠ ä¸Šå…¶ä»–çŠ¶æ€ï¼Œå¦‚activationï¼Œbufferï¼Œè¿˜æœ‰æ˜¾å­˜ç¢ç‰‡æ— æ³•åˆ©ç”¨ï¼Œå®é™…ä¸Šéœ€è¦çš„æ˜¾å­˜æ˜¯å¤§äº24Gçš„ã€‚</p>
<p>â€‹
å…¨å‚æ•°å¾®è°ƒè¿˜æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå½“LLMå°è¯•å­¦ä¹ å¤šä¸ªè¿ç»­ä»»åŠ¡æ—¶ï¼Œå®¹æ˜“å¿˜è®°ä¹‹å‰å­¦åˆ°çš„ä¸œè¥¿ï¼Œä¹Ÿå°±æ˜¯â€œç¾éš¾æ€§é—å¿˜â€ã€‚å¦‚ä½•åœ¨ä¿ç•™å…ˆå‰çŸ¥è¯†çš„åŸºç¡€ä¸Šå¢é‡åœ°å¢å¼ºLLMï¼Œå³è¿›è¡ŒæŒç»­å­¦ä¹ ï¼Œè‡³å…³é‡è¦ã€‚ç®€å•æ¥è¯´ï¼Œå…¨é‡å¾®è°ƒæœ‰overfittingï¼Œç¾éš¾æ€§é—å¿˜ï¼Œæˆæœ¬é«˜çš„é—®é¢˜ã€‚å› æ­¤ï¼Œé«˜æ•ˆå¾®è°ƒæ¨¡å‹çš„å‚æ•°å°±æˆäº†ä¸€ä¸ªæ–°æ–¹å‘ï¼ŒPEFT(Parameter-Efficient
Fine-tuning)åº”è¿è€Œç”Ÿã€‚</p>
<p>â€‹
å¾®è°ƒä¸€èˆ¬åªæ›´æ”¹æ¨¡å‹çš„éƒ¨åˆ†å‚æ•°ï¼Œè°ƒæ•´å“ªäº›å‚æ•°ï¼Œå¦‚ä½•è°ƒæ•´åˆ™å¯¹åº”äº†ä¸åŒæ–¹æ³•ã€‚ä¸€èˆ¬ä»ä¸‹æ¸¸ä»»åŠ¡å‡ºå‘ï¼Œæ¥å†³å®šä¸‹æ¸¸åº”è¯¥å¦‚ä½•æ·»åŠ å‚æ•°ã€‚</p>
<p>â€‹
ä¸‹é¢æˆ‘ä»¬ä»¥transformersåº“ä¸­çš„BERTä¸ºä¾‹ã€‚æˆ‘ä»¬éƒ½çŸ¥é“ï¼ŒBERTçš„é¢„è®­ç»ƒæ˜¯MLMä¸NSPï¼Œæ‰€ä»¥ä¸‹æ¸¸ä»»åŠ¡è‡ªç„¶æœ‰è¿™ä¸¤è€…ï¼Œä¹Ÿå°±æ˜¯<strong>ä¸‹ä¸€å¥é¢„æµ‹</strong>å’Œ<strong>å®Œå½¢å¡«ç©º</strong>ã€‚æ‰€ä»¥è¿™æ‰åº”è¯¥æ˜¯BERTçš„åŸç”Ÿä¸‹æ¸¸ä»»åŠ¡ã€‚</p>
<p>â€‹ é¦–å…ˆçœ‹NSPï¼Œæ¨¡å‹çš„ç»“æ„å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># assert logits[0, 0] &lt; logits[0, 1]  # next sentence was random
class BertForNextSentencePrediction(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)

        self.bert &#x3D; BertModel(config)
        self.cls &#x3D; BertOnlyNSPHead(config)

        # Initialize weights and apply final processing
        self.post_init()

        
class BertOnlyNSPHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.seq_relationship &#x3D; nn.Linear(config.hidden_size, 2)

    def forward(self, pooled_output):
        seq_relationship_score &#x3D; self.seq_relationship(pooled_output)
        return seq_relationship_score</code></pre>
<p>â€‹
å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äºNSPä»»åŠ¡ï¼Œå°±æ˜¯åœ¨Modelçš„åé¢æ‹¼ä¸€ä¸ªLinearå±‚ï¼Œå°†768ç»´æ˜ å°„åˆ°2ç»´ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ã€‚</p>
<p>â€‹
å†çœ‹SequenceClassificationä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯åºåˆ—åˆ†ç±»ã€‚å®é™…ä¸Šä¹Ÿæ˜¯åœ¨æœ€åå¡«å……äº†ä¸€å±‚Linearå±‚ç”¨äºåˆ†ç±»ã€‚ä¸NSPçš„åŒºåˆ«åœ¨äºpoolerå±‚ä¼šå…ˆç»è¿‡ä¸€æ¬¡dropoutï¼Œdropoutçš„æ¦‚ç‡å¯ä»¥åœ¨configä¸­è®¾ç½®ï¼Œé»˜è®¤ä¸º0.1ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class BertForSequenceClassification(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels &#x3D; config.num_labels
        self.config &#x3D; config

        self.bert &#x3D; BertModel(config)
        classifier_dropout &#x3D; (
            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob
        )
        self.dropout &#x3D; nn.Dropout(classifier_dropout)
        self.classifier &#x3D; nn.Linear(config.hidden_size, config.num_labels)</code></pre>
<p>â€‹ <img src="../images/image-20240408231535224.png"
alt="image-20240408231535224" /></p>
<p>â€‹ PEFTæŒ‰æ ¸å¿ƒæ€æƒ³å¯ä»¥åˆ†ä¸ºä¸‰ç±»ï¼š</p>
<ol type="1">
<li>æ·»åŠ ä¸€äº›å‚æ•°é‡å°çš„å±‚ï¼Œåªå¾®è°ƒè¿™äº›å±‚ï¼Œæœ€å…¸å‹çš„å°±æ˜¯LSTã€‚</li>
<li>é€‰æ‹©æŸäº›å±‚ï¼Œæˆ–å±‚ä¸­çš„æŸä¸€éƒ¨åˆ†è¿›è¡Œå¾®è°ƒï¼Œæœ€å…¸å‹çš„æ˜¯BitFitï¼Œåªå¯¹biasè¿›è¡Œå¾®è°ƒã€‚</li>
<li>é‡å‚æ•°åŒ–ï¼Œä¹Ÿç®—æ˜¯å¢åŠ ä¸€éƒ¨åˆ†å‚æ•°ï¼Œä½†æœ€ååŠ å›åŸå‚æ•°çš„å¯¹åº”éƒ¨åˆ†</li>
</ol>
<h1 id="freeze">Freeze</h1>
<p>â€‹
å†»ç»“æŸäº›å‚æ•°æ˜¯æœ€å®¹æ˜“æƒ³åˆ°çš„æ–¹æ³•ï¼Œä»…ä»…è°ƒæ•´æŸäº›æœªè¢«å†»ç»“çš„å‚æ•°å°±å¯ä»¥å‡å°‘å¤§é‡æ˜¾å­˜å ç”¨ï¼Œä½†freezeæ–¹æ³•å¤§å¤§é™ä½äº†æ¨¡å‹çš„çµæ´»æ€§ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">for name ,param in model.parameters():
	if not any (n in name for n in [layers_name1,layer_name2...])
		param.requires_grad &#x3D; False</code></pre>
<h1 id="bitfit">Bitfit</h1>
<p>å‡ºè‡ª<strong>BitFit: Simple Parameter-efficient Fine-tuning or
Transformer-based Masked Language-models</strong>ã€‚</p>
<p>pass</p>
<h1 id="prompt-tuning">Prompt-tuning</h1>
<p>pass</p>
<h1 id="lora">Lora</h1>
<h2 id="åŸç†">åŸç†</h2>
<figure>
<img src="../images/image-20240410215838916.png"
alt="image-20240410215838916" />
<figcaption aria-hidden="true">image-20240410215838916</figcaption>
</figure>
<p>â€‹ åœ¨åŸå§‹æƒé‡ä¸­å¹¶ä¸Šä¸€ä¸ªæ—è·¯çš„åˆ†æ”¯ï¼Œä»¥Linearå±‚ä¸ºä¾‹å­ï¼ŒåŸæœ¬<span
class="math inline">\(h = W_{dÃ—k}X\)</span>ï¼Œ<span
class="math inline">\(W\)</span>æ˜¯æƒé‡ï¼Œæ—è·¯æœ‰ä¸¤ä¸ªä½ç§©çš„çŸ©é˜µï¼Œå…¶ä¸­<span
class="math inline">\(A_{rÃ—k} =
N(0,Ïƒ^2)\)</span>ï¼Œä¹Ÿå°±æ˜¯ä»¥é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–ï¼Œè€Œ<span
class="math inline">\(B_{dÃ—r}\)</span>çŸ©é˜µåˆ™ä»¥å…¨0åˆå§‹åŒ–ï¼Œå…¶ä¸­ç»´åº¦rè¿œå°äºdå’Œkã€‚<span
class="math inline">\(Î”W = BA\)</span> ,æœ€ç»ˆçš„æƒé‡ä¸º$W+Î”W <span
class="math inline">\(ã€‚å¯¹äº\)</span>Î”W<span
class="math inline">\(å¯ä»¥ä½¿ç”¨ä¸€ä¸ªÎ±å‚æ•°æ¥æ§åˆ¶å€æ•°ï¼Œå³\)</span>W+Î”W$ã€‚Bä¸ºå‡ç»´çŸ©é˜µï¼ŒAä¸ºé™ç»´çŸ©é˜µã€‚å®é™…ä¸Šï¼ŒLoRAä¸€èˆ¬ç”¨äºDenseå±‚ã€‚</p>
<p>â€‹ å¯¹äºæ¢¯åº¦è®¡ç®—ï¼Œå€Ÿç”¨çŸ¥ä¹ä¸ŠCodeLearnerç­”ä¸»çš„ä¸€å¼ å›¾ã€‚</p>
<p>â€‹ <img src="../images/image-20240411005445372.png"
alt="image-20240411005445372" /></p>
<p>â€‹
æ‰€ä»¥ï¼Œåœ¨å¾®è°ƒæ—¶ï¼ŒæŸä¸€Loraå±‚åå‘ä¼ æ’­ä¸­çš„æ¢¯åº¦è®¡ç®—é‡æ˜¯è¦æ›´å¤šçš„ï¼Œä½†ç”±äºrè¿œå°äºåŸæƒé‡çš„ç»´åº¦då’Œkï¼Œæ‰€ä»¥ç›¸å¯¹äºå…¨é‡å¾®è°ƒä¿å­˜çš„æ¢¯åº¦å€¼å°±å°‘ã€‚åŒæ—¶ï¼ŒåŸè®ºæ–‡ä¸­ä¹Ÿåªå¯¹Transformer
Blockä¸­çš„Q,K,Vè¿›è¡Œäº†Loraå¤„ç†ã€‚è‹¥rç­‰äºkï¼Œé‚£ä¹ˆæ­¤æ—¶å¯ä»¥ç­‰ä»·äºå…¨å‚æ•°å¾®è°ƒã€‚</p>
<p>â€‹ å¯¹äºåˆå§‹åŒ–é—®é¢˜ï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¿è¯æœ€å¼€å§‹çš„<span
class="math inline">\(Î”W=0\)</span>ï¼Œæ‰€ä»¥éœ€è¦ABçš„å…¶ä¸­ä¸€è€…ä¸º0ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬çœ‹ä¸Šå›¾å¸¦ä¸ŠLoRAåçš„åå‘ä¼ æ’­æ¢¯åº¦è®¡ç®—ï¼Œè‹¥Aä¸º0ï¼Œé‚£ä¹ˆæ¢¯åº¦å°±ä¼šä¸€ç›´ä¸º0ï¼Œ<span
class="math inline">\(Î”W\)</span>å°±ä¸ä¼šæ›´æ–°ã€‚</p>
<p>â€‹
åŸæ–‡ä¸­æåˆ°äº†LoRAçš„Limitationï¼Œå¦‚æœé€‰æ‹©å°†Aå’ŒBçŸ©é˜µå¸æ”¶(åˆå¹¶)åˆ°WåŸå§‹æƒé‡çŸ©é˜µä¸­,ä»¥æ¶ˆé™¤é¢å¤–çš„æ¨ç†å»¶è¿Ÿ,é‚£ä¹ˆåœ¨å•ä¸ªå‰å‘ä¼ é€’ä¸­ä¸ºä¸åŒä»»åŠ¡çš„ä¸åŒAå’ŒBæ‰¹é‡è¾“å…¥æ•°æ®å°±å˜å¾—ä¸ç›´è§‚äº†ã€‚æˆ‘æƒ³åŸæ–‡çš„æ„æ€æ˜¯ä¸€ä¸ªbatché‡Œå¯èƒ½æœ‰ä¸åŒçš„ä»»åŠ¡ï¼Œé‚£ä¹ˆä¸åŒçš„ä»»åŠ¡åº”è¯¥ç”¨ä¸åŒçš„æƒé‡ï¼Œé‚£ä¹ˆæœ€å¥½æ˜¯ä¸å°†<span
class="math inline">\(Î”W\)</span>åˆå¹¶åˆ°åŸå§‹æƒé‡ï¼Œé’ˆå¯¹ä¸åŒä»»åŠ¡æ¥åŠ¨æ€é€‰æ‹©<span
class="math inline">\(Î”W\)</span>ï¼Œè¿™éœ€è¦åœ¨æ¨ç†é€Ÿåº¦ä¸Šåšå–èˆã€‚</p>
<h2 id="æºç é€Ÿè¯»">æºç é€Ÿè¯»</h2>
<p>â€‹ ä¸ºäº†æ–¹ä¾¿ç†è§£ï¼Œæˆ‘ä»¬ä»peft0.10.0çš„å®˜æ–¹ç¤ºä¾‹å‡ºå‘ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from transformers import AutoModelForSeq2SeqLM
from peft import get_peft_config, get_peft_model, LoraConfig, TaskType
model_name_or_path &#x3D; &quot;bigscience&#x2F;mt0-large&quot;
tokenizer_name_or_path &#x3D; &quot;bigscience&#x2F;mt0-large&quot;

peft_config &#x3D; LoraConfig(
    task_type&#x3D;TaskType.SEQ_2_SEQ_LM, inference_mode&#x3D;False, r&#x3D;8, lora_alpha&#x3D;32, lora_dropout&#x3D;0.1
)

model &#x3D; AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
#ç­‰ä»·äº model &#x3D; PeftModelForSeq2SeqLM(model,peft_config)
#ä¹Ÿç­‰ä»·äºmodel &#x3D; lora_model &#x3D; LoraModel(model, config, &quot;default&quot;)
model &#x3D; get_peft_model(model, peft_config) 

model.print_trainable_parameters()
&quot;trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282&quot;</code></pre>
<p>â€‹
é¦–å…ˆä»LoraConfigå‡ºå‘ï¼Œè¿™é‡Œçš„task_typeæ˜¯PeftConfigçš„å‚æ•°ï¼Œå…¶ä½™åˆ™æ˜¯LoraConfigçš„å‚æ•°ï¼Œè¿™äº›å‚æ•°åœ¨ä¸Šæ–‡çš„åŸç†ä¸­éƒ½æœ‰æåˆ°ã€‚ä¸»è¦è¿˜æ˜¯æ ¹æ®ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡è¿”å›ä¸åŒçš„æ¨¡å‹ã€‚è¿™é‡Œä¹Ÿå¯ä»¥ç›´æ¥ç”¨get_peft_configå‡½æ•°æ¥è¯»å–ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">peft_config &#x3D; LoraConfig(
    task_type&#x3D;TaskType.SEQ_2_SEQ_LM, inference_mode&#x3D;False, r&#x3D;8, lora_alpha&#x3D;32, lora_dropout&#x3D;0.1
)
----
config &#x3D; &#123;
    &quot;task_type&quot;:&quot;SEQ_2_SEQ_LM&quot;,
    &quot;peft_type&quot;:&quot;LORA&quot;,
    &quot;inference_mode&quot;:False,
    &quot;r&quot;:8,
    &quot;lora_alpha&quot;:32,
    &quot;lora_dropout&quot;:0.1
&#125;

another_config &#x3D; get_peft_config(config)</code></pre>
<h3 id="get_peft_model">get_peft_model</h3>
<p>â€‹
æ¥ä¸‹æ¥çœ‹get_peft_modelã€‚ä¼ å…¥çš„å‚æ•°æœ‰æ¨¡å‹å’Œå¯¹åº”çš„configï¼Œæœ€åä¼šè¿”å›ä¸€ä¸ªPeftModelå®ä¾‹ã€‚è¿”å›çš„å®ä¾‹ç±»å‹ä¼šæ ¹æ®ä¼ å…¥çš„configæ¥ç¡®å®šã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model &#x3D; get_peft_model(model, peft_config)
---------mapping.py----------------
def get_peft_model(
    model: PreTrainedModel, peft_config: PeftConfig, adapter_name: str &#x3D; &quot;default&quot;, mixed: bool &#x3D; False
) -&gt; PeftModel | PeftMixedModel:
    &quot;&quot;&quot;
    Returns a Peft model object from a model and a config.

    Args:
        model ([&#96;transformers.PreTrainedModel&#96;]):
            Model to be wrapped.
        peft_config ([&#96;PeftConfig&#96;]):
            Configuration object containing the parameters of the Peft model.
        adapter_name (&#96;str&#96;, &#96;optional&#96;, defaults to &#96;&quot;default&quot;&#96;):
            The name of the adapter to be injected, if not provided, the default adapter name is used (&quot;default&quot;).
        mixed (&#96;bool&#96;, &#96;optional&#96;, defaults to &#96;False&#96;):
            Whether to allow mixing different (compatible) adapter types.
    &quot;&quot;&quot;
    model_config &#x3D; getattr(model, &quot;config&quot;, &#123;&quot;model_type&quot;: &quot;custom&quot;&#125;)
    if hasattr(model_config, &quot;to_dict&quot;):
        model_config &#x3D; model_config.to_dict()

    peft_config.base_model_name_or_path &#x3D; model.__dict__.get(&quot;name_or_path&quot;, None)

    if mixed:
        return PeftMixedModel(model, peft_config, adapter_name&#x3D;adapter_name)

    if peft_config.task_type not in MODEL_TYPE_TO_PEFT_MODEL_MAPPING.keys() and not peft_config.is_prompt_learning:
        return PeftModel(model, peft_config, adapter_name&#x3D;adapter_name)

    if peft_config.is_prompt_learning:
        peft_config &#x3D; _prepare_prompt_learning_config(peft_config, model_config)
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name&#x3D;adapter_name)
----------------------
MODEL_TYPE_TO_PEFT_MODEL_MAPPING: dict[str, PeftModel] &#x3D; &#123;
    &quot;SEQ_CLS&quot;: PeftModelForSequenceClassification,
    &quot;SEQ_2_SEQ_LM&quot;: PeftModelForSeq2SeqLM,
    &quot;CAUSAL_LM&quot;: PeftModelForCausalLM,
    &quot;TOKEN_CLS&quot;: PeftModelForTokenClassification,
    &quot;QUESTION_ANS&quot;: PeftModelForQuestionAnswering,
    &quot;FEATURE_EXTRACTION&quot;: PeftModelForFeatureExtraction,
&#125;
</code></pre>
<p>â€‹
å…³é”®åœ¨äºè¿™ä¸¤å¥ã€‚å¦‚æœä»»åŠ¡ç±»å‹ä¸åœ¨æ”¯æŒçš„ç‰¹å®šä»»åŠ¡ä¸­ï¼Œè¿”å›PeftModelï¼Œå¦åˆ™è¿”å›å¯¹åº”ä»»åŠ¡ç±»å‹çš„Modelï¼Œè¿™äº›Modelç»§æ‰¿äº†PeftModelã€‚è‹¥æ˜¯æç¤ºå­¦ä¹ ç±»å‹çš„ï¼Œå¦‚prompt-tuningï¼Œåˆ™éœ€è¦é¢å¤–çš„configä¿¡æ¯ï¼Œå¦‚éšè—å±‚çš„æ•°é‡,å¯èƒ½çš„é”®åŒ…æ‹¬<code>num_hidden_layers</code>ã€<code>num_layers</code>ã€<code>n_layer</code>ï¼Œ
å¦‚æœæ— æ³•æ‰¾åˆ°,åˆ™éœ€è¦åœ¨peft_configä¸­æ‰‹åŠ¨æŒ‡å®šnum_layersï¼Œè¿™ä¸ªå‚æ•°æŒ‡å®šäº†promptå°†è¢«æ³¨å…¥åˆ°æ¨¡å‹çš„å“ªäº›å±‚ã€‚è¿˜éœ€è¦æ³¨æ„åŠ›çš„å¤´æ•°ï¼Œencoderéšè—å±‚çš„å¤§å°å’Œtokenembeddingçš„ç»´åº¦ç­‰ç­‰ã€‚è¿™äº›å‚æ•°è‹¥ä»æ¨¡å‹çš„configä¸­æ‰¾ä¸åˆ°ï¼Œåˆ™éœ€è¦åœ¨peftconfigä¸­è‡ªè¡ŒæŒ‡å®šã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">    if peft_config.task_type not in MODEL_TYPE_TO_PEFT_MODEL_MAPPING.keys() and not peft_config.is_prompt_learning:
        return PeftModel(model, peft_config, adapter_name&#x3D;adapter_name)

    if peft_config.is_prompt_learning:
        peft_config &#x3D; _prepare_prompt_learning_config(peft_config, model_config)
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name&#x3D;adapter_name)
-------------------
def _prepare_prompt_learning_config(peft_config, model_config):
    if peft_config.num_layers is None:
        if &quot;num_hidden_layers&quot; in model_config:
            num_layers &#x3D; model_config[&quot;num_hidden_layers&quot;]
        elif &quot;num_layers&quot; in model_config:
            num_layers &#x3D; model_config[&quot;num_layers&quot;]
        elif &quot;n_layer&quot; in model_config:
            num_layers &#x3D; model_config[&quot;n_layer&quot;]
        else:
            raise ValueError(&quot;Please specify &#96;num_layers&#96; in &#96;peft_config&#96;&quot;)
        peft_config.num_layers &#x3D; num_layers

    if peft_config.token_dim is None:
        if &quot;hidden_size&quot; in model_config:
            token_dim &#x3D; model_config[&quot;hidden_size&quot;]
        elif &quot;n_embd&quot; in model_config:
            token_dim &#x3D; model_config[&quot;n_embd&quot;]
        elif &quot;d_model&quot; in model_config:
            token_dim &#x3D; model_config[&quot;d_model&quot;]
        else:
            raise ValueError(&quot;Please specify &#96;token_dim&#96; in &#96;peft_config&#96;&quot;)
        peft_config.token_dim &#x3D; token_dim

    if peft_config.num_attention_heads is None:
        if &quot;num_attention_heads&quot; in model_config:
            num_attention_heads &#x3D; model_config[&quot;num_attention_heads&quot;]
        elif &quot;n_head&quot; in model_config:
            num_attention_heads &#x3D; model_config[&quot;n_head&quot;]
        elif &quot;num_heads&quot; in model_config:
            num_attention_heads &#x3D; model_config[&quot;num_heads&quot;]
        elif &quot;encoder_attention_heads&quot; in model_config:
            num_attention_heads &#x3D; model_config[&quot;encoder_attention_heads&quot;]
        else:
            raise ValueError(&quot;Please specify &#96;num_attention_heads&#96; in &#96;peft_config&#96;&quot;)
        peft_config.num_attention_heads &#x3D; num_attention_heads

    if getattr(peft_config, &quot;encoder_hidden_size&quot;, None) is None:
        setattr(peft_config, &quot;encoder_hidden_size&quot;, peft_config.token_dim)

    return peft_config</code></pre>
<p>â€‹
å›åˆ°é‡ç‚¹ï¼Œæˆ‘ä»¬ä»¥Seq2SeqModelçš„ä»£ç ä¸ºä¾‹ï¼Œç›¸å¯¹äºPeftModelï¼Œæ•´ä½“ç»“æ„ä¸€æ ·ï¼Œåªæ˜¯å¤šäº†ä¸¤ä¸ªå‚æ•°å˜é‡ï¼Œå…¶ä¸­prepare_inputs_for_generationå‡½æ•°éœ€è¦ç”Ÿæˆæ¨¡å‹åœ¨generateæ–¹æ³•ä¸­è‡ªè¡Œå®ç°ï¼Œbase_model_prepare_encoder_decoder_kwargs_for_generationå˜é‡åˆ™ä»base_modelä¸­æå–å‡ºä¸€äº›ç”Ÿæˆæ—¶çš„å‚æ•°ï¼Œå¦‚æ˜¯å¦è¦ä½¿ç”¨cacheçš„use_caheï¼Œå°†encoderçš„å‚æ•°å°è£…è¿›model_kwargsè¿”å›ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class PeftModelForSeq2SeqLM(PeftModel):
	def __init__(self, model: torch.nn.Module, peft_config: PeftConfig, adapter_name: str &#x3D; &quot;default&quot;) -&gt; None:
        super().__init__(model, peft_config, adapter_name)
        self.base_model_prepare_inputs_for_generation &#x3D; self.base_model.prepare_inputs_for_generation
        self.base_model_prepare_encoder_decoder_kwargs_for_generation &#x3D; (
            self.base_model._prepare_encoder_decoder_kwargs_for_generation
        )
</code></pre>
<h3 id="peftmodel">PeftModel</h3>
<p>â€‹ è¿˜æ˜¯å…ˆè´´å‡ºæ•´ä½“çš„åˆå§‹åŒ–æºç ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class PeftModel(PushToHubMixin, torch.nn.Module):
    
    def __init__(self, model: PreTrainedModel, peft_config: PeftConfig, adapter_name: str &#x3D; &quot;default&quot;) -&gt; None:
        super().__init__()
        self.modules_to_save &#x3D; None
        self.active_adapter &#x3D; adapter_name
        self.peft_type &#x3D; peft_config.peft_type
        # These args are special PEFT arguments that users can pass. They need to be removed before passing them to
        # forward.
        self.special_peft_forward_args &#x3D; &#123;&quot;adapter_names&quot;&#125;

        self._is_prompt_learning &#x3D; peft_config.is_prompt_learning
        if self._is_prompt_learning:
            self._peft_config &#x3D; &#123;adapter_name: peft_config&#125;
            self.base_model &#x3D; model
            self.add_adapter(adapter_name, peft_config)
        else:
            self._peft_config &#x3D; None
            cls &#x3D; PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type]
            self.base_model &#x3D; cls(model, &#123;adapter_name: peft_config&#125;, adapter_name)
            self.set_additional_trainable_modules(peft_config, adapter_name)

        if getattr(model, &quot;is_gradient_checkpointing&quot;, True):
            model &#x3D; self._prepare_model_for_gradient_checkpointing(model)

        # the &#96;pretraining_tp&#96; is set for some models to simulate Tensor Parallelism during inference to avoid
        # numerical differences, https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;issues&#x2F;76232 - to avoid any unexpected
        # behavior we disable that in this line.
        if hasattr(self.base_model, &quot;config&quot;) and hasattr(self.base_model.config, &quot;pretraining_tp&quot;):
            self.base_model.config.pretraining_tp &#x3D; 1</code></pre>
<p>â€‹
PeftModelä¸­çš„æºç å¾ˆå¤§éƒ¨åˆ†æœåŠ¡äºæç¤ºå­¦ä¹ ï¼Œå…ˆä¸çœ‹ã€‚æˆ‘ä»¬åªçœ‹_is_prompt_learningæ˜¯Falseçš„æƒ…å†µã€‚</p>
<pre class="line-numbers language-none"><code class="language-none">else:
    self._peft_config &#x3D; None
    cls &#x3D; PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type]
    self.base_model &#x3D; cls(model, &#123;adapter_name: peft_config&#125;, adapter_name)
    self.set_additional_trainable_modules(peft_config, adapter_name)</code></pre>
<p>â€‹
å¯ä»¥çœ‹åˆ°ï¼Œclsæœ€ç»ˆæ˜ å°„åˆ°å¯¹åº”peftæ–¹æ¡ˆçš„æ¨¡å‹å®ä¾‹ï¼Œè‹¥æ˜¯LORAï¼Œåˆ™è¿”å›ä¸€ä¸ªLoraModelå®ä¾‹ï¼Œçœ‹æ¥å…³é”®å°±åœ¨äºLoraModelä¸­ã€‚</p>
<h3 id="loramodel">LoraModel</h3>
<p>â€‹ LoraModelç»§æ‰¿äº†BaseTunerï¼Œè€ŒBaseTunerç»§æ‰¿nn.Moduleã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LoraModel(BaseTuner):
    prefix: str &#x3D; &quot;lora_&quot;

    def __init__(self, model, config, adapter_name) -&gt; None:
        super().__init__(model, config, adapter_name)</code></pre>
<p>â€‹
ç”±äºä»£ç é‡éå¸¸å¤§ï¼Œæ‰€ä»¥åªè´´å‡ºæ ¸å¿ƒçš„ä»£ç ï¼Œè¿™ä¸€æ®µæ˜¯LoRAçš„æ ¸å¿ƒé€»è¾‘ã€‚é¦–å…ˆåˆ¤æ–­targetæ˜¯ä¸æ˜¯Loraå±‚ï¼Œè‹¥æ˜¯ï¼Œåˆ™æ ¹æ®loraconfigæ¥æ›´æ–°è¿™ä¸€å±‚ã€‚è‹¥ä¸æ˜¯ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„Loraå±‚æ¥æ›¿æ¢åŸæ¥çš„å±‚ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def _create_and_replace(
    self,
    lora_config,
    adapter_name,
    target,
    target_name,
    parent,
    current_key,
):
    &quot;&quot;&quot;
    æ­¤å¤„çœç•¥éƒ¨åˆ†ä»£ç 
    &quot;&quot;&quot;
    from peft.tuners.adalora import AdaLoraLayer

    if isinstance(target, LoraLayer) and not isinstance(target, AdaLoraLayer):
        target.update_layer(
            adapter_name,
            r,
            lora_alpha&#x3D;alpha,
            lora_dropout&#x3D;lora_config.lora_dropout,
            init_lora_weights&#x3D;lora_config.init_lora_weights,
            use_rslora&#x3D;lora_config.use_rslora,
            use_dora&#x3D;lora_config.use_dora,
        )
    else:
        new_module &#x3D; self._create_new_module(lora_config, adapter_name, target, **kwargs)
        if adapter_name !&#x3D; self.active_adapter:
            # adding an additional adapter: it is not automatically trainable
            new_module.requires_grad_(False)
        self._replace_module(parent, target_name, new_module, target)</code></pre>
<p>â€‹
ä¸‹é¢æ˜¯<strong>update_layer</strong>å‡½æ•°çš„é€»è¾‘ï¼Œé¦–å…ˆä¿è¯ç§©å¤§äº0ï¼Œç„¶ååˆ›å»ºdropoutå±‚ã€‚</p>
<p>â€‹
å†åˆ›å»ºABçŸ©é˜µï¼Œå¯ä»¥çœ‹åˆ°Aæ˜¯é™ç»´çŸ©é˜µï¼ŒBæ˜¯å‡ç»´çŸ©é˜µã€‚ä¹‹ååˆå§‹åŒ–è¿™ä¸¤ä¸ªçŸ©é˜µï¼Œå¯ä»¥çœ‹åˆ°AçŸ©é˜µå¯ä»¥é€‰æ‹©kaiming
uniformæˆ–é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–ã€‚è‹¥æ˜¯ç”¨äºEmbeddingå±‚ï¼Œé‚£ä¹ˆAåˆå§‹åŒ–ä¸º0ï¼ŒBç”¨é«˜æ–¯åˆ†å¸ƒã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  def update_layer(
      self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights, use_rslora, use_dora: bool &#x3D; False
  ):
      # This code works for linear layers, override for other layer types
      if r &lt;&#x3D; 0:
          raise ValueError(f&quot;&#96;r&#96; should be a positive integer value but the value passed is &#123;r&#125;&quot;)

      self.r[adapter_name] &#x3D; r
      self.lora_alpha[adapter_name] &#x3D; lora_alpha
      if lora_dropout &gt; 0.0:
          lora_dropout_layer &#x3D; nn.Dropout(p&#x3D;lora_dropout)
      else:
          lora_dropout_layer &#x3D; nn.Identity()

      self.lora_dropout.update(nn.ModuleDict(&#123;adapter_name: lora_dropout_layer&#125;))
      # Actual trainable parameters
      self.lora_A[adapter_name] &#x3D; nn.Linear(self.in_features, r, bias&#x3D;False)
      self.lora_B[adapter_name] &#x3D; nn.Linear(r, self.out_features, bias&#x3D;False)
      if use_rslora:
          self.scaling[adapter_name] &#x3D; lora_alpha &#x2F; math.sqrt(r)
      else:
          self.scaling[adapter_name] &#x3D; lora_alpha &#x2F; r

      if init_lora_weights &#x3D;&#x3D; &quot;loftq&quot;:
          self.loftq_init(adapter_name)
      elif init_lora_weights:
          self.reset_lora_parameters(adapter_name, init_lora_weights)
&quot;&quot;&quot;
ä¸‹ç•¥
&quot;&quot;&quot;

      self.set_adapter(self.active_adapters)
    -----------------------------------------------------
  def reset_lora_parameters(self, adapter_name, init_lora_weights):
      if init_lora_weights is False:
          return

      if adapter_name in self.lora_A.keys():
          if init_lora_weights is True:
              # initialize A the same way as the default for nn.Linear and B to zero
              # https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;LoRA&#x2F;blob&#x2F;a0a92e0f26c067cf94747bdbf1ce73793fa44d19&#x2F;loralib&#x2F;layers.py#L124
              nn.init.kaiming_uniform_(self.lora_A[adapter_name].weight, a&#x3D;math.sqrt(5))
          elif init_lora_weights.lower() &#x3D;&#x3D; &quot;gaussian&quot;:
              nn.init.normal_(self.lora_A[adapter_name].weight, std&#x3D;1 &#x2F; self.r[adapter_name])
          else:
              raise ValueError(f&quot;Unknown initialization &#123;init_lora_weights&#x3D;&#125;&quot;)
          nn.init.zeros_(self.lora_B[adapter_name].weight)
      if adapter_name in self.lora_embedding_A.keys():
          # initialize a the same way as the default for nn.linear and b to zero
          nn.init.zeros_(self.lora_embedding_A[adapter_name])
          nn.init.normal_(self.lora_embedding_B[adapter_name])</code></pre>
<h3 id="loralayer">LoraLayer</h3>
<p>â€‹
å¯ä»¥çœ‹åˆ°layeréœ€è¦æ˜¯LoraLayerçš„å®ä¾‹æ‰ä¼šè¢«æ›´æ–°ï¼Œä¸‹é¢æ˜¯LoraLayerçš„ä»£ç ã€‚æœ€åˆçš„peft0.1.0ï¼ŒLoraåªèƒ½ç”¨äºLinearå±‚ï¼Œåæ¥åˆ™å°†ä¸€äº›denseå±‚ç»Ÿä¸€æŠ½è±¡æˆä¸ºäº†LoraLayerï¼Œå¯ä»¥çœ‹åˆ°æœ‰Linearï¼ŒEmbeddingï¼ŒConv1Då’ŒConv2Dã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LoraLayer(BaseTunerLayer):
    # All names of layers that may contain (trainable) adapter weights
    adapter_layer_names &#x3D; (&quot;lora_A&quot;, &quot;lora_B&quot;, &quot;lora_embedding_A&quot;, &quot;lora_embedding_B&quot;)
    # All names of other parameters that may contain adapter-related parameters
    other_param_names &#x3D; (&quot;r&quot;, &quot;lora_alpha&quot;, &quot;scaling&quot;, &quot;lora_dropout&quot;)

    def __init__(self, base_layer: nn.Module, **kwargs) -&gt; None:
        self.base_layer &#x3D; base_layer
        self.r &#x3D; &#123;&#125;
        self.lora_alpha &#x3D; &#123;&#125;
        self.scaling &#x3D; &#123;&#125;
        self.lora_dropout &#x3D; nn.ModuleDict(&#123;&#125;)
        self.lora_A &#x3D; nn.ModuleDict(&#123;&#125;)
        self.lora_B &#x3D; nn.ModuleDict(&#123;&#125;)
        # For Embedding layer
        self.lora_embedding_A &#x3D; nn.ParameterDict(&#123;&#125;)
        self.lora_embedding_B &#x3D; nn.ParameterDict(&#123;&#125;)
        # Mark the weight as unmerged
        self._disable_adapters &#x3D; False
        self.merged_adapters &#x3D; []
        self.use_dora: dict[str, bool] &#x3D; &#123;&#125;
        self.lora_magnitude_vector: Optional[torch.nn.ParameterDict] &#x3D; None  # for DoRA
        self._caches: dict[str, Any] &#x3D; &#123;&#125;
        self.kwargs &#x3D; kwargs

        base_layer &#x3D; self.get_base_layer()
        if isinstance(base_layer, nn.Linear):
            in_features, out_features &#x3D; base_layer.in_features, base_layer.out_features
        elif isinstance(base_layer, nn.Conv2d):
            in_features, out_features &#x3D; base_layer.in_channels, base_layer.out_channels
        elif isinstance(base_layer, nn.Embedding):
            in_features, out_features &#x3D; base_layer.num_embeddings, base_layer.embedding_dim
        elif isinstance(base_layer, Conv1D):
            in_features, out_features &#x3D; (
                base_layer.weight.ds_shape if hasattr(base_layer.weight, &quot;ds_shape&quot;) else base_layer.weight.shape
            )
        elif hasattr(base_layer, &quot;infeatures&quot;) and hasattr(base_layer, &quot;outfeatures&quot;):
            # QuantLinear
            in_features, out_features &#x3D; base_layer.infeatures, base_layer.outfeatures
        elif hasattr(base_layer, &quot;input_size&quot;) and hasattr(base_layer, &quot;output_size&quot;):
            # Megatron ColumnParallelLinear,RowParallelLinear
            in_features, out_features &#x3D; base_layer.input_size, base_layer.output_size
        elif hasattr(base_layer, &quot;codebooks&quot;) and base_layer.__class__.__name__ &#x3D;&#x3D; &quot;QuantizedLinear&quot;:
            # AQLM QuantLinear
            in_features, out_features &#x3D; base_layer.in_features, base_layer.out_features
        elif hasattr(base_layer, &quot;w_bit&quot;) and base_layer.__class__.__name__ &#x3D;&#x3D; &quot;WQLinear_GEMM&quot;:
            # Awq layers
            in_features, out_features &#x3D; base_layer.in_features, base_layer.out_features
        else:
            raise ValueError(f&quot;Unsupported layer type &#123;type(base_layer)&#125;&quot;)

        self.in_features &#x3D; in_features
        self.out_features &#x3D; out_features</code></pre>
<p>â€‹
åœ¨LoraModelä¸­çš„**_create_new_module**å‡½æ•°ä¸­æˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹åˆ°æŠ›å‡ºçš„å¼‚å¸¸ï¼Œåº”è¯äº†å½“targetä¸æ˜¯LoraLayerçš„æ—¶å€™ï¼Œå¦‚æœæƒ³åˆ›å»ºä¸€ä¸ªæ–°Loraæ¨¡å—å°†æ—§æ¨¡å—æ›¿æ¢ï¼Œé‚£ä¹ˆéœ€è¦åŸæ¨¡å—æ˜¯torch.nn.Linear,
torch.nn.Embedding, torch.nn.Conv2d,
transformers.pytorch_utils.Conv1Dè¿™å‡ ç§ç±»å‹ï¼Œæœ€ç»ˆè¿™å‡ ä¸ªç±»åœ¨LoraLayer.pyä¸­éƒ½è¢«é‡å†™äº†ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if new_module is None:
    # no module could be matched
    raise ValueError(
        f&quot;Target module &#123;target&#125; is not supported. Currently, only the following modules are supported: &quot;
        &quot;&#96;torch.nn.Linear&#96;, &#96;torch.nn.Embedding&#96;, &#96;torch.nn.Conv2d&#96;, &#96;transformers.pytorch_utils.Conv1D&#96;.&quot;
    )</code></pre>
<p>â€‹
åŒæ—¶ï¼Œå¯¹äºä¸åŒçš„æ¨¡å‹ï¼Œæ”¯æŒçš„Loraç­–ç•¥ä¹Ÿä¸åŒã€‚Transformeråº“ä¸­åˆ—ä¸¾äº†å“ªäº›æ¨¡å‹çš„å“ªäº›å±‚èƒ½å¤Ÿä½¿ç”¨å®˜æ–¹Loraæ–¹æ¡ˆã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING &#x3D; &#123;
    &quot;t5&quot;: [&quot;q&quot;, &quot;v&quot;],
    &quot;mt5&quot;: [&quot;q&quot;, &quot;v&quot;],
    &quot;bart&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;gpt2&quot;: [&quot;c_attn&quot;],
    &quot;bloom&quot;: [&quot;query_key_value&quot;],
    &quot;blip-2&quot;: [&quot;q&quot;, &quot;v&quot;, &quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;opt&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;gptj&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;gpt_neox&quot;: [&quot;query_key_value&quot;],
    &quot;gpt_neo&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;bert&quot;: [&quot;query&quot;, &quot;value&quot;],
    &quot;roberta&quot;: [&quot;query&quot;, &quot;value&quot;],
    &quot;xlm-roberta&quot;: [&quot;query&quot;, &quot;value&quot;],
    &quot;electra&quot;: [&quot;query&quot;, &quot;value&quot;],
    &quot;deberta-v2&quot;: [&quot;query_proj&quot;, &quot;value_proj&quot;],
    &quot;deberta&quot;: [&quot;in_proj&quot;],
    &quot;layoutlm&quot;: [&quot;query&quot;, &quot;value&quot;],
    &quot;llama&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;chatglm&quot;: [&quot;query_key_value&quot;],
    &quot;gpt_bigcode&quot;: [&quot;c_attn&quot;],
    &quot;mpt&quot;: [&quot;Wqkv&quot;],
    &quot;RefinedWebModel&quot;: [&quot;query_key_value&quot;],
    &quot;RefinedWeb&quot;: [&quot;query_key_value&quot;],
    &quot;falcon&quot;: [&quot;query_key_value&quot;],
    &quot;btlm&quot;: [&quot;c_proj&quot;, &quot;c_attn&quot;],
    &quot;codegen&quot;: [&quot;qkv_proj&quot;],
    &quot;mistral&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;mixtral&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;stablelm&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
    &quot;phi&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;, &quot;fc1&quot;, &quot;fc2&quot;],
    &quot;gemma&quot;: [&quot;q_proj&quot;, &quot;v_proj&quot;],
&#125;</code></pre>
<h3 id="linear">Linear</h3>
<p>â€‹
æˆ‘ä»¬åªéœ€è¦çŸ¥é“Linearæœ€ç»ˆè¢«é‡å†™äº†ï¼Œç»§æ‰¿äº†LoraLayerï¼Œç„¶åé€šè¿‡LoraLayerçš„<strong>update_layer</strong>æ–¹æ³•æ ¹æ®configçš„å‚æ•°è¿›è¡Œäº†åˆå§‹åŒ–ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class Linear(nn.Module, LoraLayer):
    # Lora implemented in a dense layer
    def __init__(
        self,
        base_layer,
        adapter_name: str,
        r: int &#x3D; 0,
        lora_alpha: int &#x3D; 1,
        lora_dropout: float &#x3D; 0.0,
        fan_in_fan_out: bool &#x3D; False,  # Set this to True if the layer to replace stores weight like (fan_in, fan_out)
        is_target_conv_1d_layer: bool &#x3D; False,
        init_lora_weights: Union[bool, str] &#x3D; True,
        use_rslora: bool &#x3D; False,
        use_dora: bool &#x3D; False,
        **kwargs,
    ) -&gt; None:
        super().__init__()
        LoraLayer.__init__(self, base_layer, **kwargs)
        self.fan_in_fan_out &#x3D; fan_in_fan_out

        self._active_adapter &#x3D; adapter_name
        self.update_layer(
            adapter_name,
            r,
            lora_alpha&#x3D;lora_alpha,
            lora_dropout&#x3D;lora_dropout,
            init_lora_weights&#x3D;init_lora_weights,
            use_rslora&#x3D;use_rslora,
            use_dora&#x3D;use_dora,
        )
        self.is_target_conv_1d_layer &#x3D; is_target_conv_1d_layer</code></pre>
<p>â€‹
æˆ‘ä»¬ç›´å¥”forwardæ–¹æ³•ã€‚å¯¹äºä¸åˆå¹¶çš„adapterï¼Œè‹¥å·²ç»åˆå¹¶ï¼ŒæŠŠå·²ç»åŠ å…¥åˆ°baselayerçš„æƒé‡ç§»å‡ºæ¥ã€‚å¯ä»¥åˆå¹¶çš„adapterè‹¥å·²ç»åˆå¹¶äº†å°±ç›´æ¥ç”¨è¿™ä¸ªæƒé‡ã€‚æœ€ç»ˆLoraä¸­çš„Linearå±‚çš„å‰å‘ä¼ æ’­ç»“æœæ˜¯ï¼š
<span class="math display">\[
X = XW + scale Ã— \frac{Î±}{r}(dropout(X))AB
\]</span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  def forward(self, x: torch.Tensor, *args: Any, **kwargs: Any) -&gt; torch.Tensor:
      self._check_forward_args(x, *args, **kwargs)
      adapter_names &#x3D; kwargs.pop(&quot;adapter_names&quot;, None)
# è‹¥ä¹‹å‰æŠŠè¯¥adapterè®¾ç½®ä¸ºä¸åˆå¹¶ä½†åˆå¹¶äº†ï¼Œé©¬ä¸ŠæŠŠåŠ è¿›å»çš„æƒé‡æ‹¿å‡ºæ¥
      if self.disable_adapters:
          if self.merged:
              self.unmerge()
          result &#x3D; self.base_layer(x, *args, **kwargs)
      elif adapter_names is not None:
          result &#x3D; self._mixed_batch_forward(x, *args, adapter_names&#x3D;adapter_names, **kwargs)
      # å·²ç»åˆå¹¶äº†å°±ç›´æ¥è¿”å›
      elif self.merged:
          result &#x3D; self.base_layer(x, *args, **kwargs)
      else:
          result &#x3D; self.base_layer(x, *args, **kwargs)
          torch_result_dtype &#x3D; result.dtype
          for active_adapter in self.active_adapters:
              if active_adapter not in self.lora_A.keys():
                  continue
              lora_A &#x3D; self.lora_A[active_adapter]
              lora_B &#x3D; self.lora_B[active_adapter]
              dropout &#x3D; self.lora_dropout[active_adapter]
              # self.scaling[adapter] &#x3D; scale * self.lora_alpha[adapter] &#x2F; self.r[adapter]
              scaling &#x3D; self.scaling[active_adapter]
              x &#x3D; x.to(lora_A.weight.dtype)

              if not self.use_dora[active_adapter]:
                  result &#x3D; result + lora_B(lora_A(dropout(x))) * scaling
              else:
                  x &#x3D; dropout(x)
                  result &#x3D; result + self._apply_dora(x, lora_A, lora_B, scaling, active_adapter)

          result &#x3D; result.to(torch_result_dtype)

      return result</code></pre>
<p>â€‹
å½“ç„¶è¿˜æœ‰å°‘ä¸äº†çš„mergeå’Œunmergeå‡½æ•°ï¼Œæœ€ç»ˆå¾®è°ƒå®Œæ¯•çš„æƒé‡æ˜¯å¯ä»¥é€‰æ‹©åŠ å›åŸæƒé‡æ¥æ¶ˆé™¤é¢å¤–çš„æ¨ç†å»¶è¿Ÿã€‚</p>
<p>â€‹ å…ˆçœ‹unmergeæ–¹æ³•ï¼Œå…¶å®å°±æ˜¯æŠŠåŠ åˆ°baselayerä¸Šçš„æƒé‡å‡å›å»ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def unmerge(self) -&gt; None:
    &quot;&quot;&quot;
    This method unmerges all merged adapter layers from the base weights.
    &quot;&quot;&quot;
    if not self.merged:
        warnings.warn(&quot;Already unmerged. Nothing to do.&quot;)
        return
    while len(self.merged_adapters) &gt; 0:
        active_adapter &#x3D; self.merged_adapters.pop()
        if active_adapter in self.lora_A.keys():
            weight &#x3D; self.get_base_layer().weight
            delta_weight &#x3D; self.get_delta_weight(active_adapter)
            if not self.use_dora[active_adapter]:
                # åªçœ‹è¿™ä¸€å¥å°±è¡Œäº†
                weight.data -&#x3D; delta_weight
            else:
                weight_norm &#x3D; self._cache_pop(f&quot;&#123;active_adapter&#125;-weight_norm&quot;)
                dora_factor &#x3D; self.lora_magnitude_vector[active_adapter] &#x2F; weight_norm
                weight_orig &#x3D; weight.data &#x2F; dora_factor.view(-1, 1) - delta_weight
                weight.data &#x3D; weight_orig</code></pre>
<p>â€‹
å†çœ‹mergeæ“ä½œï¼Œé¦–å…ˆéœ€è¦æ‹¿åˆ°<strong>delta_weight</strong>ï¼Œå…·ä½“çš„å†…å®¹å°±ä¸å¤åˆ¶äº†ï¼Œå¦‚æœæ˜¯fp16çš„æƒé‡åœ¨cpuä¸Šè®¡ç®—ï¼Œé‚£ä¹ˆç”±äºcpuåŸç”Ÿä¸æ”¯æŒè¯¥ç±»å‹ï¼Œæ‰€ä»¥éœ€è¦è½¬æ¢æˆfp32å†è½¬æ¢å›å»ã€‚æœ€ç»ˆçš„<span
class="math inline">\(W =
W+scalingÃ—BA\)</span>ï¼Œå¯ä»¥çœ‹åˆ°ä»å¤´åˆ°å°¾æ˜¯æ²¡æœ‰åŸæ¥çš„Wå‚ä¸å‰å‘ä¼ æ’­çš„ï¼ŒåŸæ¥çš„Wç›´æ¥è¢«å†»ç»“ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def merge(self, safe_merge: bool &#x3D; False, adapter_names: Optional[list[str]] &#x3D; None) -&gt; None:
    adapter_names &#x3D; check_adapters_to_merge(self, adapter_names)
    if not adapter_names:
        # no adapter to merge
        return

    for active_adapter in adapter_names:
        if active_adapter in self.lora_A.keys():
            base_layer &#x3D; self.get_base_layer()
            if safe_merge:
                # Note that safe_merge will be slower than the normal merge
                # because of the copy operation.
	&quot;&quot;&quot;
	safe_mergeå…¶å®å°±æ˜¯æŸ¥çœ‹æ˜¯å¦æœ‰NaNå€¼ï¼Œæœ‰çš„è¯æŠ›å‡ºå¼‚å¸¸ã€‚
	å¹¶ä¸”ç”±äºå¤šå¤åˆ¶äº†ä¸€æ¬¡åŸæƒé‡ï¼Œæ‰€ä»¥æ•ˆç‡ä¼šæ›´ä½ï¼Œä»£ç ç•¥
	&quot;&quot;&quot;
            else:
                # è¯¥æ“ä½œæ ¸å¿ƒå°±ä¸€å¥ï¼Œè¿”å›scaling B@A
                # output_tensor &#x3D; transpose(weight_B @ weight_A, self.fan_in_fan_out) * self.scaling[adapter]
                delta_weight &#x3D; self.get_delta_weight(active_adapter)
                if not self.use_dora[active_adapter]:
                    base_layer.weight.data &#x3D; base_layer.weight.data + delta_weight
                else:
                    # handle doraï¼Œæ­¤å¤„ç•¥
            self.merged_adapters.append(active_adapter)</code></pre>
<p>â€‹
å‰©ä¸‹çš„Embeddingï¼Œä¸€ç»´å·ç§¯å’ŒäºŒç»´å·ç§¯æ“ä½œå·®ä¸å¤šï¼Œè¿™é‡Œå°±ä¸å¤šèµ˜è¿°äº†ã€‚</p>
<h3 id="åˆå¹¶æ“ä½œ">åˆå¹¶æ“ä½œ</h3>
<p>â€‹
é¦–å…ˆéœ€è¦åˆ¤æ–­æ˜¯å¦èƒ½å¤Ÿåˆå¹¶ã€‚æœ‰ä¸¤ç§æƒ…å†µä¸‹ä¸èƒ½åˆå¹¶ï¼Œä¸€ç§æ˜¯å½“å‰ä½¿ç”¨gptqé‡åŒ–æ¨¡å‹(QLoRAå¯ä»¥)ï¼Œå¦ä¸€ç§åˆ™æ˜¯å¼€å¯äº†peftconfigä¸­çš„layer_replicationæ“ä½œã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def _check_merge_allowed(self):
    &quot;&quot;&quot;Verify that the configuration supports merging.

    Currently gptq quantization and replicated layers do not support merging.
    &quot;&quot;&quot;
    if getattr(self.model, &quot;quantization_method&quot;, None) &#x3D;&#x3D; &quot;gptq&quot;:
        raise ValueError(&quot;Cannot merge LORA layers when the model is gptq quantized&quot;)
    if self.peft_config.get(&quot;layer_replication&quot;):
        raise ValueError(&quot;Cannot merge LORA layers when base model layers are replicated&quot;)</code></pre>
<p>â€‹
æœ€ç»ˆè°ƒç”¨çš„æ–¹æ³•æ˜¯<strong>merge_and_unload</strong>æ–¹æ³•ï¼Œ<code>progressbar</code>å°±æ˜¯æ˜¯å¦ç”¨tqdmæ˜¾ç¤ºè¿›è¡Œï¼Œ<code>safe_merge</code>ä¼šæ£€æŸ¥tensorä¸­æ˜¯å¦æœ‰NaNï¼Œ<code>adapter_names</code>ç”¨äºæŒ‡å®šå“ªäº›å±‚è¦åˆå¹¶ï¼Œé»˜è®¤æ˜¯æ‰€æœ‰éƒ½åˆå¹¶ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  def merge_and_unload(
      self, progressbar: bool &#x3D; False, safe_merge: bool &#x3D; False, adapter_names: Optional[list[str]] &#x3D; None
  ) -&gt; torch.nn.Module:
# progressbarå°±æ˜¯æ˜¯å¦ç”¨tqdmæ˜¾ç¤ºè¿›è¡Œ
      return self._unload_and_optionally_merge(
          progressbar&#x3D;progressbar, safe_merge&#x3D;safe_merge, adapter_names&#x3D;adapter_names
      )</code></pre>
<p>â€‹
ä¸‹é¢æ˜¯æ ¸å¿ƒæ–¹æ³•**_unload_and_optionally_merge**ã€‚é¦–å…ˆä¼šåˆ¤æ–­æ˜¯å¦æ»¡è¶³è¦æ±‚ï¼Œä¹Ÿå°±æ˜¯ä¸ä½¿ç”¨gptqä¸”ä¸é‡‡å–replicatedç­–ç•¥çš„æƒ…å†µä¸‹æ‰èƒ½åˆå¹¶ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def _unload_and_optionally_merge(
    self,
    merge&#x3D;True,
    progressbar: bool &#x3D; False,
    safe_merge: bool &#x3D; False,
    adapter_names: Optional[list[str]] &#x3D; None,
):
    if merge:
        self._check_merge_allowed()</code></pre>
<p>â€‹
ç„¶åæ‹¿åˆ°targetlayerè¿›è¡Œåˆå¹¶æ“ä½œåï¼Œæ›¿æ¢æ‰åŸæ¥çš„moduleã€‚è¿™é‡Œçš„target.mergeæ˜¯LoraLayerçš„å­ç±»éƒ½å®ç°çš„mergeæ“ä½œï¼Œä¸Šé¢å·²ç»ç»™å‡ºã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if hasattr(target, &quot;base_layer&quot;):
    if merge:
        target.merge(safe_merge&#x3D;safe_merge, adapter_names&#x3D;adapter_names)
    self._replace_module(parent, target_name, target.get_base_layer(), target)
</code></pre>
<h1 id="å‚è€ƒèµ„æ–™">å‚è€ƒèµ„æ–™</h1>
<p>[1] <a target="_blank" rel="noopener" href='https://arxiv.org/abs/2303.15647'>Scaling Down to Scale
Up: A Guide to Parameter-Efficient Fine-Tuning</a></p>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>æœ¬æ–‡ä½œè€…ï¼š</strong>iroha</li><li class="post-copyright-link"><strong>æœ¬æ–‡é“¾æ¥ï¼š</strong><a href="http://example.com/post/ft_survey.html" title="å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ç»¼è¿°">http://example.com/post/ft_survey.html</a></li><li class="post-copyright-license"><strong>ç‰ˆæƒå£°æ˜ï¼š</strong>æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é»˜è®¤é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> è®¸å¯åè®®ã€‚</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/post/model_load.html" rel="prev" title="Huggingfaceçš„æ¨¡å‹åŠ è½½æµç¨‹"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">Huggingfaceçš„æ¨¡å‹åŠ è½½æµç¨‹</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/post/backtrace.html" rel="next" title="å›æº¯ä¹‹ç»„åˆé—®é¢˜"><span class="post-nav-text">å›æº¯ä¹‹ç»„åˆé—®é¢˜</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 â€“ 2024 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> iroha</span></div><div class="powered"><span>ç”± <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> é©±åŠ¨ v6.3.0</span><span class="footer-separator">|</span><span>ä¸»é¢˜ - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.9</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>