<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="iroha"><meta name="copyright" content="iroha"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>大模型微调方法综述 | iroha</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...undefined?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"example.com","root":"/","title":"不想摆烂","version":"1.10.11","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js" defer></script><script src="/js/load-aplayer.js" defer></script><meta name="description" content="背景 ​ 随着模型的参数越来越大，为每一个任务从头训练一个模型越来越困难，而参数量的增大也让模型理解了更深层的语义信息。随着BERT的出现，预训练+微调的方式走上历史舞台。 ​ 为什么不选择全参数微调呢？最主要的原因还是成本太高，在消费级硬件上愈发困难。以GPT2-1.5B为例，若参数与梯度以fp16保存，显存需要保存3+3 &#x3D; 6B的数据，若使用Adam优化器，那么还需要保存fp32">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型微调方法综述">
<meta property="og:url" content="http://example.com/post/ft_survey.html">
<meta property="og:site_name" content="iroha">
<meta property="og:description" content="背景 ​ 随着模型的参数越来越大，为每一个任务从头训练一个模型越来越困难，而参数量的增大也让模型理解了更深层的语义信息。随着BERT的出现，预训练+微调的方式走上历史舞台。 ​ 为什么不选择全参数微调呢？最主要的原因还是成本太高，在消费级硬件上愈发困难。以GPT2-1.5B为例，若参数与梯度以fp16保存，显存需要保存3+3 &#x3D; 6B的数据，若使用Adam优化器，那么还需要保存fp32">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20240408231535224.png">
<meta property="og:image" content="http://example.com/images/image-20240410215838916.png">
<meta property="og:image" content="http://example.com/images/image-20240411005445372.png">
<meta property="article:published_time" content="2024-04-08T04:00:00.000Z">
<meta property="article:modified_time" content="2024-04-12T19:32:20.406Z">
<meta property="article:author" content="iroha">
<meta property="article:tag" content="大模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20240408231535224.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="iroha"><img width="96" loading="lazy" src="/yun.png" alt="iroha"><span class="site-author-status" title="永远相信美好的事情即将发生">😊</span></a><div class="site-author-name"><a href="/about/">iroha</a></div><span class="site-name">iroha</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">15</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">5</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">2</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="友链" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#freeze"><span class="toc-number">2.</span> <span class="toc-text">Freeze</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#bitfit"><span class="toc-number">3.</span> <span class="toc-text">Bitfit</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#prompt-tuning"><span class="toc-number">4.</span> <span class="toc-text">Prompt-tuning</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#lora"><span class="toc-number">5.</span> <span class="toc-text">Lora</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-number">5.1.</span> <span class="toc-text">原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E9%80%9F%E8%AF%BB"><span class="toc-number">5.2.</span> <span class="toc-text">源码速读</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#get_peft_model"><span class="toc-number">5.2.1.</span> <span class="toc-text">get_peft_model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#peftmodel"><span class="toc-number">5.2.2.</span> <span class="toc-text">PeftModel</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#loramodel"><span class="toc-number">5.2.3.</span> <span class="toc-text">LoraModel</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#loralayer"><span class="toc-number">5.2.4.</span> <span class="toc-text">LoraLayer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#linear"><span class="toc-number">5.2.5.</span> <span class="toc-text">Linear</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%88%E5%B9%B6%E6%93%8D%E4%BD%9C"><span class="toc-number">5.2.6.</span> <span class="toc-text">合并操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">6.</span> <span class="toc-text">参考资料</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://example.com/post/ft_survey.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="iroha"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="iroha"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">大模型微调方法综述</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2024-04-08 12:00:00" itemprop="dateCreated datePublished" datetime="2024-04-08T12:00:00+08:00">2024-04-08</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="修改时间：2024-04-13 03:32:20" itemprop="dateModified" datetime="2024-04-13T03:32:20+08:00">2024-04-13</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">NLP</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">大模型</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><h1 id="背景">背景</h1>
<p>​
随着模型的参数越来越大，为每一个任务从头训练一个模型越来越困难，而参数量的增大也让模型理解了更深层的语义信息。随着BERT的出现，预训练+微调的方式走上历史舞台。</p>
<p>​
为什么不选择全参数微调呢？最主要的原因还是成本太高，在消费级硬件上愈发困难。以GPT2-1.5B为例，若参数与梯度以fp16保存，显存需要保存3+3
=
6B的数据，若使用Adam优化器，那么还需要保存fp32的模型参数备份，momentum与variance，又需要6+6+6
=
18G，总共需要24G。再加上其他状态，如activation，buffer，还有显存碎片无法利用，实际上需要的显存是大于24G的。</p>
<p>​
全参数微调还有一个问题，当LLM尝试学习多个连续任务时，容易忘记之前学到的东西，也就是“灾难性遗忘”。如何在保留先前知识的基础上增量地增强LLM，即进行持续学习，至关重要。简单来说，全量微调有overfitting，灾难性遗忘，成本高的问题。因此，高效微调模型的参数就成了一个新方向，PEFT(Parameter-Efficient
Fine-tuning)应运而生。</p>
<p>​
微调一般只更改模型的部分参数，调整哪些参数，如何调整则对应了不同方法。一般从下游任务出发，来决定下游应该如何添加参数。</p>
<p>​
下面我们以transformers库中的BERT为例。我们都知道，BERT的预训练是MLM与NSP，所以下游任务自然有这两者，也就是<strong>下一句预测</strong>和<strong>完形填空</strong>。所以这才应该是BERT的原生下游任务。</p>
<p>​ 首先看NSP，模型的结构如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># assert logits[0, 0] &lt; logits[0, 1]  # next sentence was random</span>
<span class="token keyword">class</span> <span class="token class-name">BertForNextSentencePrediction</span><span class="token punctuation">(</span>BertPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>bert <span class="token operator">=</span> BertModel<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>cls <span class="token operator">=</span> BertOnlyNSPHead<span class="token punctuation">(</span>config<span class="token punctuation">)</span>

        <span class="token comment"># Initialize weights and apply final processing</span>
        self<span class="token punctuation">.</span>post_init<span class="token punctuation">(</span><span class="token punctuation">)</span>

        
<span class="token keyword">class</span> <span class="token class-name">BertOnlyNSPHead</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>seq_relationship <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pooled_output<span class="token punctuation">)</span><span class="token punctuation">:</span>
        seq_relationship_score <span class="token operator">=</span> self<span class="token punctuation">.</span>seq_relationship<span class="token punctuation">(</span>pooled_output<span class="token punctuation">)</span>
        <span class="token keyword">return</span> seq_relationship_score<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​
可以看到，对于NSP任务，就是在Model的后面拼一个Linear层，将768维映射到2维，本质上是一个二分类问题。</p>
<p>​
再看SequenceClassification任务，也就是序列分类。实际上也是在最后填充了一层Linear层用于分类。与NSP的区别在于pooler层会先经过一次dropout，dropout的概率可以在config中设置，默认为0.1。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">BertForSequenceClassification</span><span class="token punctuation">(</span>BertPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_labels <span class="token operator">=</span> config<span class="token punctuation">.</span>num_labels
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config

        self<span class="token punctuation">.</span>bert <span class="token operator">=</span> BertModel<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        classifier_dropout <span class="token operator">=</span> <span class="token punctuation">(</span>
            config<span class="token punctuation">.</span>classifier_dropout <span class="token keyword">if</span> config<span class="token punctuation">.</span>classifier_dropout <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> config<span class="token punctuation">.</span>hidden_dropout_prob
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>classifier_dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>classifier <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>num_labels<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​ <img src="../images/image-20240408231535224.png"
alt="image-20240408231535224" /></p>
<p>​ PEFT按核心思想可以分为三类：</p>
<ol type="1">
<li>添加一些参数量小的层，只微调这些层，最典型的就是LST。</li>
<li>选择某些层，或层中的某一部分进行微调，最典型的是BitFit，只对bias进行微调。</li>
<li>重参数化，也算是增加一部分参数，但最后加回原参数的对应部分</li>
</ol>
<h1 id="freeze">Freeze</h1>
<p>​
冻结某些参数是最容易想到的方法，仅仅调整某些未被冻结的参数就可以减少大量显存占用，但freeze方法大大降低了模型的灵活性。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> name <span class="token punctuation">,</span>param <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">any</span> <span class="token punctuation">(</span>n <span class="token keyword">in</span> name <span class="token keyword">for</span> n <span class="token keyword">in</span> <span class="token punctuation">[</span>layers_name1<span class="token punctuation">,</span>layer_name2<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
		param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h1 id="bitfit">Bitfit</h1>
<p>出自<strong>BitFit: Simple Parameter-efficient Fine-tuning or
Transformer-based Masked Language-models</strong>。</p>
<p>pass</p>
<h1 id="prompt-tuning">Prompt-tuning</h1>
<p>pass</p>
<h1 id="lora">Lora</h1>
<h2 id="原理">原理</h2>
<figure>
<img src="../images/image-20240410215838916.png"
alt="image-20240410215838916" />
<figcaption aria-hidden="true">image-20240410215838916</figcaption>
</figure>
<p>​ 在原始权重中并上一个旁路的分支，以Linear层为例子，原本<span
class="math inline">\(h = W_{d×k}X\)</span>，<span
class="math inline">\(W\)</span>是权重，旁路有两个低秩的矩阵，其中<span
class="math inline">\(A_{r×k} =
N(0,σ^2)\)</span>，也就是以高斯分布初始化，而<span
class="math inline">\(B_{d×r}\)</span>矩阵则以全0初始化，其中维度r远小于d和k。<span
class="math inline">\(ΔW = BA\)</span> ,最终的权重为$W+ΔW <span
class="math inline">\(。对于\)</span>ΔW<span
class="math inline">\(可以使用一个α参数来控制倍数，即\)</span>W+ΔW$。B为升维矩阵，A为降维矩阵。实际上，LoRA一般用于Dense层。</p>
<p>​ 对于梯度计算，借用知乎上CodeLearner答主的一张图。</p>
<p>​ <img src="../images/image-20240411005445372.png"
alt="image-20240411005445372" /></p>
<p>​
所以，在微调时，某一Lora层反向传播中的梯度计算量是要更多的，但由于r远小于原权重的维度d和k，所以相对于全量微调保存的梯度值就少。同时，原论文中也只对Transformer
Block中的Q,K,V进行了Lora处理。若r等于k，那么此时可以等价于全参数微调。</p>
<p>​ 对于初始化问题，首先，我们需要保证最开始的<span
class="math inline">\(ΔW=0\)</span>，所以需要AB的其中一者为0。其次，我们看上图带上LoRA后的反向传播梯度计算，若A为0，那么梯度就会一直为0，<span
class="math inline">\(ΔW\)</span>就不会更新。</p>
<p>​
原文中提到了LoRA的Limitation，如果选择将A和B矩阵吸收(合并)到W原始权重矩阵中,以消除额外的推理延迟,那么在单个前向传递中为不同任务的不同A和B批量输入数据就变得不直观了。我想原文的意思是一个batch里可能有不同的任务，那么不同的任务应该用不同的权重，那么最好是不将<span
class="math inline">\(ΔW\)</span>合并到原始权重，针对不同任务来动态选择<span
class="math inline">\(ΔW\)</span>，这需要在推理速度上做取舍。</p>
<h2 id="源码速读">源码速读</h2>
<p>​ 为了方便理解，我们从peft0.10.0的官方示例出发。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForSeq2SeqLM
<span class="token keyword">from</span> peft <span class="token keyword">import</span> get_peft_config<span class="token punctuation">,</span> get_peft_model<span class="token punctuation">,</span> LoraConfig<span class="token punctuation">,</span> TaskType
model_name_or_path <span class="token operator">=</span> <span class="token string">"bigscience/mt0-large"</span>
tokenizer_name_or_path <span class="token operator">=</span> <span class="token string">"bigscience/mt0-large"</span>

peft_config <span class="token operator">=</span> LoraConfig<span class="token punctuation">(</span>
    task_type<span class="token operator">=</span>TaskType<span class="token punctuation">.</span>SEQ_2_SEQ_LM<span class="token punctuation">,</span> inference_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> r<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> lora_alpha<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> lora_dropout<span class="token operator">=</span><span class="token number">0.1</span>
<span class="token punctuation">)</span>

model <span class="token operator">=</span> AutoModelForSeq2SeqLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name_or_path<span class="token punctuation">)</span>
<span class="token comment">#等价于 model = PeftModelForSeq2SeqLM(model,peft_config)</span>
<span class="token comment">#也等价于model = lora_model = LoraModel(model, config, "default")</span>
model <span class="token operator">=</span> get_peft_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">)</span> 

model<span class="token punctuation">.</span>print_trainable_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token string">"trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​
首先从LoraConfig出发，这里的task_type是PeftConfig的参数，其余则是LoraConfig的参数，这些参数在上文的原理中都有提到。主要还是根据不同的下游任务返回不同的模型。这里也可以直接用get_peft_config函数来读取。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">peft_config <span class="token operator">=</span> LoraConfig<span class="token punctuation">(</span>
    task_type<span class="token operator">=</span>TaskType<span class="token punctuation">.</span>SEQ_2_SEQ_LM<span class="token punctuation">,</span> inference_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> r<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> lora_alpha<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> lora_dropout<span class="token operator">=</span><span class="token number">0.1</span>
<span class="token punctuation">)</span>
<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
config <span class="token operator">=</span> <span class="token punctuation">&#123;</span>
    <span class="token string">"task_type"</span><span class="token punctuation">:</span><span class="token string">"SEQ_2_SEQ_LM"</span><span class="token punctuation">,</span>
    <span class="token string">"peft_type"</span><span class="token punctuation">:</span><span class="token string">"LORA"</span><span class="token punctuation">,</span>
    <span class="token string">"inference_mode"</span><span class="token punctuation">:</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    <span class="token string">"r"</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">,</span>
    <span class="token string">"lora_alpha"</span><span class="token punctuation">:</span><span class="token number">32</span><span class="token punctuation">,</span>
    <span class="token string">"lora_dropout"</span><span class="token punctuation">:</span><span class="token number">0.1</span>
<span class="token punctuation">&#125;</span>

another_config <span class="token operator">=</span> get_peft_config<span class="token punctuation">(</span>config<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="get_peft_model">get_peft_model</h3>
<p>​
接下来看get_peft_model。传入的参数有模型和对应的config，最后会返回一个PeftModel实例。返回的实例类型会根据传入的config来确定。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> get_peft_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">)</span>
<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>mapping<span class="token punctuation">.</span>py<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
<span class="token keyword">def</span> <span class="token function">get_peft_model</span><span class="token punctuation">(</span>
    model<span class="token punctuation">:</span> PreTrainedModel<span class="token punctuation">,</span> peft_config<span class="token punctuation">:</span> PeftConfig<span class="token punctuation">,</span> adapter_name<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">"default"</span><span class="token punctuation">,</span> mixed<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span>
<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> PeftModel <span class="token operator">|</span> PeftMixedModel<span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Returns a Peft model object from a model and a config.

    Args:
        model ([`transformers.PreTrainedModel`]):
            Model to be wrapped.
        peft_config ([`PeftConfig`]):
            Configuration object containing the parameters of the Peft model.
        adapter_name (`str`, `optional`, defaults to `"default"`):
            The name of the adapter to be injected, if not provided, the default adapter name is used ("default").
        mixed (`bool`, `optional`, defaults to `False`):
            Whether to allow mixing different (compatible) adapter types.
    """</span>
    model_config <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token string">"config"</span><span class="token punctuation">,</span> <span class="token punctuation">&#123;</span><span class="token string">"model_type"</span><span class="token punctuation">:</span> <span class="token string">"custom"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>model_config<span class="token punctuation">,</span> <span class="token string">"to_dict"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        model_config <span class="token operator">=</span> model_config<span class="token punctuation">.</span>to_dict<span class="token punctuation">(</span><span class="token punctuation">)</span>

    peft_config<span class="token punctuation">.</span>base_model_name_or_path <span class="token operator">=</span> model<span class="token punctuation">.</span>__dict__<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"name_or_path"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> mixed<span class="token punctuation">:</span>
        <span class="token keyword">return</span> PeftMixedModel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">,</span> adapter_name<span class="token operator">=</span>adapter_name<span class="token punctuation">)</span>

    <span class="token keyword">if</span> peft_config<span class="token punctuation">.</span>task_type <span class="token keyword">not</span> <span class="token keyword">in</span> MODEL_TYPE_TO_PEFT_MODEL_MAPPING<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token keyword">not</span> peft_config<span class="token punctuation">.</span>is_prompt_learning<span class="token punctuation">:</span>
        <span class="token keyword">return</span> PeftModel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">,</span> adapter_name<span class="token operator">=</span>adapter_name<span class="token punctuation">)</span>

    <span class="token keyword">if</span> peft_config<span class="token punctuation">.</span>is_prompt_learning<span class="token punctuation">:</span>
        peft_config <span class="token operator">=</span> _prepare_prompt_learning_config<span class="token punctuation">(</span>peft_config<span class="token punctuation">,</span> model_config<span class="token punctuation">)</span>
    <span class="token keyword">return</span> MODEL_TYPE_TO_PEFT_MODEL_MAPPING<span class="token punctuation">[</span>peft_config<span class="token punctuation">.</span>task_type<span class="token punctuation">]</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">,</span> adapter_name<span class="token operator">=</span>adapter_name<span class="token punctuation">)</span>
<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
MODEL_TYPE_TO_PEFT_MODEL_MAPPING<span class="token punctuation">:</span> <span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> PeftModel<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span>
    <span class="token string">"SEQ_CLS"</span><span class="token punctuation">:</span> PeftModelForSequenceClassification<span class="token punctuation">,</span>
    <span class="token string">"SEQ_2_SEQ_LM"</span><span class="token punctuation">:</span> PeftModelForSeq2SeqLM<span class="token punctuation">,</span>
    <span class="token string">"CAUSAL_LM"</span><span class="token punctuation">:</span> PeftModelForCausalLM<span class="token punctuation">,</span>
    <span class="token string">"TOKEN_CLS"</span><span class="token punctuation">:</span> PeftModelForTokenClassification<span class="token punctuation">,</span>
    <span class="token string">"QUESTION_ANS"</span><span class="token punctuation">:</span> PeftModelForQuestionAnswering<span class="token punctuation">,</span>
    <span class="token string">"FEATURE_EXTRACTION"</span><span class="token punctuation">:</span> PeftModelForFeatureExtraction<span class="token punctuation">,</span>
<span class="token punctuation">&#125;</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​
关键在于这两句。如果任务类型不在支持的特定任务中，返回PeftModel，否则返回对应任务类型的Model，这些Model继承了PeftModel。若是提示学习类型的，如prompt-tuning，则需要额外的config信息，如隐藏层的数量,可能的键包括<code>num_hidden_layers</code>、<code>num_layers</code>、<code>n_layer</code>，
如果无法找到,则需要在peft_config中手动指定num_layers，这个参数指定了prompt将被注入到模型的哪些层。还需要注意力的头数，encoder隐藏层的大小和tokenembedding的维度等等。这些参数若从模型的config中找不到，则需要在peftconfig中自行指定。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">    <span class="token keyword">if</span> peft_config<span class="token punctuation">.</span>task_type <span class="token keyword">not</span> <span class="token keyword">in</span> MODEL_TYPE_TO_PEFT_MODEL_MAPPING<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token keyword">not</span> peft_config<span class="token punctuation">.</span>is_prompt_learning<span class="token punctuation">:</span>
        <span class="token keyword">return</span> PeftModel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">,</span> adapter_name<span class="token operator">=</span>adapter_name<span class="token punctuation">)</span>

    <span class="token keyword">if</span> peft_config<span class="token punctuation">.</span>is_prompt_learning<span class="token punctuation">:</span>
        peft_config <span class="token operator">=</span> _prepare_prompt_learning_config<span class="token punctuation">(</span>peft_config<span class="token punctuation">,</span> model_config<span class="token punctuation">)</span>
    <span class="token keyword">return</span> MODEL_TYPE_TO_PEFT_MODEL_MAPPING<span class="token punctuation">[</span>peft_config<span class="token punctuation">.</span>task_type<span class="token punctuation">]</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">,</span> adapter_name<span class="token operator">=</span>adapter_name<span class="token punctuation">)</span>
<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
<span class="token keyword">def</span> <span class="token function">_prepare_prompt_learning_config</span><span class="token punctuation">(</span>peft_config<span class="token punctuation">,</span> model_config<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> peft_config<span class="token punctuation">.</span>num_layers <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">"num_hidden_layers"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            num_layers <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"num_hidden_layers"</span><span class="token punctuation">]</span>
        <span class="token keyword">elif</span> <span class="token string">"num_layers"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            num_layers <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"num_layers"</span><span class="token punctuation">]</span>
        <span class="token keyword">elif</span> <span class="token string">"n_layer"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            num_layers <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"n_layer"</span><span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Please specify `num_layers` in `peft_config`"</span><span class="token punctuation">)</span>
        peft_config<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> num_layers

    <span class="token keyword">if</span> peft_config<span class="token punctuation">.</span>token_dim <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">"hidden_size"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            token_dim <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"hidden_size"</span><span class="token punctuation">]</span>
        <span class="token keyword">elif</span> <span class="token string">"n_embd"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            token_dim <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"n_embd"</span><span class="token punctuation">]</span>
        <span class="token keyword">elif</span> <span class="token string">"d_model"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            token_dim <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"d_model"</span><span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Please specify `token_dim` in `peft_config`"</span><span class="token punctuation">)</span>
        peft_config<span class="token punctuation">.</span>token_dim <span class="token operator">=</span> token_dim

    <span class="token keyword">if</span> peft_config<span class="token punctuation">.</span>num_attention_heads <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">"num_attention_heads"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            num_attention_heads <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"num_attention_heads"</span><span class="token punctuation">]</span>
        <span class="token keyword">elif</span> <span class="token string">"n_head"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            num_attention_heads <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"n_head"</span><span class="token punctuation">]</span>
        <span class="token keyword">elif</span> <span class="token string">"num_heads"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            num_attention_heads <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"num_heads"</span><span class="token punctuation">]</span>
        <span class="token keyword">elif</span> <span class="token string">"encoder_attention_heads"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            num_attention_heads <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"encoder_attention_heads"</span><span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Please specify `num_attention_heads` in `peft_config`"</span><span class="token punctuation">)</span>
        peft_config<span class="token punctuation">.</span>num_attention_heads <span class="token operator">=</span> num_attention_heads

    <span class="token keyword">if</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>peft_config<span class="token punctuation">,</span> <span class="token string">"encoder_hidden_size"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token builtin">setattr</span><span class="token punctuation">(</span>peft_config<span class="token punctuation">,</span> <span class="token string">"encoder_hidden_size"</span><span class="token punctuation">,</span> peft_config<span class="token punctuation">.</span>token_dim<span class="token punctuation">)</span>

    <span class="token keyword">return</span> peft_config<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​
回到重点，我们以Seq2SeqModel的代码为例，相对于PeftModel，整体结构一样，只是多了两个参数变量，其中prepare_inputs_for_generation函数需要生成模型在generate方法中自行实现，base_model_prepare_encoder_decoder_kwargs_for_generation变量则从base_model中提取出一些生成时的参数，如是否要使用cache的use_cahe，将encoder的参数封装进model_kwargs返回。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PeftModelForSeq2SeqLM</span><span class="token punctuation">(</span>PeftModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">,</span> peft_config<span class="token punctuation">:</span> PeftConfig<span class="token punctuation">,</span> adapter_name<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">"default"</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">,</span> adapter_name<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>base_model_prepare_inputs_for_generation <span class="token operator">=</span> self<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>prepare_inputs_for_generation
        self<span class="token punctuation">.</span>base_model_prepare_encoder_decoder_kwargs_for_generation <span class="token operator">=</span> <span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>_prepare_encoder_decoder_kwargs_for_generation
        <span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="peftmodel">PeftModel</h3>
<p>​ 还是先贴出整体的初始化源码。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PeftModel</span><span class="token punctuation">(</span>PushToHubMixin<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">:</span> PreTrainedModel<span class="token punctuation">,</span> peft_config<span class="token punctuation">:</span> PeftConfig<span class="token punctuation">,</span> adapter_name<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">"default"</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>modules_to_save <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>active_adapter <span class="token operator">=</span> adapter_name
        self<span class="token punctuation">.</span>peft_type <span class="token operator">=</span> peft_config<span class="token punctuation">.</span>peft_type
        <span class="token comment"># These args are special PEFT arguments that users can pass. They need to be removed before passing them to</span>
        <span class="token comment"># forward.</span>
        self<span class="token punctuation">.</span>special_peft_forward_args <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">"adapter_names"</span><span class="token punctuation">&#125;</span>

        self<span class="token punctuation">.</span>_is_prompt_learning <span class="token operator">=</span> peft_config<span class="token punctuation">.</span>is_prompt_learning
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>_is_prompt_learning<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>_peft_config <span class="token operator">=</span> <span class="token punctuation">&#123;</span>adapter_name<span class="token punctuation">:</span> peft_config<span class="token punctuation">&#125;</span>
            self<span class="token punctuation">.</span>base_model <span class="token operator">=</span> model
            self<span class="token punctuation">.</span>add_adapter<span class="token punctuation">(</span>adapter_name<span class="token punctuation">,</span> peft_config<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>_peft_config <span class="token operator">=</span> <span class="token boolean">None</span>
            cls <span class="token operator">=</span> PEFT_TYPE_TO_MODEL_MAPPING<span class="token punctuation">[</span>peft_config<span class="token punctuation">.</span>peft_type<span class="token punctuation">]</span>
            self<span class="token punctuation">.</span>base_model <span class="token operator">=</span> cls<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token punctuation">&#123;</span>adapter_name<span class="token punctuation">:</span> peft_config<span class="token punctuation">&#125;</span><span class="token punctuation">,</span> adapter_name<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>set_additional_trainable_modules<span class="token punctuation">(</span>peft_config<span class="token punctuation">,</span> adapter_name<span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token string">"is_gradient_checkpointing"</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            model <span class="token operator">=</span> self<span class="token punctuation">.</span>_prepare_model_for_gradient_checkpointing<span class="token punctuation">(</span>model<span class="token punctuation">)</span>

        <span class="token comment"># the `pretraining_tp` is set for some models to simulate Tensor Parallelism during inference to avoid</span>
        <span class="token comment"># numerical differences, https://github.com/pytorch/pytorch/issues/76232 - to avoid any unexpected</span>
        <span class="token comment"># behavior we disable that in this line.</span>
        <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>base_model<span class="token punctuation">,</span> <span class="token string">"config"</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>config<span class="token punctuation">,</span> <span class="token string">"pretraining_tp"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp <span class="token operator">=</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​
PeftModel中的源码很大部分服务于提示学习，先不看。我们只看_is_prompt_learning是False的情况。</p>
<pre class="line-numbers language-none"><code class="language-none">else:
    self._peft_config &#x3D; None
    cls &#x3D; PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type]
    self.base_model &#x3D; cls(model, &#123;adapter_name: peft_config&#125;, adapter_name)
    self.set_additional_trainable_modules(peft_config, adapter_name)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​
可以看到，cls最终映射到对应peft方案的模型实例，若是LORA，则返回一个LoraModel实例，看来关键就在于LoraModel中。</p>
<h3 id="loramodel">LoraModel</h3>
<p>​ LoraModel继承了BaseTuner，而BaseTuner继承nn.Module。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LoraModel</span><span class="token punctuation">(</span>BaseTuner<span class="token punctuation">)</span><span class="token punctuation">:</span>
    prefix<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">"lora_"</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">,</span> config<span class="token punctuation">,</span> adapter_name<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>model<span class="token punctuation">,</span> config<span class="token punctuation">,</span> adapter_name<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​
由于代码量非常大，所以只贴出核心的代码，这一段是LoRA的核心逻辑。首先判断target是不是Lora层，若是，则根据loraconfig来更新这一层。若不是，则创建一个新的Lora层来替换原来的层。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_create_and_replace</span><span class="token punctuation">(</span>
    self<span class="token punctuation">,</span>
    lora_config<span class="token punctuation">,</span>
    adapter_name<span class="token punctuation">,</span>
    target<span class="token punctuation">,</span>
    target_name<span class="token punctuation">,</span>
    parent<span class="token punctuation">,</span>
    current_key<span class="token punctuation">,</span>
<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    此处省略部分代码
    """</span>
    <span class="token keyword">from</span> peft<span class="token punctuation">.</span>tuners<span class="token punctuation">.</span>adalora <span class="token keyword">import</span> AdaLoraLayer

    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>target<span class="token punctuation">,</span> LoraLayer<span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>target<span class="token punctuation">,</span> AdaLoraLayer<span class="token punctuation">)</span><span class="token punctuation">:</span>
        target<span class="token punctuation">.</span>update_layer<span class="token punctuation">(</span>
            adapter_name<span class="token punctuation">,</span>
            r<span class="token punctuation">,</span>
            lora_alpha<span class="token operator">=</span>alpha<span class="token punctuation">,</span>
            lora_dropout<span class="token operator">=</span>lora_config<span class="token punctuation">.</span>lora_dropout<span class="token punctuation">,</span>
            init_lora_weights<span class="token operator">=</span>lora_config<span class="token punctuation">.</span>init_lora_weights<span class="token punctuation">,</span>
            use_rslora<span class="token operator">=</span>lora_config<span class="token punctuation">.</span>use_rslora<span class="token punctuation">,</span>
            use_dora<span class="token operator">=</span>lora_config<span class="token punctuation">.</span>use_dora<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        new_module <span class="token operator">=</span> self<span class="token punctuation">.</span>_create_new_module<span class="token punctuation">(</span>lora_config<span class="token punctuation">,</span> adapter_name<span class="token punctuation">,</span> target<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token keyword">if</span> adapter_name <span class="token operator">!=</span> self<span class="token punctuation">.</span>active_adapter<span class="token punctuation">:</span>
            <span class="token comment"># adding an additional adapter: it is not automatically trainable</span>
            new_module<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_replace_module<span class="token punctuation">(</span>parent<span class="token punctuation">,</span> target_name<span class="token punctuation">,</span> new_module<span class="token punctuation">,</span> target<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​
下面是<strong>update_layer</strong>函数的逻辑，首先保证秩大于0，然后创建dropout层。</p>
<p>​
再创建AB矩阵，可以看到A是降维矩阵，B是升维矩阵。之后初始化这两个矩阵，可以看到A矩阵可以选择kaiming
uniform或高斯分布初始化。若是用于Embedding层，那么A初始化为0，B用高斯分布。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  <span class="token keyword">def</span> <span class="token function">update_layer</span><span class="token punctuation">(</span>
      self<span class="token punctuation">,</span> adapter_name<span class="token punctuation">,</span> r<span class="token punctuation">,</span> lora_alpha<span class="token punctuation">,</span> lora_dropout<span class="token punctuation">,</span> init_lora_weights<span class="token punctuation">,</span> use_rslora<span class="token punctuation">,</span> use_dora<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span>
  <span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token comment"># This code works for linear layers, override for other layer types</span>
      <span class="token keyword">if</span> r <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">:</span>
          <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"`r` should be a positive integer value but the value passed is </span><span class="token interpolation"><span class="token punctuation">&#123;</span>r<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

      self<span class="token punctuation">.</span>r<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span> <span class="token operator">=</span> r
      self<span class="token punctuation">.</span>lora_alpha<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span> <span class="token operator">=</span> lora_alpha
      <span class="token keyword">if</span> lora_dropout <span class="token operator">></span> <span class="token number">0.0</span><span class="token punctuation">:</span>
          lora_dropout_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>lora_dropout<span class="token punctuation">)</span>
      <span class="token keyword">else</span><span class="token punctuation">:</span>
          lora_dropout_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>

      self<span class="token punctuation">.</span>lora_dropout<span class="token punctuation">.</span>update<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>ModuleDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span>adapter_name<span class="token punctuation">:</span> lora_dropout_layer<span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token comment"># Actual trainable parameters</span>
      self<span class="token punctuation">.</span>lora_A<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span> <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>in_features<span class="token punctuation">,</span> r<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
      self<span class="token punctuation">.</span>lora_B<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span> <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>r<span class="token punctuation">,</span> self<span class="token punctuation">.</span>out_features<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
      <span class="token keyword">if</span> use_rslora<span class="token punctuation">:</span>
          self<span class="token punctuation">.</span>scaling<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span> <span class="token operator">=</span> lora_alpha <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>r<span class="token punctuation">)</span>
      <span class="token keyword">else</span><span class="token punctuation">:</span>
          self<span class="token punctuation">.</span>scaling<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span> <span class="token operator">=</span> lora_alpha <span class="token operator">/</span> r

      <span class="token keyword">if</span> init_lora_weights <span class="token operator">==</span> <span class="token string">"loftq"</span><span class="token punctuation">:</span>
          self<span class="token punctuation">.</span>loftq_init<span class="token punctuation">(</span>adapter_name<span class="token punctuation">)</span>
      <span class="token keyword">elif</span> init_lora_weights<span class="token punctuation">:</span>
          self<span class="token punctuation">.</span>reset_lora_parameters<span class="token punctuation">(</span>adapter_name<span class="token punctuation">,</span> init_lora_weights<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
下略
"""</span>

      self<span class="token punctuation">.</span>set_adapter<span class="token punctuation">(</span>self<span class="token punctuation">.</span>active_adapters<span class="token punctuation">)</span>
    <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
  <span class="token keyword">def</span> <span class="token function">reset_lora_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> adapter_name<span class="token punctuation">,</span> init_lora_weights<span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">if</span> init_lora_weights <span class="token keyword">is</span> <span class="token boolean">False</span><span class="token punctuation">:</span>
          <span class="token keyword">return</span>

      <span class="token keyword">if</span> adapter_name <span class="token keyword">in</span> self<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
          <span class="token keyword">if</span> init_lora_weights <span class="token keyword">is</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
              <span class="token comment"># initialize A the same way as the default for nn.Linear and B to zero</span>
              <span class="token comment"># https://github.com/microsoft/LoRA/blob/a0a92e0f26c067cf94747bdbf1ce73793fa44d19/loralib/layers.py#L124</span>
              nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lora_A<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">,</span> a<span class="token operator">=</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
          <span class="token keyword">elif</span> init_lora_weights<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">"gaussian"</span><span class="token punctuation">:</span>
              nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lora_A<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>r<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span><span class="token punctuation">)</span>
          <span class="token keyword">else</span><span class="token punctuation">:</span>
              <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Unknown initialization </span><span class="token interpolation"><span class="token punctuation">&#123;</span>init_lora_weights<span class="token operator">=</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
          nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>zeros_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lora_B<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
      <span class="token keyword">if</span> adapter_name <span class="token keyword">in</span> self<span class="token punctuation">.</span>lora_embedding_A<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
          <span class="token comment"># initialize a the same way as the default for nn.linear and b to zero</span>
          nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>zeros_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lora_embedding_A<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span><span class="token punctuation">)</span>
          nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lora_embedding_B<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="loralayer">LoraLayer</h3>
<p>​
可以看到layer需要是LoraLayer的实例才会被更新，下面是LoraLayer的代码。最初的peft0.1.0，Lora只能用于Linear层，后来则将一些dense层统一抽象成为了LoraLayer，可以看到有Linear，Embedding，Conv1D和Conv2D。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LoraLayer</span><span class="token punctuation">(</span>BaseTunerLayer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># All names of layers that may contain (trainable) adapter weights</span>
    adapter_layer_names <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">"lora_A"</span><span class="token punctuation">,</span> <span class="token string">"lora_B"</span><span class="token punctuation">,</span> <span class="token string">"lora_embedding_A"</span><span class="token punctuation">,</span> <span class="token string">"lora_embedding_B"</span><span class="token punctuation">)</span>
    <span class="token comment"># All names of other parameters that may contain adapter-related parameters</span>
    other_param_names <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">"r"</span><span class="token punctuation">,</span> <span class="token string">"lora_alpha"</span><span class="token punctuation">,</span> <span class="token string">"scaling"</span><span class="token punctuation">,</span> <span class="token string">"lora_dropout"</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> base_layer<span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>base_layer <span class="token operator">=</span> base_layer
        self<span class="token punctuation">.</span>r <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        self<span class="token punctuation">.</span>lora_alpha <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        self<span class="token punctuation">.</span>scaling <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        self<span class="token punctuation">.</span>lora_dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lora_A <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lora_B <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>
        <span class="token comment"># For Embedding layer</span>
        self<span class="token punctuation">.</span>lora_embedding_A <span class="token operator">=</span> nn<span class="token punctuation">.</span>ParameterDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lora_embedding_B <span class="token operator">=</span> nn<span class="token punctuation">.</span>ParameterDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>
        <span class="token comment"># Mark the weight as unmerged</span>
        self<span class="token punctuation">.</span>_disable_adapters <span class="token operator">=</span> <span class="token boolean">False</span>
        self<span class="token punctuation">.</span>merged_adapters <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>use_dora<span class="token punctuation">:</span> <span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> <span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        self<span class="token punctuation">.</span>lora_magnitude_vector<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ParameterDict<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>  <span class="token comment"># for DoRA</span>
        self<span class="token punctuation">.</span>_caches<span class="token punctuation">:</span> <span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Any<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        self<span class="token punctuation">.</span>kwargs <span class="token operator">=</span> kwargs

        base_layer <span class="token operator">=</span> self<span class="token punctuation">.</span>get_base_layer<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">)</span><span class="token punctuation">:</span>
            in_features<span class="token punctuation">,</span> out_features <span class="token operator">=</span> base_layer<span class="token punctuation">.</span>in_features<span class="token punctuation">,</span> base_layer<span class="token punctuation">.</span>out_features
        <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
            in_features<span class="token punctuation">,</span> out_features <span class="token operator">=</span> base_layer<span class="token punctuation">.</span>in_channels<span class="token punctuation">,</span> base_layer<span class="token punctuation">.</span>out_channels
        <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">)</span><span class="token punctuation">:</span>
            in_features<span class="token punctuation">,</span> out_features <span class="token operator">=</span> base_layer<span class="token punctuation">.</span>num_embeddings<span class="token punctuation">,</span> base_layer<span class="token punctuation">.</span>embedding_dim
        <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> Conv1D<span class="token punctuation">)</span><span class="token punctuation">:</span>
            in_features<span class="token punctuation">,</span> out_features <span class="token operator">=</span> <span class="token punctuation">(</span>
                base_layer<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>ds_shape <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token string">"ds_shape"</span><span class="token punctuation">)</span> <span class="token keyword">else</span> base_layer<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>shape
            <span class="token punctuation">)</span>
        <span class="token keyword">elif</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> <span class="token string">"infeatures"</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> <span class="token string">"outfeatures"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># QuantLinear</span>
            in_features<span class="token punctuation">,</span> out_features <span class="token operator">=</span> base_layer<span class="token punctuation">.</span>infeatures<span class="token punctuation">,</span> base_layer<span class="token punctuation">.</span>outfeatures
        <span class="token keyword">elif</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> <span class="token string">"input_size"</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> <span class="token string">"output_size"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># Megatron ColumnParallelLinear,RowParallelLinear</span>
            in_features<span class="token punctuation">,</span> out_features <span class="token operator">=</span> base_layer<span class="token punctuation">.</span>input_size<span class="token punctuation">,</span> base_layer<span class="token punctuation">.</span>output_size
        <span class="token keyword">elif</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> <span class="token string">"codebooks"</span><span class="token punctuation">)</span> <span class="token keyword">and</span> base_layer<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__ <span class="token operator">==</span> <span class="token string">"QuantizedLinear"</span><span class="token punctuation">:</span>
            <span class="token comment"># AQLM QuantLinear</span>
            in_features<span class="token punctuation">,</span> out_features <span class="token operator">=</span> base_layer<span class="token punctuation">.</span>in_features<span class="token punctuation">,</span> base_layer<span class="token punctuation">.</span>out_features
        <span class="token keyword">elif</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> <span class="token string">"w_bit"</span><span class="token punctuation">)</span> <span class="token keyword">and</span> base_layer<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__ <span class="token operator">==</span> <span class="token string">"WQLinear_GEMM"</span><span class="token punctuation">:</span>
            <span class="token comment"># Awq layers</span>
            in_features<span class="token punctuation">,</span> out_features <span class="token operator">=</span> base_layer<span class="token punctuation">.</span>in_features<span class="token punctuation">,</span> base_layer<span class="token punctuation">.</span>out_features
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Unsupported layer type </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">type</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>in_features <span class="token operator">=</span> in_features
        self<span class="token punctuation">.</span>out_features <span class="token operator">=</span> out_features<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​
在LoraModel中的**_create_new_module**函数中我们也可以看到抛出的异常，应证了当target不是LoraLayer的时候，如果想创建一个新Lora模块将旧模块替换，那么需要原模块是torch.nn.Linear,
torch.nn.Embedding, torch.nn.Conv2d,
transformers.pytorch_utils.Conv1D这几种类型，最终这几个类在LoraLayer.py中都被重写了。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> new_module <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    <span class="token comment"># no module could be matched</span>
    <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
        <span class="token string-interpolation"><span class="token string">f"Target module </span><span class="token interpolation"><span class="token punctuation">&#123;</span>target<span class="token punctuation">&#125;</span></span><span class="token string"> is not supported. Currently, only the following modules are supported: "</span></span>
        <span class="token string">"`torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`."</span>
    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​
同时，对于不同的模型，支持的Lora策略也不同。Transformer库中列举了哪些模型的哪些层能够使用官方Lora方案。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING <span class="token operator">=</span> <span class="token punctuation">&#123;</span>
    <span class="token string">"t5"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q"</span><span class="token punctuation">,</span> <span class="token string">"v"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"mt5"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q"</span><span class="token punctuation">,</span> <span class="token string">"v"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"bart"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"gpt2"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"c_attn"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"bloom"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query_key_value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"blip-2"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q"</span><span class="token punctuation">,</span> <span class="token string">"v"</span><span class="token punctuation">,</span> <span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"opt"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"gptj"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"gpt_neox"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query_key_value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"gpt_neo"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"bert"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query"</span><span class="token punctuation">,</span> <span class="token string">"value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"roberta"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query"</span><span class="token punctuation">,</span> <span class="token string">"value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"xlm-roberta"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query"</span><span class="token punctuation">,</span> <span class="token string">"value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"electra"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query"</span><span class="token punctuation">,</span> <span class="token string">"value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"deberta-v2"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query_proj"</span><span class="token punctuation">,</span> <span class="token string">"value_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"deberta"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"in_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"layoutlm"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query"</span><span class="token punctuation">,</span> <span class="token string">"value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"llama"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"chatglm"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query_key_value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"gpt_bigcode"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"c_attn"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"mpt"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"Wqkv"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"RefinedWebModel"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query_key_value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"RefinedWeb"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query_key_value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"falcon"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query_key_value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"btlm"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"c_proj"</span><span class="token punctuation">,</span> <span class="token string">"c_attn"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"codegen"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"qkv_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"mistral"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"mixtral"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"stablelm"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"phi"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">,</span> <span class="token string">"fc1"</span><span class="token punctuation">,</span> <span class="token string">"fc2"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"gemma"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="linear">Linear</h3>
<p>​
我们只需要知道Linear最终被重写了，继承了LoraLayer，然后通过LoraLayer的<strong>update_layer</strong>方法根据config的参数进行了初始化。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Linear</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">,</span> LoraLayer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Lora implemented in a dense layer</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        base_layer<span class="token punctuation">,</span>
        adapter_name<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span>
        r<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>
        lora_alpha<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>
        lora_dropout<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">0.0</span><span class="token punctuation">,</span>
        fan_in_fan_out<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>  <span class="token comment"># Set this to True if the layer to replace stores weight like (fan_in, fan_out)</span>
        is_target_conv_1d_layer<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        init_lora_weights<span class="token punctuation">:</span> Union<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
        use_rslora<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        use_dora<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        LoraLayer<span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>self<span class="token punctuation">,</span> base_layer<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fan_in_fan_out <span class="token operator">=</span> fan_in_fan_out

        self<span class="token punctuation">.</span>_active_adapter <span class="token operator">=</span> adapter_name
        self<span class="token punctuation">.</span>update_layer<span class="token punctuation">(</span>
            adapter_name<span class="token punctuation">,</span>
            r<span class="token punctuation">,</span>
            lora_alpha<span class="token operator">=</span>lora_alpha<span class="token punctuation">,</span>
            lora_dropout<span class="token operator">=</span>lora_dropout<span class="token punctuation">,</span>
            init_lora_weights<span class="token operator">=</span>init_lora_weights<span class="token punctuation">,</span>
            use_rslora<span class="token operator">=</span>use_rslora<span class="token punctuation">,</span>
            use_dora<span class="token operator">=</span>use_dora<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>is_target_conv_1d_layer <span class="token operator">=</span> is_target_conv_1d_layer<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​
我们直奔forward方法。对于不合并的adapter，若已经合并，把已经加入到baselayer的权重移出来。可以合并的adapter若已经合并了就直接用这个权重。最终Lora中的Linear层的前向传播结果是：
<span class="math display">\[
X = XW + scale × \frac{α}{r}(dropout(X))AB
\]</span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">:</span> Any<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> Any<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
      self<span class="token punctuation">.</span>_check_forward_args<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
      adapter_names <span class="token operator">=</span> kwargs<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token string">"adapter_names"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment"># 若之前把该adapter设置为不合并但合并了，马上把加进去的权重拿出来</span>
      <span class="token keyword">if</span> self<span class="token punctuation">.</span>disable_adapters<span class="token punctuation">:</span>
          <span class="token keyword">if</span> self<span class="token punctuation">.</span>merged<span class="token punctuation">:</span>
              self<span class="token punctuation">.</span>unmerge<span class="token punctuation">(</span><span class="token punctuation">)</span>
          result <span class="token operator">=</span> self<span class="token punctuation">.</span>base_layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
      <span class="token keyword">elif</span> adapter_names <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
          result <span class="token operator">=</span> self<span class="token punctuation">.</span>_mixed_batch_forward<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> adapter_names<span class="token operator">=</span>adapter_names<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
      <span class="token comment"># 已经合并了就直接返回</span>
      <span class="token keyword">elif</span> self<span class="token punctuation">.</span>merged<span class="token punctuation">:</span>
          result <span class="token operator">=</span> self<span class="token punctuation">.</span>base_layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
      <span class="token keyword">else</span><span class="token punctuation">:</span>
          result <span class="token operator">=</span> self<span class="token punctuation">.</span>base_layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
          torch_result_dtype <span class="token operator">=</span> result<span class="token punctuation">.</span>dtype
          <span class="token keyword">for</span> active_adapter <span class="token keyword">in</span> self<span class="token punctuation">.</span>active_adapters<span class="token punctuation">:</span>
              <span class="token keyword">if</span> active_adapter <span class="token keyword">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                  <span class="token keyword">continue</span>
              lora_A <span class="token operator">=</span> self<span class="token punctuation">.</span>lora_A<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span>
              lora_B <span class="token operator">=</span> self<span class="token punctuation">.</span>lora_B<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span>
              dropout <span class="token operator">=</span> self<span class="token punctuation">.</span>lora_dropout<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span>
              <span class="token comment"># self.scaling[adapter] = scale * self.lora_alpha[adapter] / self.r[adapter]</span>
              scaling <span class="token operator">=</span> self<span class="token punctuation">.</span>scaling<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span>
              x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>lora_A<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>

              <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>use_dora<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span><span class="token punctuation">:</span>
                  result <span class="token operator">=</span> result <span class="token operator">+</span> lora_B<span class="token punctuation">(</span>lora_A<span class="token punctuation">(</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> scaling
              <span class="token keyword">else</span><span class="token punctuation">:</span>
                  x <span class="token operator">=</span> dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
                  result <span class="token operator">=</span> result <span class="token operator">+</span> self<span class="token punctuation">.</span>_apply_dora<span class="token punctuation">(</span>x<span class="token punctuation">,</span> lora_A<span class="token punctuation">,</span> lora_B<span class="token punctuation">,</span> scaling<span class="token punctuation">,</span> active_adapter<span class="token punctuation">)</span>

          result <span class="token operator">=</span> result<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch_result_dtype<span class="token punctuation">)</span>

      <span class="token keyword">return</span> result<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​
当然还有少不了的merge和unmerge函数，最终微调完毕的权重是可以选择加回原权重来消除额外的推理延迟。</p>
<p>​ 先看unmerge方法，其实就是把加到baselayer上的权重减回去。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">unmerge</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    This method unmerges all merged adapter layers from the base weights.
    """</span>
    <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>merged<span class="token punctuation">:</span>
        warnings<span class="token punctuation">.</span>warn<span class="token punctuation">(</span><span class="token string">"Already unmerged. Nothing to do."</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span>
    <span class="token keyword">while</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>merged_adapters<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
        active_adapter <span class="token operator">=</span> self<span class="token punctuation">.</span>merged_adapters<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> active_adapter <span class="token keyword">in</span> self<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            weight <span class="token operator">=</span> self<span class="token punctuation">.</span>get_base_layer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>weight
            delta_weight <span class="token operator">=</span> self<span class="token punctuation">.</span>get_delta_weight<span class="token punctuation">(</span>active_adapter<span class="token punctuation">)</span>
            <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>use_dora<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span><span class="token punctuation">:</span>
                <span class="token comment"># 只看这一句就行了</span>
                weight<span class="token punctuation">.</span>data <span class="token operator">-=</span> delta_weight
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                weight_norm <span class="token operator">=</span> self<span class="token punctuation">.</span>_cache_pop<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>active_adapter<span class="token punctuation">&#125;</span></span><span class="token string">-weight_norm"</span></span><span class="token punctuation">)</span>
                dora_factor <span class="token operator">=</span> self<span class="token punctuation">.</span>lora_magnitude_vector<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span> <span class="token operator">/</span> weight_norm
                weight_orig <span class="token operator">=</span> weight<span class="token punctuation">.</span>data <span class="token operator">/</span> dora_factor<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span> delta_weight
                weight<span class="token punctuation">.</span>data <span class="token operator">=</span> weight_orig<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​
再看merge操作，首先需要拿到<strong>delta_weight</strong>，具体的内容就不复制了，如果是fp16的权重在cpu上计算，那么由于cpu原生不支持该类型，所以需要转换成fp32再转换回去。最终的<span
class="math inline">\(W =
W+scaling×BA\)</span>，可以看到从头到尾是没有原来的W参与前向传播的，原来的W直接被冻结。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">merge</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> safe_merge<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span> adapter_names<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    adapter_names <span class="token operator">=</span> check_adapters_to_merge<span class="token punctuation">(</span>self<span class="token punctuation">,</span> adapter_names<span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token keyword">not</span> adapter_names<span class="token punctuation">:</span>
        <span class="token comment"># no adapter to merge</span>
        <span class="token keyword">return</span>

    <span class="token keyword">for</span> active_adapter <span class="token keyword">in</span> adapter_names<span class="token punctuation">:</span>
        <span class="token keyword">if</span> active_adapter <span class="token keyword">in</span> self<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            base_layer <span class="token operator">=</span> self<span class="token punctuation">.</span>get_base_layer<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> safe_merge<span class="token punctuation">:</span>
                <span class="token comment"># Note that safe_merge will be slower than the normal merge</span>
                <span class="token comment"># because of the copy operation.</span>
	<span class="token triple-quoted-string string">"""
	safe_merge其实就是查看是否有NaN值，有的话抛出异常。
	并且由于多复制了一次原权重，所以效率会更低，代码略
	"""</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token comment"># 该操作核心就一句，返回scaling B@A</span>
                <span class="token comment"># output_tensor = transpose(weight_B @ weight_A, self.fan_in_fan_out) * self.scaling[adapter]</span>
                delta_weight <span class="token operator">=</span> self<span class="token punctuation">.</span>get_delta_weight<span class="token punctuation">(</span>active_adapter<span class="token punctuation">)</span>
                <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>use_dora<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span><span class="token punctuation">:</span>
                    base_layer<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data <span class="token operator">=</span> base_layer<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data <span class="token operator">+</span> delta_weight
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    <span class="token comment"># handle dora，此处略</span>
            self<span class="token punctuation">.</span>merged_adapters<span class="token punctuation">.</span>append<span class="token punctuation">(</span>active_adapter<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​
剩下的Embedding，一维卷积和二维卷积操作差不多，这里就不多赘述了。</p>
<h3 id="合并操作">合并操作</h3>
<p>​
首先需要判断是否能够合并。有两种情况下不能合并，一种是当前使用gptq量化模型(QLoRA可以)，另一种则是开启了peftconfig中的layer_replication操作。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_check_merge_allowed</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Verify that the configuration supports merging.

    Currently gptq quantization and replicated layers do not support merging.
    """</span>
    <span class="token keyword">if</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>model<span class="token punctuation">,</span> <span class="token string">"quantization_method"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">"gptq"</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Cannot merge LORA layers when the model is gptq quantized"</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>peft_config<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"layer_replication"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Cannot merge LORA layers when base model layers are replicated"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​
最终调用的方法是<strong>merge_and_unload</strong>方法，<code>progressbar</code>就是是否用tqdm显示进行，<code>safe_merge</code>会检查tensor中是否有NaN，<code>adapter_names</code>用于指定哪些层要合并，默认是所有都合并。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  <span class="token keyword">def</span> <span class="token function">merge_and_unload</span><span class="token punctuation">(</span>
      self<span class="token punctuation">,</span> progressbar<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span> safe_merge<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span> adapter_names<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
  <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">:</span>
<span class="token comment"># progressbar就是是否用tqdm显示进行</span>
      <span class="token keyword">return</span> self<span class="token punctuation">.</span>_unload_and_optionally_merge<span class="token punctuation">(</span>
          progressbar<span class="token operator">=</span>progressbar<span class="token punctuation">,</span> safe_merge<span class="token operator">=</span>safe_merge<span class="token punctuation">,</span> adapter_names<span class="token operator">=</span>adapter_names
      <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​
下面是核心方法**_unload_and_optionally_merge**。首先会判断是否满足要求，也就是不使用gptq且不采取replicated策略的情况下才能合并。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_unload_and_optionally_merge</span><span class="token punctuation">(</span>
    self<span class="token punctuation">,</span>
    merge<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    progressbar<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    safe_merge<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    adapter_names<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> merge<span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>_check_merge_allowed<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>​
然后拿到targetlayer进行合并操作后，替换掉原来的module。这里的target.merge是LoraLayer的子类都实现的merge操作，上面已经给出。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>target<span class="token punctuation">,</span> <span class="token string">"base_layer"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> merge<span class="token punctuation">:</span>
        target<span class="token punctuation">.</span>merge<span class="token punctuation">(</span>safe_merge<span class="token operator">=</span>safe_merge<span class="token punctuation">,</span> adapter_names<span class="token operator">=</span>adapter_names<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>_replace_module<span class="token punctuation">(</span>parent<span class="token punctuation">,</span> target_name<span class="token punctuation">,</span> target<span class="token punctuation">.</span>get_base_layer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> target<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h1 id="参考资料">参考资料</h1>
<p>[1] <a target="_blank" rel="noopener" href='https://arxiv.org/abs/2303.15647'>Scaling Down to Scale
Up: A Guide to Parameter-Efficient Fine-Tuning</a></p>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>iroha</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://example.com/post/ft_survey.html" title="大模型微调方法综述">http://example.com/post/ft_survey.html</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/post/model_load.html" rel="prev" title="Huggingface的模型加载流程"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">Huggingface的模型加载流程</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/post/backtrace.html" rel="next" title="回溯之组合问题"><span class="post-nav-text">回溯之组合问题</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2025 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> iroha</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.11</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>