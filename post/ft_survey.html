<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="iroha"><meta name="copyright" content="iroha"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ç»¼è¿° | iroha</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...undefined?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"example.com","root":"/","title":"ä¸æƒ³æ‘†çƒ‚","version":"1.10.11","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"æœç´¢...","empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹: ${query}","hits":"æ‰¾åˆ° ${hits} æ¡ç»“æœ","hits_time":"æ‰¾åˆ° ${hits} æ¡ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js" defer></script><script src="/js/load-aplayer.js" defer></script><meta name="description" content="èƒŒæ™¯ â€‹ éšç€æ¨¡å‹çš„å‚æ•°è¶Šæ¥è¶Šå¤§ï¼Œä¸ºæ¯ä¸€ä¸ªä»»åŠ¡ä»å¤´è®­ç»ƒä¸€ä¸ªæ¨¡å‹è¶Šæ¥è¶Šå›°éš¾ï¼Œè€Œå‚æ•°é‡çš„å¢å¤§ä¹Ÿè®©æ¨¡å‹ç†è§£äº†æ›´æ·±å±‚çš„è¯­ä¹‰ä¿¡æ¯ã€‚éšç€BERTçš„å‡ºç°ï¼Œé¢„è®­ç»ƒ+å¾®è°ƒçš„æ–¹å¼èµ°ä¸Šå†å²èˆå°ã€‚ â€‹ ä¸ºä»€ä¹ˆä¸é€‰æ‹©å…¨å‚æ•°å¾®è°ƒå‘¢ï¼Ÿæœ€ä¸»è¦çš„åŸå› è¿˜æ˜¯æˆæœ¬å¤ªé«˜ï¼Œåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šæ„ˆå‘å›°éš¾ã€‚ä»¥GPT2-1.5Bä¸ºä¾‹ï¼Œè‹¥å‚æ•°ä¸æ¢¯åº¦ä»¥fp16ä¿å­˜ï¼Œæ˜¾å­˜éœ€è¦ä¿å­˜3+3 &#x3D; 6Bçš„æ•°æ®ï¼Œè‹¥ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œé‚£ä¹ˆè¿˜éœ€è¦ä¿å­˜fp32">
<meta property="og:type" content="article">
<meta property="og:title" content="å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ç»¼è¿°">
<meta property="og:url" content="http://example.com/post/ft_survey.html">
<meta property="og:site_name" content="iroha">
<meta property="og:description" content="èƒŒæ™¯ â€‹ éšç€æ¨¡å‹çš„å‚æ•°è¶Šæ¥è¶Šå¤§ï¼Œä¸ºæ¯ä¸€ä¸ªä»»åŠ¡ä»å¤´è®­ç»ƒä¸€ä¸ªæ¨¡å‹è¶Šæ¥è¶Šå›°éš¾ï¼Œè€Œå‚æ•°é‡çš„å¢å¤§ä¹Ÿè®©æ¨¡å‹ç†è§£äº†æ›´æ·±å±‚çš„è¯­ä¹‰ä¿¡æ¯ã€‚éšç€BERTçš„å‡ºç°ï¼Œé¢„è®­ç»ƒ+å¾®è°ƒçš„æ–¹å¼èµ°ä¸Šå†å²èˆå°ã€‚ â€‹ ä¸ºä»€ä¹ˆä¸é€‰æ‹©å…¨å‚æ•°å¾®è°ƒå‘¢ï¼Ÿæœ€ä¸»è¦çš„åŸå› è¿˜æ˜¯æˆæœ¬å¤ªé«˜ï¼Œåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šæ„ˆå‘å›°éš¾ã€‚ä»¥GPT2-1.5Bä¸ºä¾‹ï¼Œè‹¥å‚æ•°ä¸æ¢¯åº¦ä»¥fp16ä¿å­˜ï¼Œæ˜¾å­˜éœ€è¦ä¿å­˜3+3 &#x3D; 6Bçš„æ•°æ®ï¼Œè‹¥ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œé‚£ä¹ˆè¿˜éœ€è¦ä¿å­˜fp32">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20240408231535224.png">
<meta property="og:image" content="http://example.com/images/image-20240410215838916.png">
<meta property="og:image" content="http://example.com/images/image-20240411005445372.png">
<meta property="article:published_time" content="2024-04-08T04:00:00.000Z">
<meta property="article:modified_time" content="2024-04-12T19:32:20.406Z">
<meta property="article:author" content="iroha">
<meta property="article:tag" content="å¤§æ¨¡å‹">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20240408231535224.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="æ–‡ç« ç›®å½•"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="ç«™ç‚¹æ¦‚è§ˆ"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="iroha"><img width="96" loading="lazy" src="/yun.png" alt="iroha"><span class="site-author-status" title="æ°¸è¿œç›¸ä¿¡ç¾å¥½çš„äº‹æƒ…å³å°†å‘ç”Ÿ">ğŸ˜Š</span></a><div class="site-author-name"><a href="/about/">iroha</a></div><span class="site-name">iroha</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="é¦–é¡µ"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="å½’æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">15</span></a></div><div class="site-state-item"><a href="/categories/" title="åˆ†ç±»"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">5</span></a></div><div class="site-state-item"><a href="/tags/" title="æ ‡ç­¾"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">2</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="æ–‡æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="å‹é“¾" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">1.</span> <span class="toc-text">èƒŒæ™¯</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#freeze"><span class="toc-number">2.</span> <span class="toc-text">Freeze</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#bitfit"><span class="toc-number">3.</span> <span class="toc-text">Bitfit</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#prompt-tuning"><span class="toc-number">4.</span> <span class="toc-text">Prompt-tuning</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#lora"><span class="toc-number">5.</span> <span class="toc-text">Lora</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-number">5.1.</span> <span class="toc-text">åŸç†</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E9%80%9F%E8%AF%BB"><span class="toc-number">5.2.</span> <span class="toc-text">æºç é€Ÿè¯»</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#get_peft_model"><span class="toc-number">5.2.1.</span> <span class="toc-text">get_peft_model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#peftmodel"><span class="toc-number">5.2.2.</span> <span class="toc-text">PeftModel</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#loramodel"><span class="toc-number">5.2.3.</span> <span class="toc-text">LoraModel</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#loralayer"><span class="toc-number">5.2.4.</span> <span class="toc-text">LoraLayer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#linear"><span class="toc-number">5.2.5.</span> <span class="toc-text">Linear</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%88%E5%B9%B6%E6%93%8D%E4%BD%9C"><span class="toc-number">5.2.6.</span> <span class="toc-text">åˆå¹¶æ“ä½œ</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">6.</span> <span class="toc-text">å‚è€ƒèµ„æ–™</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://example.com/post/ft_survey.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="iroha"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="iroha"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ç»¼è¿°</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="åˆ›å»ºæ—¶é—´ï¼š2024-04-08 12:00:00" itemprop="dateCreated datePublished" datetime="2024-04-08T12:00:00+08:00">2024-04-08</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="ä¿®æ”¹æ—¶é—´ï¼š2024-04-13 03:32:20" itemprop="dateModified" datetime="2024-04-13T03:32:20+08:00">2024-04-13</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">NLP</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">å¤§æ¨¡å‹</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><h1 id="èƒŒæ™¯">èƒŒæ™¯</h1>
<p>â€‹
éšç€æ¨¡å‹çš„å‚æ•°è¶Šæ¥è¶Šå¤§ï¼Œä¸ºæ¯ä¸€ä¸ªä»»åŠ¡ä»å¤´è®­ç»ƒä¸€ä¸ªæ¨¡å‹è¶Šæ¥è¶Šå›°éš¾ï¼Œè€Œå‚æ•°é‡çš„å¢å¤§ä¹Ÿè®©æ¨¡å‹ç†è§£äº†æ›´æ·±å±‚çš„è¯­ä¹‰ä¿¡æ¯ã€‚éšç€BERTçš„å‡ºç°ï¼Œé¢„è®­ç»ƒ+å¾®è°ƒçš„æ–¹å¼èµ°ä¸Šå†å²èˆå°ã€‚</p>
<p>â€‹
ä¸ºä»€ä¹ˆä¸é€‰æ‹©å…¨å‚æ•°å¾®è°ƒå‘¢ï¼Ÿæœ€ä¸»è¦çš„åŸå› è¿˜æ˜¯æˆæœ¬å¤ªé«˜ï¼Œåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šæ„ˆå‘å›°éš¾ã€‚ä»¥GPT2-1.5Bä¸ºä¾‹ï¼Œè‹¥å‚æ•°ä¸æ¢¯åº¦ä»¥fp16ä¿å­˜ï¼Œæ˜¾å­˜éœ€è¦ä¿å­˜3+3
=
6Bçš„æ•°æ®ï¼Œè‹¥ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œé‚£ä¹ˆè¿˜éœ€è¦ä¿å­˜fp32çš„æ¨¡å‹å‚æ•°å¤‡ä»½ï¼Œmomentumä¸varianceï¼Œåˆéœ€è¦6+6+6
=
18Gï¼Œæ€»å…±éœ€è¦24Gã€‚å†åŠ ä¸Šå…¶ä»–çŠ¶æ€ï¼Œå¦‚activationï¼Œbufferï¼Œè¿˜æœ‰æ˜¾å­˜ç¢ç‰‡æ— æ³•åˆ©ç”¨ï¼Œå®é™…ä¸Šéœ€è¦çš„æ˜¾å­˜æ˜¯å¤§äº24Gçš„ã€‚</p>
<p>â€‹
å…¨å‚æ•°å¾®è°ƒè¿˜æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå½“LLMå°è¯•å­¦ä¹ å¤šä¸ªè¿ç»­ä»»åŠ¡æ—¶ï¼Œå®¹æ˜“å¿˜è®°ä¹‹å‰å­¦åˆ°çš„ä¸œè¥¿ï¼Œä¹Ÿå°±æ˜¯â€œç¾éš¾æ€§é—å¿˜â€ã€‚å¦‚ä½•åœ¨ä¿ç•™å…ˆå‰çŸ¥è¯†çš„åŸºç¡€ä¸Šå¢é‡åœ°å¢å¼ºLLMï¼Œå³è¿›è¡ŒæŒç»­å­¦ä¹ ï¼Œè‡³å…³é‡è¦ã€‚ç®€å•æ¥è¯´ï¼Œå…¨é‡å¾®è°ƒæœ‰overfittingï¼Œç¾éš¾æ€§é—å¿˜ï¼Œæˆæœ¬é«˜çš„é—®é¢˜ã€‚å› æ­¤ï¼Œé«˜æ•ˆå¾®è°ƒæ¨¡å‹çš„å‚æ•°å°±æˆäº†ä¸€ä¸ªæ–°æ–¹å‘ï¼ŒPEFT(Parameter-Efficient
Fine-tuning)åº”è¿è€Œç”Ÿã€‚</p>
<p>â€‹
å¾®è°ƒä¸€èˆ¬åªæ›´æ”¹æ¨¡å‹çš„éƒ¨åˆ†å‚æ•°ï¼Œè°ƒæ•´å“ªäº›å‚æ•°ï¼Œå¦‚ä½•è°ƒæ•´åˆ™å¯¹åº”äº†ä¸åŒæ–¹æ³•ã€‚ä¸€èˆ¬ä»ä¸‹æ¸¸ä»»åŠ¡å‡ºå‘ï¼Œæ¥å†³å®šä¸‹æ¸¸åº”è¯¥å¦‚ä½•æ·»åŠ å‚æ•°ã€‚</p>
<p>â€‹
ä¸‹é¢æˆ‘ä»¬ä»¥transformersåº“ä¸­çš„BERTä¸ºä¾‹ã€‚æˆ‘ä»¬éƒ½çŸ¥é“ï¼ŒBERTçš„é¢„è®­ç»ƒæ˜¯MLMä¸NSPï¼Œæ‰€ä»¥ä¸‹æ¸¸ä»»åŠ¡è‡ªç„¶æœ‰è¿™ä¸¤è€…ï¼Œä¹Ÿå°±æ˜¯<strong>ä¸‹ä¸€å¥é¢„æµ‹</strong>å’Œ<strong>å®Œå½¢å¡«ç©º</strong>ã€‚æ‰€ä»¥è¿™æ‰åº”è¯¥æ˜¯BERTçš„åŸç”Ÿä¸‹æ¸¸ä»»åŠ¡ã€‚</p>
<p>â€‹ é¦–å…ˆçœ‹NSPï¼Œæ¨¡å‹çš„ç»“æ„å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># assert logits[0, 0] &lt; logits[0, 1]  # next sentence was random</span>
<span class="token keyword">class</span> <span class="token class-name">BertForNextSentencePrediction</span><span class="token punctuation">(</span>BertPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>bert <span class="token operator">=</span> BertModel<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>cls <span class="token operator">=</span> BertOnlyNSPHead<span class="token punctuation">(</span>config<span class="token punctuation">)</span>

        <span class="token comment"># Initialize weights and apply final processing</span>
        self<span class="token punctuation">.</span>post_init<span class="token punctuation">(</span><span class="token punctuation">)</span>

        
<span class="token keyword">class</span> <span class="token class-name">BertOnlyNSPHead</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>seq_relationship <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pooled_output<span class="token punctuation">)</span><span class="token punctuation">:</span>
        seq_relationship_score <span class="token operator">=</span> self<span class="token punctuation">.</span>seq_relationship<span class="token punctuation">(</span>pooled_output<span class="token punctuation">)</span>
        <span class="token keyword">return</span> seq_relationship_score<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹
å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äºNSPä»»åŠ¡ï¼Œå°±æ˜¯åœ¨Modelçš„åé¢æ‹¼ä¸€ä¸ªLinearå±‚ï¼Œå°†768ç»´æ˜ å°„åˆ°2ç»´ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ã€‚</p>
<p>â€‹
å†çœ‹SequenceClassificationä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯åºåˆ—åˆ†ç±»ã€‚å®é™…ä¸Šä¹Ÿæ˜¯åœ¨æœ€åå¡«å……äº†ä¸€å±‚Linearå±‚ç”¨äºåˆ†ç±»ã€‚ä¸NSPçš„åŒºåˆ«åœ¨äºpoolerå±‚ä¼šå…ˆç»è¿‡ä¸€æ¬¡dropoutï¼Œdropoutçš„æ¦‚ç‡å¯ä»¥åœ¨configä¸­è®¾ç½®ï¼Œé»˜è®¤ä¸º0.1ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">BertForSequenceClassification</span><span class="token punctuation">(</span>BertPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_labels <span class="token operator">=</span> config<span class="token punctuation">.</span>num_labels
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config

        self<span class="token punctuation">.</span>bert <span class="token operator">=</span> BertModel<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        classifier_dropout <span class="token operator">=</span> <span class="token punctuation">(</span>
            config<span class="token punctuation">.</span>classifier_dropout <span class="token keyword">if</span> config<span class="token punctuation">.</span>classifier_dropout <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> config<span class="token punctuation">.</span>hidden_dropout_prob
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>classifier_dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>classifier <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>num_labels<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹ <img src="../images/image-20240408231535224.png"
alt="image-20240408231535224" /></p>
<p>â€‹ PEFTæŒ‰æ ¸å¿ƒæ€æƒ³å¯ä»¥åˆ†ä¸ºä¸‰ç±»ï¼š</p>
<ol type="1">
<li>æ·»åŠ ä¸€äº›å‚æ•°é‡å°çš„å±‚ï¼Œåªå¾®è°ƒè¿™äº›å±‚ï¼Œæœ€å…¸å‹çš„å°±æ˜¯LSTã€‚</li>
<li>é€‰æ‹©æŸäº›å±‚ï¼Œæˆ–å±‚ä¸­çš„æŸä¸€éƒ¨åˆ†è¿›è¡Œå¾®è°ƒï¼Œæœ€å…¸å‹çš„æ˜¯BitFitï¼Œåªå¯¹biasè¿›è¡Œå¾®è°ƒã€‚</li>
<li>é‡å‚æ•°åŒ–ï¼Œä¹Ÿç®—æ˜¯å¢åŠ ä¸€éƒ¨åˆ†å‚æ•°ï¼Œä½†æœ€ååŠ å›åŸå‚æ•°çš„å¯¹åº”éƒ¨åˆ†</li>
</ol>
<h1 id="freeze">Freeze</h1>
<p>â€‹
å†»ç»“æŸäº›å‚æ•°æ˜¯æœ€å®¹æ˜“æƒ³åˆ°çš„æ–¹æ³•ï¼Œä»…ä»…è°ƒæ•´æŸäº›æœªè¢«å†»ç»“çš„å‚æ•°å°±å¯ä»¥å‡å°‘å¤§é‡æ˜¾å­˜å ç”¨ï¼Œä½†freezeæ–¹æ³•å¤§å¤§é™ä½äº†æ¨¡å‹çš„çµæ´»æ€§ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> name <span class="token punctuation">,</span>param <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">any</span> <span class="token punctuation">(</span>n <span class="token keyword">in</span> name <span class="token keyword">for</span> n <span class="token keyword">in</span> <span class="token punctuation">[</span>layers_name1<span class="token punctuation">,</span>layer_name2<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
		param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h1 id="bitfit">Bitfit</h1>
<p>å‡ºè‡ª<strong>BitFit: Simple Parameter-efficient Fine-tuning or
Transformer-based Masked Language-models</strong>ã€‚</p>
<p>pass</p>
<h1 id="prompt-tuning">Prompt-tuning</h1>
<p>pass</p>
<h1 id="lora">Lora</h1>
<h2 id="åŸç†">åŸç†</h2>
<figure>
<img src="../images/image-20240410215838916.png"
alt="image-20240410215838916" />
<figcaption aria-hidden="true">image-20240410215838916</figcaption>
</figure>
<p>â€‹ åœ¨åŸå§‹æƒé‡ä¸­å¹¶ä¸Šä¸€ä¸ªæ—è·¯çš„åˆ†æ”¯ï¼Œä»¥Linearå±‚ä¸ºä¾‹å­ï¼ŒåŸæœ¬<span
class="math inline">\(h = W_{dÃ—k}X\)</span>ï¼Œ<span
class="math inline">\(W\)</span>æ˜¯æƒé‡ï¼Œæ—è·¯æœ‰ä¸¤ä¸ªä½ç§©çš„çŸ©é˜µï¼Œå…¶ä¸­<span
class="math inline">\(A_{rÃ—k} =
N(0,Ïƒ^2)\)</span>ï¼Œä¹Ÿå°±æ˜¯ä»¥é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–ï¼Œè€Œ<span
class="math inline">\(B_{dÃ—r}\)</span>çŸ©é˜µåˆ™ä»¥å…¨0åˆå§‹åŒ–ï¼Œå…¶ä¸­ç»´åº¦rè¿œå°äºdå’Œkã€‚<span
class="math inline">\(Î”W = BA\)</span> ,æœ€ç»ˆçš„æƒé‡ä¸º$W+Î”W <span
class="math inline">\(ã€‚å¯¹äº\)</span>Î”W<span
class="math inline">\(å¯ä»¥ä½¿ç”¨ä¸€ä¸ªÎ±å‚æ•°æ¥æ§åˆ¶å€æ•°ï¼Œå³\)</span>W+Î”W$ã€‚Bä¸ºå‡ç»´çŸ©é˜µï¼ŒAä¸ºé™ç»´çŸ©é˜µã€‚å®é™…ä¸Šï¼ŒLoRAä¸€èˆ¬ç”¨äºDenseå±‚ã€‚</p>
<p>â€‹ å¯¹äºæ¢¯åº¦è®¡ç®—ï¼Œå€Ÿç”¨çŸ¥ä¹ä¸ŠCodeLearnerç­”ä¸»çš„ä¸€å¼ å›¾ã€‚</p>
<p>â€‹ <img src="../images/image-20240411005445372.png"
alt="image-20240411005445372" /></p>
<p>â€‹
æ‰€ä»¥ï¼Œåœ¨å¾®è°ƒæ—¶ï¼ŒæŸä¸€Loraå±‚åå‘ä¼ æ’­ä¸­çš„æ¢¯åº¦è®¡ç®—é‡æ˜¯è¦æ›´å¤šçš„ï¼Œä½†ç”±äºrè¿œå°äºåŸæƒé‡çš„ç»´åº¦då’Œkï¼Œæ‰€ä»¥ç›¸å¯¹äºå…¨é‡å¾®è°ƒä¿å­˜çš„æ¢¯åº¦å€¼å°±å°‘ã€‚åŒæ—¶ï¼ŒåŸè®ºæ–‡ä¸­ä¹Ÿåªå¯¹Transformer
Blockä¸­çš„Q,K,Vè¿›è¡Œäº†Loraå¤„ç†ã€‚è‹¥rç­‰äºkï¼Œé‚£ä¹ˆæ­¤æ—¶å¯ä»¥ç­‰ä»·äºå…¨å‚æ•°å¾®è°ƒã€‚</p>
<p>â€‹ å¯¹äºåˆå§‹åŒ–é—®é¢˜ï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¿è¯æœ€å¼€å§‹çš„<span
class="math inline">\(Î”W=0\)</span>ï¼Œæ‰€ä»¥éœ€è¦ABçš„å…¶ä¸­ä¸€è€…ä¸º0ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬çœ‹ä¸Šå›¾å¸¦ä¸ŠLoRAåçš„åå‘ä¼ æ’­æ¢¯åº¦è®¡ç®—ï¼Œè‹¥Aä¸º0ï¼Œé‚£ä¹ˆæ¢¯åº¦å°±ä¼šä¸€ç›´ä¸º0ï¼Œ<span
class="math inline">\(Î”W\)</span>å°±ä¸ä¼šæ›´æ–°ã€‚</p>
<p>â€‹
åŸæ–‡ä¸­æåˆ°äº†LoRAçš„Limitationï¼Œå¦‚æœé€‰æ‹©å°†Aå’ŒBçŸ©é˜µå¸æ”¶(åˆå¹¶)åˆ°WåŸå§‹æƒé‡çŸ©é˜µä¸­,ä»¥æ¶ˆé™¤é¢å¤–çš„æ¨ç†å»¶è¿Ÿ,é‚£ä¹ˆåœ¨å•ä¸ªå‰å‘ä¼ é€’ä¸­ä¸ºä¸åŒä»»åŠ¡çš„ä¸åŒAå’ŒBæ‰¹é‡è¾“å…¥æ•°æ®å°±å˜å¾—ä¸ç›´è§‚äº†ã€‚æˆ‘æƒ³åŸæ–‡çš„æ„æ€æ˜¯ä¸€ä¸ªbatché‡Œå¯èƒ½æœ‰ä¸åŒçš„ä»»åŠ¡ï¼Œé‚£ä¹ˆä¸åŒçš„ä»»åŠ¡åº”è¯¥ç”¨ä¸åŒçš„æƒé‡ï¼Œé‚£ä¹ˆæœ€å¥½æ˜¯ä¸å°†<span
class="math inline">\(Î”W\)</span>åˆå¹¶åˆ°åŸå§‹æƒé‡ï¼Œé’ˆå¯¹ä¸åŒä»»åŠ¡æ¥åŠ¨æ€é€‰æ‹©<span
class="math inline">\(Î”W\)</span>ï¼Œè¿™éœ€è¦åœ¨æ¨ç†é€Ÿåº¦ä¸Šåšå–èˆã€‚</p>
<h2 id="æºç é€Ÿè¯»">æºç é€Ÿè¯»</h2>
<p>â€‹ ä¸ºäº†æ–¹ä¾¿ç†è§£ï¼Œæˆ‘ä»¬ä»peft0.10.0çš„å®˜æ–¹ç¤ºä¾‹å‡ºå‘ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForSeq2SeqLM
<span class="token keyword">from</span> peft <span class="token keyword">import</span> get_peft_config<span class="token punctuation">,</span> get_peft_model<span class="token punctuation">,</span> LoraConfig<span class="token punctuation">,</span> TaskType
model_name_or_path <span class="token operator">=</span> <span class="token string">"bigscience/mt0-large"</span>
tokenizer_name_or_path <span class="token operator">=</span> <span class="token string">"bigscience/mt0-large"</span>

peft_config <span class="token operator">=</span> LoraConfig<span class="token punctuation">(</span>
    task_type<span class="token operator">=</span>TaskType<span class="token punctuation">.</span>SEQ_2_SEQ_LM<span class="token punctuation">,</span> inference_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> r<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> lora_alpha<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> lora_dropout<span class="token operator">=</span><span class="token number">0.1</span>
<span class="token punctuation">)</span>

model <span class="token operator">=</span> AutoModelForSeq2SeqLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name_or_path<span class="token punctuation">)</span>
<span class="token comment">#ç­‰ä»·äº model = PeftModelForSeq2SeqLM(model,peft_config)</span>
<span class="token comment">#ä¹Ÿç­‰ä»·äºmodel = lora_model = LoraModel(model, config, "default")</span>
model <span class="token operator">=</span> get_peft_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">)</span> 

model<span class="token punctuation">.</span>print_trainable_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token string">"trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹
é¦–å…ˆä»LoraConfigå‡ºå‘ï¼Œè¿™é‡Œçš„task_typeæ˜¯PeftConfigçš„å‚æ•°ï¼Œå…¶ä½™åˆ™æ˜¯LoraConfigçš„å‚æ•°ï¼Œè¿™äº›å‚æ•°åœ¨ä¸Šæ–‡çš„åŸç†ä¸­éƒ½æœ‰æåˆ°ã€‚ä¸»è¦è¿˜æ˜¯æ ¹æ®ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡è¿”å›ä¸åŒçš„æ¨¡å‹ã€‚è¿™é‡Œä¹Ÿå¯ä»¥ç›´æ¥ç”¨get_peft_configå‡½æ•°æ¥è¯»å–ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">peft_config <span class="token operator">=</span> LoraConfig<span class="token punctuation">(</span>
    task_type<span class="token operator">=</span>TaskType<span class="token punctuation">.</span>SEQ_2_SEQ_LM<span class="token punctuation">,</span> inference_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> r<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> lora_alpha<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> lora_dropout<span class="token operator">=</span><span class="token number">0.1</span>
<span class="token punctuation">)</span>
<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
config <span class="token operator">=</span> <span class="token punctuation">&#123;</span>
    <span class="token string">"task_type"</span><span class="token punctuation">:</span><span class="token string">"SEQ_2_SEQ_LM"</span><span class="token punctuation">,</span>
    <span class="token string">"peft_type"</span><span class="token punctuation">:</span><span class="token string">"LORA"</span><span class="token punctuation">,</span>
    <span class="token string">"inference_mode"</span><span class="token punctuation">:</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    <span class="token string">"r"</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">,</span>
    <span class="token string">"lora_alpha"</span><span class="token punctuation">:</span><span class="token number">32</span><span class="token punctuation">,</span>
    <span class="token string">"lora_dropout"</span><span class="token punctuation">:</span><span class="token number">0.1</span>
<span class="token punctuation">&#125;</span>

another_config <span class="token operator">=</span> get_peft_config<span class="token punctuation">(</span>config<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="get_peft_model">get_peft_model</h3>
<p>â€‹
æ¥ä¸‹æ¥çœ‹get_peft_modelã€‚ä¼ å…¥çš„å‚æ•°æœ‰æ¨¡å‹å’Œå¯¹åº”çš„configï¼Œæœ€åä¼šè¿”å›ä¸€ä¸ªPeftModelå®ä¾‹ã€‚è¿”å›çš„å®ä¾‹ç±»å‹ä¼šæ ¹æ®ä¼ å…¥çš„configæ¥ç¡®å®šã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> get_peft_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">)</span>
<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>mapping<span class="token punctuation">.</span>py<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
<span class="token keyword">def</span> <span class="token function">get_peft_model</span><span class="token punctuation">(</span>
    model<span class="token punctuation">:</span> PreTrainedModel<span class="token punctuation">,</span> peft_config<span class="token punctuation">:</span> PeftConfig<span class="token punctuation">,</span> adapter_name<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">"default"</span><span class="token punctuation">,</span> mixed<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span>
<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> PeftModel <span class="token operator">|</span> PeftMixedModel<span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Returns a Peft model object from a model and a config.

    Args:
        model ([`transformers.PreTrainedModel`]):
            Model to be wrapped.
        peft_config ([`PeftConfig`]):
            Configuration object containing the parameters of the Peft model.
        adapter_name (`str`, `optional`, defaults to `"default"`):
            The name of the adapter to be injected, if not provided, the default adapter name is used ("default").
        mixed (`bool`, `optional`, defaults to `False`):
            Whether to allow mixing different (compatible) adapter types.
    """</span>
    model_config <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token string">"config"</span><span class="token punctuation">,</span> <span class="token punctuation">&#123;</span><span class="token string">"model_type"</span><span class="token punctuation">:</span> <span class="token string">"custom"</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>model_config<span class="token punctuation">,</span> <span class="token string">"to_dict"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        model_config <span class="token operator">=</span> model_config<span class="token punctuation">.</span>to_dict<span class="token punctuation">(</span><span class="token punctuation">)</span>

    peft_config<span class="token punctuation">.</span>base_model_name_or_path <span class="token operator">=</span> model<span class="token punctuation">.</span>__dict__<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"name_or_path"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> mixed<span class="token punctuation">:</span>
        <span class="token keyword">return</span> PeftMixedModel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">,</span> adapter_name<span class="token operator">=</span>adapter_name<span class="token punctuation">)</span>

    <span class="token keyword">if</span> peft_config<span class="token punctuation">.</span>task_type <span class="token keyword">not</span> <span class="token keyword">in</span> MODEL_TYPE_TO_PEFT_MODEL_MAPPING<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token keyword">not</span> peft_config<span class="token punctuation">.</span>is_prompt_learning<span class="token punctuation">:</span>
        <span class="token keyword">return</span> PeftModel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">,</span> adapter_name<span class="token operator">=</span>adapter_name<span class="token punctuation">)</span>

    <span class="token keyword">if</span> peft_config<span class="token punctuation">.</span>is_prompt_learning<span class="token punctuation">:</span>
        peft_config <span class="token operator">=</span> _prepare_prompt_learning_config<span class="token punctuation">(</span>peft_config<span class="token punctuation">,</span> model_config<span class="token punctuation">)</span>
    <span class="token keyword">return</span> MODEL_TYPE_TO_PEFT_MODEL_MAPPING<span class="token punctuation">[</span>peft_config<span class="token punctuation">.</span>task_type<span class="token punctuation">]</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">,</span> adapter_name<span class="token operator">=</span>adapter_name<span class="token punctuation">)</span>
<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
MODEL_TYPE_TO_PEFT_MODEL_MAPPING<span class="token punctuation">:</span> <span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> PeftModel<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span>
    <span class="token string">"SEQ_CLS"</span><span class="token punctuation">:</span> PeftModelForSequenceClassification<span class="token punctuation">,</span>
    <span class="token string">"SEQ_2_SEQ_LM"</span><span class="token punctuation">:</span> PeftModelForSeq2SeqLM<span class="token punctuation">,</span>
    <span class="token string">"CAUSAL_LM"</span><span class="token punctuation">:</span> PeftModelForCausalLM<span class="token punctuation">,</span>
    <span class="token string">"TOKEN_CLS"</span><span class="token punctuation">:</span> PeftModelForTokenClassification<span class="token punctuation">,</span>
    <span class="token string">"QUESTION_ANS"</span><span class="token punctuation">:</span> PeftModelForQuestionAnswering<span class="token punctuation">,</span>
    <span class="token string">"FEATURE_EXTRACTION"</span><span class="token punctuation">:</span> PeftModelForFeatureExtraction<span class="token punctuation">,</span>
<span class="token punctuation">&#125;</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹
å…³é”®åœ¨äºè¿™ä¸¤å¥ã€‚å¦‚æœä»»åŠ¡ç±»å‹ä¸åœ¨æ”¯æŒçš„ç‰¹å®šä»»åŠ¡ä¸­ï¼Œè¿”å›PeftModelï¼Œå¦åˆ™è¿”å›å¯¹åº”ä»»åŠ¡ç±»å‹çš„Modelï¼Œè¿™äº›Modelç»§æ‰¿äº†PeftModelã€‚è‹¥æ˜¯æç¤ºå­¦ä¹ ç±»å‹çš„ï¼Œå¦‚prompt-tuningï¼Œåˆ™éœ€è¦é¢å¤–çš„configä¿¡æ¯ï¼Œå¦‚éšè—å±‚çš„æ•°é‡,å¯èƒ½çš„é”®åŒ…æ‹¬<code>num_hidden_layers</code>ã€<code>num_layers</code>ã€<code>n_layer</code>ï¼Œ
å¦‚æœæ— æ³•æ‰¾åˆ°,åˆ™éœ€è¦åœ¨peft_configä¸­æ‰‹åŠ¨æŒ‡å®šnum_layersï¼Œè¿™ä¸ªå‚æ•°æŒ‡å®šäº†promptå°†è¢«æ³¨å…¥åˆ°æ¨¡å‹çš„å“ªäº›å±‚ã€‚è¿˜éœ€è¦æ³¨æ„åŠ›çš„å¤´æ•°ï¼Œencoderéšè—å±‚çš„å¤§å°å’Œtokenembeddingçš„ç»´åº¦ç­‰ç­‰ã€‚è¿™äº›å‚æ•°è‹¥ä»æ¨¡å‹çš„configä¸­æ‰¾ä¸åˆ°ï¼Œåˆ™éœ€è¦åœ¨peftconfigä¸­è‡ªè¡ŒæŒ‡å®šã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">    <span class="token keyword">if</span> peft_config<span class="token punctuation">.</span>task_type <span class="token keyword">not</span> <span class="token keyword">in</span> MODEL_TYPE_TO_PEFT_MODEL_MAPPING<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token keyword">not</span> peft_config<span class="token punctuation">.</span>is_prompt_learning<span class="token punctuation">:</span>
        <span class="token keyword">return</span> PeftModel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">,</span> adapter_name<span class="token operator">=</span>adapter_name<span class="token punctuation">)</span>

    <span class="token keyword">if</span> peft_config<span class="token punctuation">.</span>is_prompt_learning<span class="token punctuation">:</span>
        peft_config <span class="token operator">=</span> _prepare_prompt_learning_config<span class="token punctuation">(</span>peft_config<span class="token punctuation">,</span> model_config<span class="token punctuation">)</span>
    <span class="token keyword">return</span> MODEL_TYPE_TO_PEFT_MODEL_MAPPING<span class="token punctuation">[</span>peft_config<span class="token punctuation">.</span>task_type<span class="token punctuation">]</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">,</span> adapter_name<span class="token operator">=</span>adapter_name<span class="token punctuation">)</span>
<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
<span class="token keyword">def</span> <span class="token function">_prepare_prompt_learning_config</span><span class="token punctuation">(</span>peft_config<span class="token punctuation">,</span> model_config<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> peft_config<span class="token punctuation">.</span>num_layers <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">"num_hidden_layers"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            num_layers <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"num_hidden_layers"</span><span class="token punctuation">]</span>
        <span class="token keyword">elif</span> <span class="token string">"num_layers"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            num_layers <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"num_layers"</span><span class="token punctuation">]</span>
        <span class="token keyword">elif</span> <span class="token string">"n_layer"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            num_layers <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"n_layer"</span><span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Please specify `num_layers` in `peft_config`"</span><span class="token punctuation">)</span>
        peft_config<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> num_layers

    <span class="token keyword">if</span> peft_config<span class="token punctuation">.</span>token_dim <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">"hidden_size"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            token_dim <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"hidden_size"</span><span class="token punctuation">]</span>
        <span class="token keyword">elif</span> <span class="token string">"n_embd"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            token_dim <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"n_embd"</span><span class="token punctuation">]</span>
        <span class="token keyword">elif</span> <span class="token string">"d_model"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            token_dim <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"d_model"</span><span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Please specify `token_dim` in `peft_config`"</span><span class="token punctuation">)</span>
        peft_config<span class="token punctuation">.</span>token_dim <span class="token operator">=</span> token_dim

    <span class="token keyword">if</span> peft_config<span class="token punctuation">.</span>num_attention_heads <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">"num_attention_heads"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            num_attention_heads <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"num_attention_heads"</span><span class="token punctuation">]</span>
        <span class="token keyword">elif</span> <span class="token string">"n_head"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            num_attention_heads <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"n_head"</span><span class="token punctuation">]</span>
        <span class="token keyword">elif</span> <span class="token string">"num_heads"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            num_attention_heads <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"num_heads"</span><span class="token punctuation">]</span>
        <span class="token keyword">elif</span> <span class="token string">"encoder_attention_heads"</span> <span class="token keyword">in</span> model_config<span class="token punctuation">:</span>
            num_attention_heads <span class="token operator">=</span> model_config<span class="token punctuation">[</span><span class="token string">"encoder_attention_heads"</span><span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Please specify `num_attention_heads` in `peft_config`"</span><span class="token punctuation">)</span>
        peft_config<span class="token punctuation">.</span>num_attention_heads <span class="token operator">=</span> num_attention_heads

    <span class="token keyword">if</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>peft_config<span class="token punctuation">,</span> <span class="token string">"encoder_hidden_size"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token builtin">setattr</span><span class="token punctuation">(</span>peft_config<span class="token punctuation">,</span> <span class="token string">"encoder_hidden_size"</span><span class="token punctuation">,</span> peft_config<span class="token punctuation">.</span>token_dim<span class="token punctuation">)</span>

    <span class="token keyword">return</span> peft_config<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹
å›åˆ°é‡ç‚¹ï¼Œæˆ‘ä»¬ä»¥Seq2SeqModelçš„ä»£ç ä¸ºä¾‹ï¼Œç›¸å¯¹äºPeftModelï¼Œæ•´ä½“ç»“æ„ä¸€æ ·ï¼Œåªæ˜¯å¤šäº†ä¸¤ä¸ªå‚æ•°å˜é‡ï¼Œå…¶ä¸­prepare_inputs_for_generationå‡½æ•°éœ€è¦ç”Ÿæˆæ¨¡å‹åœ¨generateæ–¹æ³•ä¸­è‡ªè¡Œå®ç°ï¼Œbase_model_prepare_encoder_decoder_kwargs_for_generationå˜é‡åˆ™ä»base_modelä¸­æå–å‡ºä¸€äº›ç”Ÿæˆæ—¶çš„å‚æ•°ï¼Œå¦‚æ˜¯å¦è¦ä½¿ç”¨cacheçš„use_caheï¼Œå°†encoderçš„å‚æ•°å°è£…è¿›model_kwargsè¿”å›ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PeftModelForSeq2SeqLM</span><span class="token punctuation">(</span>PeftModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">,</span> peft_config<span class="token punctuation">:</span> PeftConfig<span class="token punctuation">,</span> adapter_name<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">"default"</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>model<span class="token punctuation">,</span> peft_config<span class="token punctuation">,</span> adapter_name<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>base_model_prepare_inputs_for_generation <span class="token operator">=</span> self<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>prepare_inputs_for_generation
        self<span class="token punctuation">.</span>base_model_prepare_encoder_decoder_kwargs_for_generation <span class="token operator">=</span> <span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>_prepare_encoder_decoder_kwargs_for_generation
        <span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="peftmodel">PeftModel</h3>
<p>â€‹ è¿˜æ˜¯å…ˆè´´å‡ºæ•´ä½“çš„åˆå§‹åŒ–æºç ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PeftModel</span><span class="token punctuation">(</span>PushToHubMixin<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">:</span> PreTrainedModel<span class="token punctuation">,</span> peft_config<span class="token punctuation">:</span> PeftConfig<span class="token punctuation">,</span> adapter_name<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">"default"</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>modules_to_save <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>active_adapter <span class="token operator">=</span> adapter_name
        self<span class="token punctuation">.</span>peft_type <span class="token operator">=</span> peft_config<span class="token punctuation">.</span>peft_type
        <span class="token comment"># These args are special PEFT arguments that users can pass. They need to be removed before passing them to</span>
        <span class="token comment"># forward.</span>
        self<span class="token punctuation">.</span>special_peft_forward_args <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">"adapter_names"</span><span class="token punctuation">&#125;</span>

        self<span class="token punctuation">.</span>_is_prompt_learning <span class="token operator">=</span> peft_config<span class="token punctuation">.</span>is_prompt_learning
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>_is_prompt_learning<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>_peft_config <span class="token operator">=</span> <span class="token punctuation">&#123;</span>adapter_name<span class="token punctuation">:</span> peft_config<span class="token punctuation">&#125;</span>
            self<span class="token punctuation">.</span>base_model <span class="token operator">=</span> model
            self<span class="token punctuation">.</span>add_adapter<span class="token punctuation">(</span>adapter_name<span class="token punctuation">,</span> peft_config<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>_peft_config <span class="token operator">=</span> <span class="token boolean">None</span>
            cls <span class="token operator">=</span> PEFT_TYPE_TO_MODEL_MAPPING<span class="token punctuation">[</span>peft_config<span class="token punctuation">.</span>peft_type<span class="token punctuation">]</span>
            self<span class="token punctuation">.</span>base_model <span class="token operator">=</span> cls<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token punctuation">&#123;</span>adapter_name<span class="token punctuation">:</span> peft_config<span class="token punctuation">&#125;</span><span class="token punctuation">,</span> adapter_name<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>set_additional_trainable_modules<span class="token punctuation">(</span>peft_config<span class="token punctuation">,</span> adapter_name<span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token string">"is_gradient_checkpointing"</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            model <span class="token operator">=</span> self<span class="token punctuation">.</span>_prepare_model_for_gradient_checkpointing<span class="token punctuation">(</span>model<span class="token punctuation">)</span>

        <span class="token comment"># the `pretraining_tp` is set for some models to simulate Tensor Parallelism during inference to avoid</span>
        <span class="token comment"># numerical differences, https://github.com/pytorch/pytorch/issues/76232 - to avoid any unexpected</span>
        <span class="token comment"># behavior we disable that in this line.</span>
        <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>base_model<span class="token punctuation">,</span> <span class="token string">"config"</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>config<span class="token punctuation">,</span> <span class="token string">"pretraining_tp"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp <span class="token operator">=</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹
PeftModelä¸­çš„æºç å¾ˆå¤§éƒ¨åˆ†æœåŠ¡äºæç¤ºå­¦ä¹ ï¼Œå…ˆä¸çœ‹ã€‚æˆ‘ä»¬åªçœ‹_is_prompt_learningæ˜¯Falseçš„æƒ…å†µã€‚</p>
<pre class="line-numbers language-none"><code class="language-none">else:
    self._peft_config &#x3D; None
    cls &#x3D; PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type]
    self.base_model &#x3D; cls(model, &#123;adapter_name: peft_config&#125;, adapter_name)
    self.set_additional_trainable_modules(peft_config, adapter_name)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹
å¯ä»¥çœ‹åˆ°ï¼Œclsæœ€ç»ˆæ˜ å°„åˆ°å¯¹åº”peftæ–¹æ¡ˆçš„æ¨¡å‹å®ä¾‹ï¼Œè‹¥æ˜¯LORAï¼Œåˆ™è¿”å›ä¸€ä¸ªLoraModelå®ä¾‹ï¼Œçœ‹æ¥å…³é”®å°±åœ¨äºLoraModelä¸­ã€‚</p>
<h3 id="loramodel">LoraModel</h3>
<p>â€‹ LoraModelç»§æ‰¿äº†BaseTunerï¼Œè€ŒBaseTunerç»§æ‰¿nn.Moduleã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LoraModel</span><span class="token punctuation">(</span>BaseTuner<span class="token punctuation">)</span><span class="token punctuation">:</span>
    prefix<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">"lora_"</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">,</span> config<span class="token punctuation">,</span> adapter_name<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>model<span class="token punctuation">,</span> config<span class="token punctuation">,</span> adapter_name<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹
ç”±äºä»£ç é‡éå¸¸å¤§ï¼Œæ‰€ä»¥åªè´´å‡ºæ ¸å¿ƒçš„ä»£ç ï¼Œè¿™ä¸€æ®µæ˜¯LoRAçš„æ ¸å¿ƒé€»è¾‘ã€‚é¦–å…ˆåˆ¤æ–­targetæ˜¯ä¸æ˜¯Loraå±‚ï¼Œè‹¥æ˜¯ï¼Œåˆ™æ ¹æ®loraconfigæ¥æ›´æ–°è¿™ä¸€å±‚ã€‚è‹¥ä¸æ˜¯ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„Loraå±‚æ¥æ›¿æ¢åŸæ¥çš„å±‚ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_create_and_replace</span><span class="token punctuation">(</span>
    self<span class="token punctuation">,</span>
    lora_config<span class="token punctuation">,</span>
    adapter_name<span class="token punctuation">,</span>
    target<span class="token punctuation">,</span>
    target_name<span class="token punctuation">,</span>
    parent<span class="token punctuation">,</span>
    current_key<span class="token punctuation">,</span>
<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    æ­¤å¤„çœç•¥éƒ¨åˆ†ä»£ç 
    """</span>
    <span class="token keyword">from</span> peft<span class="token punctuation">.</span>tuners<span class="token punctuation">.</span>adalora <span class="token keyword">import</span> AdaLoraLayer

    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>target<span class="token punctuation">,</span> LoraLayer<span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>target<span class="token punctuation">,</span> AdaLoraLayer<span class="token punctuation">)</span><span class="token punctuation">:</span>
        target<span class="token punctuation">.</span>update_layer<span class="token punctuation">(</span>
            adapter_name<span class="token punctuation">,</span>
            r<span class="token punctuation">,</span>
            lora_alpha<span class="token operator">=</span>alpha<span class="token punctuation">,</span>
            lora_dropout<span class="token operator">=</span>lora_config<span class="token punctuation">.</span>lora_dropout<span class="token punctuation">,</span>
            init_lora_weights<span class="token operator">=</span>lora_config<span class="token punctuation">.</span>init_lora_weights<span class="token punctuation">,</span>
            use_rslora<span class="token operator">=</span>lora_config<span class="token punctuation">.</span>use_rslora<span class="token punctuation">,</span>
            use_dora<span class="token operator">=</span>lora_config<span class="token punctuation">.</span>use_dora<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        new_module <span class="token operator">=</span> self<span class="token punctuation">.</span>_create_new_module<span class="token punctuation">(</span>lora_config<span class="token punctuation">,</span> adapter_name<span class="token punctuation">,</span> target<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token keyword">if</span> adapter_name <span class="token operator">!=</span> self<span class="token punctuation">.</span>active_adapter<span class="token punctuation">:</span>
            <span class="token comment"># adding an additional adapter: it is not automatically trainable</span>
            new_module<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_replace_module<span class="token punctuation">(</span>parent<span class="token punctuation">,</span> target_name<span class="token punctuation">,</span> new_module<span class="token punctuation">,</span> target<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹
ä¸‹é¢æ˜¯<strong>update_layer</strong>å‡½æ•°çš„é€»è¾‘ï¼Œé¦–å…ˆä¿è¯ç§©å¤§äº0ï¼Œç„¶ååˆ›å»ºdropoutå±‚ã€‚</p>
<p>â€‹
å†åˆ›å»ºABçŸ©é˜µï¼Œå¯ä»¥çœ‹åˆ°Aæ˜¯é™ç»´çŸ©é˜µï¼ŒBæ˜¯å‡ç»´çŸ©é˜µã€‚ä¹‹ååˆå§‹åŒ–è¿™ä¸¤ä¸ªçŸ©é˜µï¼Œå¯ä»¥çœ‹åˆ°AçŸ©é˜µå¯ä»¥é€‰æ‹©kaiming
uniformæˆ–é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–ã€‚è‹¥æ˜¯ç”¨äºEmbeddingå±‚ï¼Œé‚£ä¹ˆAåˆå§‹åŒ–ä¸º0ï¼ŒBç”¨é«˜æ–¯åˆ†å¸ƒã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  <span class="token keyword">def</span> <span class="token function">update_layer</span><span class="token punctuation">(</span>
      self<span class="token punctuation">,</span> adapter_name<span class="token punctuation">,</span> r<span class="token punctuation">,</span> lora_alpha<span class="token punctuation">,</span> lora_dropout<span class="token punctuation">,</span> init_lora_weights<span class="token punctuation">,</span> use_rslora<span class="token punctuation">,</span> use_dora<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span>
  <span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token comment"># This code works for linear layers, override for other layer types</span>
      <span class="token keyword">if</span> r <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">:</span>
          <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"`r` should be a positive integer value but the value passed is </span><span class="token interpolation"><span class="token punctuation">&#123;</span>r<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

      self<span class="token punctuation">.</span>r<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span> <span class="token operator">=</span> r
      self<span class="token punctuation">.</span>lora_alpha<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span> <span class="token operator">=</span> lora_alpha
      <span class="token keyword">if</span> lora_dropout <span class="token operator">></span> <span class="token number">0.0</span><span class="token punctuation">:</span>
          lora_dropout_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>lora_dropout<span class="token punctuation">)</span>
      <span class="token keyword">else</span><span class="token punctuation">:</span>
          lora_dropout_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>

      self<span class="token punctuation">.</span>lora_dropout<span class="token punctuation">.</span>update<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>ModuleDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span>adapter_name<span class="token punctuation">:</span> lora_dropout_layer<span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token comment"># Actual trainable parameters</span>
      self<span class="token punctuation">.</span>lora_A<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span> <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>in_features<span class="token punctuation">,</span> r<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
      self<span class="token punctuation">.</span>lora_B<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span> <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>r<span class="token punctuation">,</span> self<span class="token punctuation">.</span>out_features<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
      <span class="token keyword">if</span> use_rslora<span class="token punctuation">:</span>
          self<span class="token punctuation">.</span>scaling<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span> <span class="token operator">=</span> lora_alpha <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>r<span class="token punctuation">)</span>
      <span class="token keyword">else</span><span class="token punctuation">:</span>
          self<span class="token punctuation">.</span>scaling<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span> <span class="token operator">=</span> lora_alpha <span class="token operator">/</span> r

      <span class="token keyword">if</span> init_lora_weights <span class="token operator">==</span> <span class="token string">"loftq"</span><span class="token punctuation">:</span>
          self<span class="token punctuation">.</span>loftq_init<span class="token punctuation">(</span>adapter_name<span class="token punctuation">)</span>
      <span class="token keyword">elif</span> init_lora_weights<span class="token punctuation">:</span>
          self<span class="token punctuation">.</span>reset_lora_parameters<span class="token punctuation">(</span>adapter_name<span class="token punctuation">,</span> init_lora_weights<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
ä¸‹ç•¥
"""</span>

      self<span class="token punctuation">.</span>set_adapter<span class="token punctuation">(</span>self<span class="token punctuation">.</span>active_adapters<span class="token punctuation">)</span>
    <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
  <span class="token keyword">def</span> <span class="token function">reset_lora_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> adapter_name<span class="token punctuation">,</span> init_lora_weights<span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">if</span> init_lora_weights <span class="token keyword">is</span> <span class="token boolean">False</span><span class="token punctuation">:</span>
          <span class="token keyword">return</span>

      <span class="token keyword">if</span> adapter_name <span class="token keyword">in</span> self<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
          <span class="token keyword">if</span> init_lora_weights <span class="token keyword">is</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
              <span class="token comment"># initialize A the same way as the default for nn.Linear and B to zero</span>
              <span class="token comment"># https://github.com/microsoft/LoRA/blob/a0a92e0f26c067cf94747bdbf1ce73793fa44d19/loralib/layers.py#L124</span>
              nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lora_A<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">,</span> a<span class="token operator">=</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
          <span class="token keyword">elif</span> init_lora_weights<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">"gaussian"</span><span class="token punctuation">:</span>
              nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lora_A<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>r<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span><span class="token punctuation">)</span>
          <span class="token keyword">else</span><span class="token punctuation">:</span>
              <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Unknown initialization </span><span class="token interpolation"><span class="token punctuation">&#123;</span>init_lora_weights<span class="token operator">=</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
          nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>zeros_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lora_B<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
      <span class="token keyword">if</span> adapter_name <span class="token keyword">in</span> self<span class="token punctuation">.</span>lora_embedding_A<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
          <span class="token comment"># initialize a the same way as the default for nn.linear and b to zero</span>
          nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>zeros_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lora_embedding_A<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span><span class="token punctuation">)</span>
          nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>lora_embedding_B<span class="token punctuation">[</span>adapter_name<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="loralayer">LoraLayer</h3>
<p>â€‹
å¯ä»¥çœ‹åˆ°layeréœ€è¦æ˜¯LoraLayerçš„å®ä¾‹æ‰ä¼šè¢«æ›´æ–°ï¼Œä¸‹é¢æ˜¯LoraLayerçš„ä»£ç ã€‚æœ€åˆçš„peft0.1.0ï¼ŒLoraåªèƒ½ç”¨äºLinearå±‚ï¼Œåæ¥åˆ™å°†ä¸€äº›denseå±‚ç»Ÿä¸€æŠ½è±¡æˆä¸ºäº†LoraLayerï¼Œå¯ä»¥çœ‹åˆ°æœ‰Linearï¼ŒEmbeddingï¼ŒConv1Då’ŒConv2Dã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LoraLayer</span><span class="token punctuation">(</span>BaseTunerLayer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># All names of layers that may contain (trainable) adapter weights</span>
    adapter_layer_names <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">"lora_A"</span><span class="token punctuation">,</span> <span class="token string">"lora_B"</span><span class="token punctuation">,</span> <span class="token string">"lora_embedding_A"</span><span class="token punctuation">,</span> <span class="token string">"lora_embedding_B"</span><span class="token punctuation">)</span>
    <span class="token comment"># All names of other parameters that may contain adapter-related parameters</span>
    other_param_names <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">"r"</span><span class="token punctuation">,</span> <span class="token string">"lora_alpha"</span><span class="token punctuation">,</span> <span class="token string">"scaling"</span><span class="token punctuation">,</span> <span class="token string">"lora_dropout"</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> base_layer<span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>base_layer <span class="token operator">=</span> base_layer
        self<span class="token punctuation">.</span>r <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        self<span class="token punctuation">.</span>lora_alpha <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        self<span class="token punctuation">.</span>scaling <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        self<span class="token punctuation">.</span>lora_dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lora_A <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lora_B <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>
        <span class="token comment"># For Embedding layer</span>
        self<span class="token punctuation">.</span>lora_embedding_A <span class="token operator">=</span> nn<span class="token punctuation">.</span>ParameterDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lora_embedding_B <span class="token operator">=</span> nn<span class="token punctuation">.</span>ParameterDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>
        <span class="token comment"># Mark the weight as unmerged</span>
        self<span class="token punctuation">.</span>_disable_adapters <span class="token operator">=</span> <span class="token boolean">False</span>
        self<span class="token punctuation">.</span>merged_adapters <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>use_dora<span class="token punctuation">:</span> <span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> <span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        self<span class="token punctuation">.</span>lora_magnitude_vector<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ParameterDict<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>  <span class="token comment"># for DoRA</span>
        self<span class="token punctuation">.</span>_caches<span class="token punctuation">:</span> <span class="token builtin">dict</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Any<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        self<span class="token punctuation">.</span>kwargs <span class="token operator">=</span> kwargs

        base_layer <span class="token operator">=</span> self<span class="token punctuation">.</span>get_base_layer<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">)</span><span class="token punctuation">:</span>
            in_features<span class="token punctuation">,</span> out_features <span class="token operator">=</span> base_layer<span class="token punctuation">.</span>in_features<span class="token punctuation">,</span> base_layer<span class="token punctuation">.</span>out_features
        <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
            in_features<span class="token punctuation">,</span> out_features <span class="token operator">=</span> base_layer<span class="token punctuation">.</span>in_channels<span class="token punctuation">,</span> base_layer<span class="token punctuation">.</span>out_channels
        <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">)</span><span class="token punctuation">:</span>
            in_features<span class="token punctuation">,</span> out_features <span class="token operator">=</span> base_layer<span class="token punctuation">.</span>num_embeddings<span class="token punctuation">,</span> base_layer<span class="token punctuation">.</span>embedding_dim
        <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> Conv1D<span class="token punctuation">)</span><span class="token punctuation">:</span>
            in_features<span class="token punctuation">,</span> out_features <span class="token operator">=</span> <span class="token punctuation">(</span>
                base_layer<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>ds_shape <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token string">"ds_shape"</span><span class="token punctuation">)</span> <span class="token keyword">else</span> base_layer<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>shape
            <span class="token punctuation">)</span>
        <span class="token keyword">elif</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> <span class="token string">"infeatures"</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> <span class="token string">"outfeatures"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># QuantLinear</span>
            in_features<span class="token punctuation">,</span> out_features <span class="token operator">=</span> base_layer<span class="token punctuation">.</span>infeatures<span class="token punctuation">,</span> base_layer<span class="token punctuation">.</span>outfeatures
        <span class="token keyword">elif</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> <span class="token string">"input_size"</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> <span class="token string">"output_size"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># Megatron ColumnParallelLinear,RowParallelLinear</span>
            in_features<span class="token punctuation">,</span> out_features <span class="token operator">=</span> base_layer<span class="token punctuation">.</span>input_size<span class="token punctuation">,</span> base_layer<span class="token punctuation">.</span>output_size
        <span class="token keyword">elif</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> <span class="token string">"codebooks"</span><span class="token punctuation">)</span> <span class="token keyword">and</span> base_layer<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__ <span class="token operator">==</span> <span class="token string">"QuantizedLinear"</span><span class="token punctuation">:</span>
            <span class="token comment"># AQLM QuantLinear</span>
            in_features<span class="token punctuation">,</span> out_features <span class="token operator">=</span> base_layer<span class="token punctuation">.</span>in_features<span class="token punctuation">,</span> base_layer<span class="token punctuation">.</span>out_features
        <span class="token keyword">elif</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">,</span> <span class="token string">"w_bit"</span><span class="token punctuation">)</span> <span class="token keyword">and</span> base_layer<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__ <span class="token operator">==</span> <span class="token string">"WQLinear_GEMM"</span><span class="token punctuation">:</span>
            <span class="token comment"># Awq layers</span>
            in_features<span class="token punctuation">,</span> out_features <span class="token operator">=</span> base_layer<span class="token punctuation">.</span>in_features<span class="token punctuation">,</span> base_layer<span class="token punctuation">.</span>out_features
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Unsupported layer type </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token builtin">type</span><span class="token punctuation">(</span>base_layer<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>in_features <span class="token operator">=</span> in_features
        self<span class="token punctuation">.</span>out_features <span class="token operator">=</span> out_features<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹
åœ¨LoraModelä¸­çš„**_create_new_module**å‡½æ•°ä¸­æˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹åˆ°æŠ›å‡ºçš„å¼‚å¸¸ï¼Œåº”è¯äº†å½“targetä¸æ˜¯LoraLayerçš„æ—¶å€™ï¼Œå¦‚æœæƒ³åˆ›å»ºä¸€ä¸ªæ–°Loraæ¨¡å—å°†æ—§æ¨¡å—æ›¿æ¢ï¼Œé‚£ä¹ˆéœ€è¦åŸæ¨¡å—æ˜¯torch.nn.Linear,
torch.nn.Embedding, torch.nn.Conv2d,
transformers.pytorch_utils.Conv1Dè¿™å‡ ç§ç±»å‹ï¼Œæœ€ç»ˆè¿™å‡ ä¸ªç±»åœ¨LoraLayer.pyä¸­éƒ½è¢«é‡å†™äº†ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> new_module <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    <span class="token comment"># no module could be matched</span>
    <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
        <span class="token string-interpolation"><span class="token string">f"Target module </span><span class="token interpolation"><span class="token punctuation">&#123;</span>target<span class="token punctuation">&#125;</span></span><span class="token string"> is not supported. Currently, only the following modules are supported: "</span></span>
        <span class="token string">"`torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`."</span>
    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹
åŒæ—¶ï¼Œå¯¹äºä¸åŒçš„æ¨¡å‹ï¼Œæ”¯æŒçš„Loraç­–ç•¥ä¹Ÿä¸åŒã€‚Transformeråº“ä¸­åˆ—ä¸¾äº†å“ªäº›æ¨¡å‹çš„å“ªäº›å±‚èƒ½å¤Ÿä½¿ç”¨å®˜æ–¹Loraæ–¹æ¡ˆã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING <span class="token operator">=</span> <span class="token punctuation">&#123;</span>
    <span class="token string">"t5"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q"</span><span class="token punctuation">,</span> <span class="token string">"v"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"mt5"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q"</span><span class="token punctuation">,</span> <span class="token string">"v"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"bart"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"gpt2"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"c_attn"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"bloom"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query_key_value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"blip-2"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q"</span><span class="token punctuation">,</span> <span class="token string">"v"</span><span class="token punctuation">,</span> <span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"opt"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"gptj"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"gpt_neox"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query_key_value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"gpt_neo"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"bert"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query"</span><span class="token punctuation">,</span> <span class="token string">"value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"roberta"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query"</span><span class="token punctuation">,</span> <span class="token string">"value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"xlm-roberta"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query"</span><span class="token punctuation">,</span> <span class="token string">"value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"electra"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query"</span><span class="token punctuation">,</span> <span class="token string">"value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"deberta-v2"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query_proj"</span><span class="token punctuation">,</span> <span class="token string">"value_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"deberta"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"in_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"layoutlm"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query"</span><span class="token punctuation">,</span> <span class="token string">"value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"llama"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"chatglm"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query_key_value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"gpt_bigcode"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"c_attn"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"mpt"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"Wqkv"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"RefinedWebModel"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query_key_value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"RefinedWeb"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query_key_value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"falcon"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query_key_value"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"btlm"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"c_proj"</span><span class="token punctuation">,</span> <span class="token string">"c_attn"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"codegen"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"qkv_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"mistral"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"mixtral"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"stablelm"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"phi"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">,</span> <span class="token string">"fc1"</span><span class="token punctuation">,</span> <span class="token string">"fc2"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"gemma"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="linear">Linear</h3>
<p>â€‹
æˆ‘ä»¬åªéœ€è¦çŸ¥é“Linearæœ€ç»ˆè¢«é‡å†™äº†ï¼Œç»§æ‰¿äº†LoraLayerï¼Œç„¶åé€šè¿‡LoraLayerçš„<strong>update_layer</strong>æ–¹æ³•æ ¹æ®configçš„å‚æ•°è¿›è¡Œäº†åˆå§‹åŒ–ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Linear</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">,</span> LoraLayer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Lora implemented in a dense layer</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        base_layer<span class="token punctuation">,</span>
        adapter_name<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span>
        r<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>
        lora_alpha<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>
        lora_dropout<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">0.0</span><span class="token punctuation">,</span>
        fan_in_fan_out<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>  <span class="token comment"># Set this to True if the layer to replace stores weight like (fan_in, fan_out)</span>
        is_target_conv_1d_layer<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        init_lora_weights<span class="token punctuation">:</span> Union<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
        use_rslora<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        use_dora<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        LoraLayer<span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>self<span class="token punctuation">,</span> base_layer<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fan_in_fan_out <span class="token operator">=</span> fan_in_fan_out

        self<span class="token punctuation">.</span>_active_adapter <span class="token operator">=</span> adapter_name
        self<span class="token punctuation">.</span>update_layer<span class="token punctuation">(</span>
            adapter_name<span class="token punctuation">,</span>
            r<span class="token punctuation">,</span>
            lora_alpha<span class="token operator">=</span>lora_alpha<span class="token punctuation">,</span>
            lora_dropout<span class="token operator">=</span>lora_dropout<span class="token punctuation">,</span>
            init_lora_weights<span class="token operator">=</span>init_lora_weights<span class="token punctuation">,</span>
            use_rslora<span class="token operator">=</span>use_rslora<span class="token punctuation">,</span>
            use_dora<span class="token operator">=</span>use_dora<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>is_target_conv_1d_layer <span class="token operator">=</span> is_target_conv_1d_layer<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹
æˆ‘ä»¬ç›´å¥”forwardæ–¹æ³•ã€‚å¯¹äºä¸åˆå¹¶çš„adapterï¼Œè‹¥å·²ç»åˆå¹¶ï¼ŒæŠŠå·²ç»åŠ å…¥åˆ°baselayerçš„æƒé‡ç§»å‡ºæ¥ã€‚å¯ä»¥åˆå¹¶çš„adapterè‹¥å·²ç»åˆå¹¶äº†å°±ç›´æ¥ç”¨è¿™ä¸ªæƒé‡ã€‚æœ€ç»ˆLoraä¸­çš„Linearå±‚çš„å‰å‘ä¼ æ’­ç»“æœæ˜¯ï¼š
<span class="math display">\[
X = XW + scale Ã— \frac{Î±}{r}(dropout(X))AB
\]</span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">:</span> Any<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> Any<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
      self<span class="token punctuation">.</span>_check_forward_args<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
      adapter_names <span class="token operator">=</span> kwargs<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token string">"adapter_names"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment"># è‹¥ä¹‹å‰æŠŠè¯¥adapterè®¾ç½®ä¸ºä¸åˆå¹¶ä½†åˆå¹¶äº†ï¼Œé©¬ä¸ŠæŠŠåŠ è¿›å»çš„æƒé‡æ‹¿å‡ºæ¥</span>
      <span class="token keyword">if</span> self<span class="token punctuation">.</span>disable_adapters<span class="token punctuation">:</span>
          <span class="token keyword">if</span> self<span class="token punctuation">.</span>merged<span class="token punctuation">:</span>
              self<span class="token punctuation">.</span>unmerge<span class="token punctuation">(</span><span class="token punctuation">)</span>
          result <span class="token operator">=</span> self<span class="token punctuation">.</span>base_layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
      <span class="token keyword">elif</span> adapter_names <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
          result <span class="token operator">=</span> self<span class="token punctuation">.</span>_mixed_batch_forward<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> adapter_names<span class="token operator">=</span>adapter_names<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
      <span class="token comment"># å·²ç»åˆå¹¶äº†å°±ç›´æ¥è¿”å›</span>
      <span class="token keyword">elif</span> self<span class="token punctuation">.</span>merged<span class="token punctuation">:</span>
          result <span class="token operator">=</span> self<span class="token punctuation">.</span>base_layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
      <span class="token keyword">else</span><span class="token punctuation">:</span>
          result <span class="token operator">=</span> self<span class="token punctuation">.</span>base_layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
          torch_result_dtype <span class="token operator">=</span> result<span class="token punctuation">.</span>dtype
          <span class="token keyword">for</span> active_adapter <span class="token keyword">in</span> self<span class="token punctuation">.</span>active_adapters<span class="token punctuation">:</span>
              <span class="token keyword">if</span> active_adapter <span class="token keyword">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                  <span class="token keyword">continue</span>
              lora_A <span class="token operator">=</span> self<span class="token punctuation">.</span>lora_A<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span>
              lora_B <span class="token operator">=</span> self<span class="token punctuation">.</span>lora_B<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span>
              dropout <span class="token operator">=</span> self<span class="token punctuation">.</span>lora_dropout<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span>
              <span class="token comment"># self.scaling[adapter] = scale * self.lora_alpha[adapter] / self.r[adapter]</span>
              scaling <span class="token operator">=</span> self<span class="token punctuation">.</span>scaling<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span>
              x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>lora_A<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>

              <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>use_dora<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span><span class="token punctuation">:</span>
                  result <span class="token operator">=</span> result <span class="token operator">+</span> lora_B<span class="token punctuation">(</span>lora_A<span class="token punctuation">(</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> scaling
              <span class="token keyword">else</span><span class="token punctuation">:</span>
                  x <span class="token operator">=</span> dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
                  result <span class="token operator">=</span> result <span class="token operator">+</span> self<span class="token punctuation">.</span>_apply_dora<span class="token punctuation">(</span>x<span class="token punctuation">,</span> lora_A<span class="token punctuation">,</span> lora_B<span class="token punctuation">,</span> scaling<span class="token punctuation">,</span> active_adapter<span class="token punctuation">)</span>

          result <span class="token operator">=</span> result<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch_result_dtype<span class="token punctuation">)</span>

      <span class="token keyword">return</span> result<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹
å½“ç„¶è¿˜æœ‰å°‘ä¸äº†çš„mergeå’Œunmergeå‡½æ•°ï¼Œæœ€ç»ˆå¾®è°ƒå®Œæ¯•çš„æƒé‡æ˜¯å¯ä»¥é€‰æ‹©åŠ å›åŸæƒé‡æ¥æ¶ˆé™¤é¢å¤–çš„æ¨ç†å»¶è¿Ÿã€‚</p>
<p>â€‹ å…ˆçœ‹unmergeæ–¹æ³•ï¼Œå…¶å®å°±æ˜¯æŠŠåŠ åˆ°baselayerä¸Šçš„æƒé‡å‡å›å»ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">unmerge</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    This method unmerges all merged adapter layers from the base weights.
    """</span>
    <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>merged<span class="token punctuation">:</span>
        warnings<span class="token punctuation">.</span>warn<span class="token punctuation">(</span><span class="token string">"Already unmerged. Nothing to do."</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span>
    <span class="token keyword">while</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>merged_adapters<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
        active_adapter <span class="token operator">=</span> self<span class="token punctuation">.</span>merged_adapters<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> active_adapter <span class="token keyword">in</span> self<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            weight <span class="token operator">=</span> self<span class="token punctuation">.</span>get_base_layer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>weight
            delta_weight <span class="token operator">=</span> self<span class="token punctuation">.</span>get_delta_weight<span class="token punctuation">(</span>active_adapter<span class="token punctuation">)</span>
            <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>use_dora<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span><span class="token punctuation">:</span>
                <span class="token comment"># åªçœ‹è¿™ä¸€å¥å°±è¡Œäº†</span>
                weight<span class="token punctuation">.</span>data <span class="token operator">-=</span> delta_weight
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                weight_norm <span class="token operator">=</span> self<span class="token punctuation">.</span>_cache_pop<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>active_adapter<span class="token punctuation">&#125;</span></span><span class="token string">-weight_norm"</span></span><span class="token punctuation">)</span>
                dora_factor <span class="token operator">=</span> self<span class="token punctuation">.</span>lora_magnitude_vector<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span> <span class="token operator">/</span> weight_norm
                weight_orig <span class="token operator">=</span> weight<span class="token punctuation">.</span>data <span class="token operator">/</span> dora_factor<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span> delta_weight
                weight<span class="token punctuation">.</span>data <span class="token operator">=</span> weight_orig<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹
å†çœ‹mergeæ“ä½œï¼Œé¦–å…ˆéœ€è¦æ‹¿åˆ°<strong>delta_weight</strong>ï¼Œå…·ä½“çš„å†…å®¹å°±ä¸å¤åˆ¶äº†ï¼Œå¦‚æœæ˜¯fp16çš„æƒé‡åœ¨cpuä¸Šè®¡ç®—ï¼Œé‚£ä¹ˆç”±äºcpuåŸç”Ÿä¸æ”¯æŒè¯¥ç±»å‹ï¼Œæ‰€ä»¥éœ€è¦è½¬æ¢æˆfp32å†è½¬æ¢å›å»ã€‚æœ€ç»ˆçš„<span
class="math inline">\(W =
W+scalingÃ—BA\)</span>ï¼Œå¯ä»¥çœ‹åˆ°ä»å¤´åˆ°å°¾æ˜¯æ²¡æœ‰åŸæ¥çš„Wå‚ä¸å‰å‘ä¼ æ’­çš„ï¼ŒåŸæ¥çš„Wç›´æ¥è¢«å†»ç»“ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">merge</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> safe_merge<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span> adapter_names<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    adapter_names <span class="token operator">=</span> check_adapters_to_merge<span class="token punctuation">(</span>self<span class="token punctuation">,</span> adapter_names<span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token keyword">not</span> adapter_names<span class="token punctuation">:</span>
        <span class="token comment"># no adapter to merge</span>
        <span class="token keyword">return</span>

    <span class="token keyword">for</span> active_adapter <span class="token keyword">in</span> adapter_names<span class="token punctuation">:</span>
        <span class="token keyword">if</span> active_adapter <span class="token keyword">in</span> self<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            base_layer <span class="token operator">=</span> self<span class="token punctuation">.</span>get_base_layer<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> safe_merge<span class="token punctuation">:</span>
                <span class="token comment"># Note that safe_merge will be slower than the normal merge</span>
                <span class="token comment"># because of the copy operation.</span>
	<span class="token triple-quoted-string string">"""
	safe_mergeå…¶å®å°±æ˜¯æŸ¥çœ‹æ˜¯å¦æœ‰NaNå€¼ï¼Œæœ‰çš„è¯æŠ›å‡ºå¼‚å¸¸ã€‚
	å¹¶ä¸”ç”±äºå¤šå¤åˆ¶äº†ä¸€æ¬¡åŸæƒé‡ï¼Œæ‰€ä»¥æ•ˆç‡ä¼šæ›´ä½ï¼Œä»£ç ç•¥
	"""</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token comment"># è¯¥æ“ä½œæ ¸å¿ƒå°±ä¸€å¥ï¼Œè¿”å›scaling B@A</span>
                <span class="token comment"># output_tensor = transpose(weight_B @ weight_A, self.fan_in_fan_out) * self.scaling[adapter]</span>
                delta_weight <span class="token operator">=</span> self<span class="token punctuation">.</span>get_delta_weight<span class="token punctuation">(</span>active_adapter<span class="token punctuation">)</span>
                <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>use_dora<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span><span class="token punctuation">:</span>
                    base_layer<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data <span class="token operator">=</span> base_layer<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data <span class="token operator">+</span> delta_weight
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    <span class="token comment"># handle doraï¼Œæ­¤å¤„ç•¥</span>
            self<span class="token punctuation">.</span>merged_adapters<span class="token punctuation">.</span>append<span class="token punctuation">(</span>active_adapter<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹
å‰©ä¸‹çš„Embeddingï¼Œä¸€ç»´å·ç§¯å’ŒäºŒç»´å·ç§¯æ“ä½œå·®ä¸å¤šï¼Œè¿™é‡Œå°±ä¸å¤šèµ˜è¿°äº†ã€‚</p>
<h3 id="åˆå¹¶æ“ä½œ">åˆå¹¶æ“ä½œ</h3>
<p>â€‹
é¦–å…ˆéœ€è¦åˆ¤æ–­æ˜¯å¦èƒ½å¤Ÿåˆå¹¶ã€‚æœ‰ä¸¤ç§æƒ…å†µä¸‹ä¸èƒ½åˆå¹¶ï¼Œä¸€ç§æ˜¯å½“å‰ä½¿ç”¨gptqé‡åŒ–æ¨¡å‹(QLoRAå¯ä»¥)ï¼Œå¦ä¸€ç§åˆ™æ˜¯å¼€å¯äº†peftconfigä¸­çš„layer_replicationæ“ä½œã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_check_merge_allowed</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Verify that the configuration supports merging.

    Currently gptq quantization and replicated layers do not support merging.
    """</span>
    <span class="token keyword">if</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>model<span class="token punctuation">,</span> <span class="token string">"quantization_method"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">"gptq"</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Cannot merge LORA layers when the model is gptq quantized"</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>peft_config<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"layer_replication"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Cannot merge LORA layers when base model layers are replicated"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹
æœ€ç»ˆè°ƒç”¨çš„æ–¹æ³•æ˜¯<strong>merge_and_unload</strong>æ–¹æ³•ï¼Œ<code>progressbar</code>å°±æ˜¯æ˜¯å¦ç”¨tqdmæ˜¾ç¤ºè¿›è¡Œï¼Œ<code>safe_merge</code>ä¼šæ£€æŸ¥tensorä¸­æ˜¯å¦æœ‰NaNï¼Œ<code>adapter_names</code>ç”¨äºæŒ‡å®šå“ªäº›å±‚è¦åˆå¹¶ï¼Œé»˜è®¤æ˜¯æ‰€æœ‰éƒ½åˆå¹¶ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  <span class="token keyword">def</span> <span class="token function">merge_and_unload</span><span class="token punctuation">(</span>
      self<span class="token punctuation">,</span> progressbar<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span> safe_merge<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span> adapter_names<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
  <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">:</span>
<span class="token comment"># progressbarå°±æ˜¯æ˜¯å¦ç”¨tqdmæ˜¾ç¤ºè¿›è¡Œ</span>
      <span class="token keyword">return</span> self<span class="token punctuation">.</span>_unload_and_optionally_merge<span class="token punctuation">(</span>
          progressbar<span class="token operator">=</span>progressbar<span class="token punctuation">,</span> safe_merge<span class="token operator">=</span>safe_merge<span class="token punctuation">,</span> adapter_names<span class="token operator">=</span>adapter_names
      <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹
ä¸‹é¢æ˜¯æ ¸å¿ƒæ–¹æ³•**_unload_and_optionally_merge**ã€‚é¦–å…ˆä¼šåˆ¤æ–­æ˜¯å¦æ»¡è¶³è¦æ±‚ï¼Œä¹Ÿå°±æ˜¯ä¸ä½¿ç”¨gptqä¸”ä¸é‡‡å–replicatedç­–ç•¥çš„æƒ…å†µä¸‹æ‰èƒ½åˆå¹¶ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_unload_and_optionally_merge</span><span class="token punctuation">(</span>
    self<span class="token punctuation">,</span>
    merge<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    progressbar<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    safe_merge<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    adapter_names<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> merge<span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>_check_merge_allowed<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>â€‹
ç„¶åæ‹¿åˆ°targetlayerè¿›è¡Œåˆå¹¶æ“ä½œåï¼Œæ›¿æ¢æ‰åŸæ¥çš„moduleã€‚è¿™é‡Œçš„target.mergeæ˜¯LoraLayerçš„å­ç±»éƒ½å®ç°çš„mergeæ“ä½œï¼Œä¸Šé¢å·²ç»ç»™å‡ºã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>target<span class="token punctuation">,</span> <span class="token string">"base_layer"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> merge<span class="token punctuation">:</span>
        target<span class="token punctuation">.</span>merge<span class="token punctuation">(</span>safe_merge<span class="token operator">=</span>safe_merge<span class="token punctuation">,</span> adapter_names<span class="token operator">=</span>adapter_names<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>_replace_module<span class="token punctuation">(</span>parent<span class="token punctuation">,</span> target_name<span class="token punctuation">,</span> target<span class="token punctuation">.</span>get_base_layer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> target<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<h1 id="å‚è€ƒèµ„æ–™">å‚è€ƒèµ„æ–™</h1>
<p>[1] <a target="_blank" rel="noopener" href='https://arxiv.org/abs/2303.15647'>Scaling Down to Scale
Up: A Guide to Parameter-Efficient Fine-Tuning</a></p>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>æœ¬æ–‡ä½œè€…ï¼š</strong>iroha</li><li class="post-copyright-link"><strong>æœ¬æ–‡é“¾æ¥ï¼š</strong><a href="http://example.com/post/ft_survey.html" title="å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ç»¼è¿°">http://example.com/post/ft_survey.html</a></li><li class="post-copyright-license"><strong>ç‰ˆæƒå£°æ˜ï¼š</strong>æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é»˜è®¤é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> è®¸å¯åè®®ã€‚</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/post/model_load.html" rel="prev" title="Huggingfaceçš„æ¨¡å‹åŠ è½½æµç¨‹"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">Huggingfaceçš„æ¨¡å‹åŠ è½½æµç¨‹</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/post/backtrace.html" rel="next" title="å›æº¯ä¹‹ç»„åˆé—®é¢˜"><span class="post-nav-text">å›æº¯ä¹‹ç»„åˆé—®é¢˜</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 â€“ 2025 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> iroha</span></div><div class="powered"><span>ç”± <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> é©±åŠ¨ v6.3.0</span><span class="footer-separator">|</span><span>ä¸»é¢˜ - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.11</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>