<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="iroha"><meta name="copyright" content="iroha"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ç»¼è¿° | iroha</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...undefined?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"example.com","root":"/","title":"ä¸æƒ³æ‘†çƒ‚","version":"1.10.9","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"æœç´¢...","empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹: ${query}","hits":"æ‰¾åˆ° ${hits} æ¡ç»“æœ","hits_time":"æ‰¾åˆ° ${hits} æ¡ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><meta name="description" content="èƒŒæ™¯â€‹        éšç€æ¨¡å‹çš„å‚æ•°è¶Šæ¥è¶Šå¤§ï¼Œä¸ºæ¯ä¸€ä¸ªä»»åŠ¡ä»å¤´è®­ç»ƒä¸€ä¸ªæ¨¡å‹è¶Šæ¥è¶Šå›°éš¾ï¼Œè€Œå‚æ•°é‡çš„å¢å¤§ä¹Ÿè®©æ¨¡å‹ç†è§£äº†æ›´æ·±å±‚çš„è¯­ä¹‰ä¿¡æ¯ã€‚éšç€BERTçš„å‡ºç°ï¼Œé¢„è®­ç»ƒ+å¾®è°ƒçš„æ–¹å¼èµ°ä¸Šå†å²èˆå°ã€‚ â€‹        ä¸ºä»€ä¹ˆä¸é€‰æ‹©å…¨å‚æ•°å¾®è°ƒå‘¢ï¼Ÿæœ€ä¸»è¦çš„åŸå› è¿˜æ˜¯æˆæœ¬å¤ªé«˜ï¼Œåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šæ„ˆå‘å›°éš¾ã€‚ä»¥GPT2-1.5Bä¸ºä¾‹ï¼Œè‹¥å‚æ•°ä¸æ¢¯åº¦ä»¥fp16ä¿å­˜ï¼Œæ˜¾å­˜éœ€è¦ä¿å­˜3+3 &#x3D; 6Bçš„æ•°æ®ï¼Œè‹¥ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œé‚£ä¹ˆè¿˜éœ€">
<meta property="og:type" content="article">
<meta property="og:title" content="å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ç»¼è¿°">
<meta property="og:url" content="http://example.com/post/ft_survey.html">
<meta property="og:site_name" content="iroha">
<meta property="og:description" content="èƒŒæ™¯â€‹        éšç€æ¨¡å‹çš„å‚æ•°è¶Šæ¥è¶Šå¤§ï¼Œä¸ºæ¯ä¸€ä¸ªä»»åŠ¡ä»å¤´è®­ç»ƒä¸€ä¸ªæ¨¡å‹è¶Šæ¥è¶Šå›°éš¾ï¼Œè€Œå‚æ•°é‡çš„å¢å¤§ä¹Ÿè®©æ¨¡å‹ç†è§£äº†æ›´æ·±å±‚çš„è¯­ä¹‰ä¿¡æ¯ã€‚éšç€BERTçš„å‡ºç°ï¼Œé¢„è®­ç»ƒ+å¾®è°ƒçš„æ–¹å¼èµ°ä¸Šå†å²èˆå°ã€‚ â€‹        ä¸ºä»€ä¹ˆä¸é€‰æ‹©å…¨å‚æ•°å¾®è°ƒå‘¢ï¼Ÿæœ€ä¸»è¦çš„åŸå› è¿˜æ˜¯æˆæœ¬å¤ªé«˜ï¼Œåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šæ„ˆå‘å›°éš¾ã€‚ä»¥GPT2-1.5Bä¸ºä¾‹ï¼Œè‹¥å‚æ•°ä¸æ¢¯åº¦ä»¥fp16ä¿å­˜ï¼Œæ˜¾å­˜éœ€è¦ä¿å­˜3+3 &#x3D; 6Bçš„æ•°æ®ï¼Œè‹¥ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œé‚£ä¹ˆè¿˜éœ€">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20240408231535224.png">
<meta property="og:image" content="http://example.com/images/image-20240410215838916.png">
<meta property="og:image" content="http://example.com/images/image-20240411005445372.png">
<meta property="article:published_time" content="2024-04-08T04:00:00.000Z">
<meta property="article:modified_time" content="2024-04-12T19:32:20.406Z">
<meta property="article:author" content="iroha">
<meta property="article:tag" content="å¤§æ¨¡å‹">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20240408231535224.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="æ–‡ç« ç›®å½•"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="ç«™ç‚¹æ¦‚è§ˆ"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="iroha"><img width="96" loading="lazy" src="/yun.png" alt="iroha"><span class="site-author-status" title="æ°¸è¿œç›¸ä¿¡ç¾å¥½çš„äº‹æƒ…å³å°†å‘ç”Ÿ">ğŸ˜Š</span></a><div class="site-author-name"><a href="/about/">iroha</a></div><span class="site-name">iroha</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="é¦–é¡µ"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="å½’æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">13</span></a></div><div class="site-state-item"><a href="/categories/" title="åˆ†ç±»"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">4</span></a></div><div class="site-state-item"><a href="/tags/" title="æ ‡ç­¾"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">2</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="æ–‡æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="å‹é“¾" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">1.</span> <span class="toc-text">èƒŒæ™¯</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Freeze"><span class="toc-number">2.</span> <span class="toc-text">Freeze</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Bitfit"><span class="toc-number">3.</span> <span class="toc-text">Bitfit</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Prompt-tuning"><span class="toc-number">4.</span> <span class="toc-text">Prompt-tuning</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Lora"><span class="toc-number">5.</span> <span class="toc-text">Lora</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-number">5.1.</span> <span class="toc-text">åŸç†</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E9%80%9F%E8%AF%BB"><span class="toc-number">5.2.</span> <span class="toc-text">æºç é€Ÿè¯»</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#get-peft-model"><span class="toc-number">5.2.1.</span> <span class="toc-text">get_peft_model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PeftModel"><span class="toc-number">5.2.2.</span> <span class="toc-text">PeftModel</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LoraModel"><span class="toc-number">5.2.3.</span> <span class="toc-text">LoraModel</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LoraLayer"><span class="toc-number">5.2.4.</span> <span class="toc-text">LoraLayer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Linear"><span class="toc-number">5.2.5.</span> <span class="toc-text">Linear</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%88%E5%B9%B6%E6%93%8D%E4%BD%9C"><span class="toc-number">5.2.6.</span> <span class="toc-text">åˆå¹¶æ“ä½œ</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">6.</span> <span class="toc-text">å‚è€ƒèµ„æ–™</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://example.com/post/ft_survey.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="iroha"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="iroha"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ç»¼è¿°</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="åˆ›å»ºæ—¶é—´ï¼š2024-04-08 12:00:00" itemprop="dateCreated datePublished" datetime="2024-04-08T12:00:00+08:00">2024-04-08</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="ä¿®æ”¹æ—¶é—´ï¼š2024-04-13 03:32:20" itemprop="dateModified" datetime="2024-04-13T03:32:20+08:00">2024-04-13</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">NLP</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">å¤§æ¨¡å‹</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><h1 id="èƒŒæ™¯"><a href="#èƒŒæ™¯" class="headerlink" title="èƒŒæ™¯"></a>èƒŒæ™¯</h1><p>â€‹        éšç€æ¨¡å‹çš„å‚æ•°è¶Šæ¥è¶Šå¤§ï¼Œä¸ºæ¯ä¸€ä¸ªä»»åŠ¡ä»å¤´è®­ç»ƒä¸€ä¸ªæ¨¡å‹è¶Šæ¥è¶Šå›°éš¾ï¼Œè€Œå‚æ•°é‡çš„å¢å¤§ä¹Ÿè®©æ¨¡å‹ç†è§£äº†æ›´æ·±å±‚çš„è¯­ä¹‰ä¿¡æ¯ã€‚éšç€BERTçš„å‡ºç°ï¼Œé¢„è®­ç»ƒ+å¾®è°ƒçš„æ–¹å¼èµ°ä¸Šå†å²èˆå°ã€‚</p>
<p>â€‹        ä¸ºä»€ä¹ˆä¸é€‰æ‹©å…¨å‚æ•°å¾®è°ƒå‘¢ï¼Ÿæœ€ä¸»è¦çš„åŸå› è¿˜æ˜¯æˆæœ¬å¤ªé«˜ï¼Œåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šæ„ˆå‘å›°éš¾ã€‚ä»¥GPT2-1.5Bä¸ºä¾‹ï¼Œè‹¥å‚æ•°ä¸æ¢¯åº¦ä»¥fp16ä¿å­˜ï¼Œæ˜¾å­˜éœ€è¦ä¿å­˜3+3 = 6Bçš„æ•°æ®ï¼Œè‹¥ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œé‚£ä¹ˆè¿˜éœ€è¦ä¿å­˜fp32çš„æ¨¡å‹å‚æ•°å¤‡ä»½ï¼Œmomentumä¸varianceï¼Œåˆéœ€è¦6+6+6 = 18Gï¼Œæ€»å…±éœ€è¦24Gã€‚å†åŠ ä¸Šå…¶ä»–çŠ¶æ€ï¼Œå¦‚activationï¼Œbufferï¼Œè¿˜æœ‰æ˜¾å­˜ç¢ç‰‡æ— æ³•åˆ©ç”¨ï¼Œå®é™…ä¸Šéœ€è¦çš„æ˜¾å­˜æ˜¯å¤§äº24Gçš„ã€‚</p>
<p>â€‹        å…¨å‚æ•°å¾®è°ƒè¿˜æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå½“LLMå°è¯•å­¦ä¹ å¤šä¸ªè¿ç»­ä»»åŠ¡æ—¶ï¼Œå®¹æ˜“å¿˜è®°ä¹‹å‰å­¦åˆ°çš„ä¸œè¥¿ï¼Œä¹Ÿå°±æ˜¯â€œç¾éš¾æ€§é—å¿˜â€ã€‚å¦‚ä½•åœ¨ä¿ç•™å…ˆå‰çŸ¥è¯†çš„åŸºç¡€ä¸Šå¢é‡åœ°å¢å¼ºLLMï¼Œå³è¿›è¡ŒæŒç»­å­¦ä¹ ï¼Œè‡³å…³é‡è¦ã€‚ç®€å•æ¥è¯´ï¼Œå…¨é‡å¾®è°ƒæœ‰overfittingï¼Œç¾éš¾æ€§é—å¿˜ï¼Œæˆæœ¬é«˜çš„é—®é¢˜ã€‚å› æ­¤ï¼Œé«˜æ•ˆå¾®è°ƒæ¨¡å‹çš„å‚æ•°å°±æˆäº†ä¸€ä¸ªæ–°æ–¹å‘ï¼ŒPEFT(Parameter-Efficient Fine-tuning)åº”è¿è€Œç”Ÿã€‚</p>
<p>â€‹        å¾®è°ƒä¸€èˆ¬åªæ›´æ”¹æ¨¡å‹çš„éƒ¨åˆ†å‚æ•°ï¼Œè°ƒæ•´å“ªäº›å‚æ•°ï¼Œå¦‚ä½•è°ƒæ•´åˆ™å¯¹åº”äº†ä¸åŒæ–¹æ³•ã€‚ä¸€èˆ¬ä»ä¸‹æ¸¸ä»»åŠ¡å‡ºå‘ï¼Œæ¥å†³å®šä¸‹æ¸¸åº”è¯¥å¦‚ä½•æ·»åŠ å‚æ•°ã€‚</p>
<p>â€‹        ä¸‹é¢æˆ‘ä»¬ä»¥transformersåº“ä¸­çš„BERTä¸ºä¾‹ã€‚æˆ‘ä»¬éƒ½çŸ¥é“ï¼ŒBERTçš„é¢„è®­ç»ƒæ˜¯MLMä¸NSPï¼Œæ‰€ä»¥ä¸‹æ¸¸ä»»åŠ¡è‡ªç„¶æœ‰è¿™ä¸¤è€…ï¼Œä¹Ÿå°±æ˜¯<strong>ä¸‹ä¸€å¥é¢„æµ‹</strong>å’Œ<strong>å®Œå½¢å¡«ç©º</strong>ã€‚æ‰€ä»¥è¿™æ‰åº”è¯¥æ˜¯BERTçš„åŸç”Ÿä¸‹æ¸¸ä»»åŠ¡ã€‚</p>
<p>â€‹        é¦–å…ˆçœ‹NSPï¼Œæ¨¡å‹çš„ç»“æ„å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># assert logits[0, 0] &lt; logits[0, 1]  # next sentence was random
class BertForNextSentencePrediction(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)

        self.bert = BertModel(config)
        self.cls = BertOnlyNSPHead(config)

        # Initialize weights and apply final processing
        self.post_init()

        
class BertOnlyNSPHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.seq_relationship = nn.Linear(config.hidden_size, 2)

    def forward(self, pooled_output):
        seq_relationship_score = self.seq_relationship(pooled_output)
        return seq_relationship_score</code></pre>
<p>â€‹        å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äºNSPä»»åŠ¡ï¼Œå°±æ˜¯åœ¨Modelçš„åé¢æ‹¼ä¸€ä¸ªLinearå±‚ï¼Œå°†768ç»´æ˜ å°„åˆ°2ç»´ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ã€‚</p>
<p>â€‹        å†çœ‹SequenceClassificationä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯åºåˆ—åˆ†ç±»ã€‚å®é™…ä¸Šä¹Ÿæ˜¯åœ¨æœ€åå¡«å……äº†ä¸€å±‚Linearå±‚ç”¨äºåˆ†ç±»ã€‚ä¸NSPçš„åŒºåˆ«åœ¨äºpoolerå±‚ä¼šå…ˆç»è¿‡ä¸€æ¬¡dropoutï¼Œdropoutçš„æ¦‚ç‡å¯ä»¥åœ¨configä¸­è®¾ç½®ï¼Œé»˜è®¤ä¸º0.1ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class BertForSequenceClassification(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.config = config

        self.bert = BertModel(config)
        classifier_dropout = (
            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob
        )
        self.dropout = nn.Dropout(classifier_dropout)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)</code></pre>
<p>â€‹        <img src="../images/image-20240408231535224.png" alt="image-20240408231535224" loading="lazy"></p>
<p>â€‹        PEFTæŒ‰æ ¸å¿ƒæ€æƒ³å¯ä»¥åˆ†ä¸ºä¸‰ç±»ï¼š</p>
<ol>
<li>æ·»åŠ ä¸€äº›å‚æ•°é‡å°çš„å±‚ï¼Œåªå¾®è°ƒè¿™äº›å±‚ï¼Œæœ€å…¸å‹çš„å°±æ˜¯LSTã€‚</li>
<li>é€‰æ‹©æŸäº›å±‚ï¼Œæˆ–å±‚ä¸­çš„æŸä¸€éƒ¨åˆ†è¿›è¡Œå¾®è°ƒï¼Œæœ€å…¸å‹çš„æ˜¯BitFitï¼Œåªå¯¹biasè¿›è¡Œå¾®è°ƒã€‚</li>
<li>é‡å‚æ•°åŒ–ï¼Œä¹Ÿç®—æ˜¯å¢åŠ ä¸€éƒ¨åˆ†å‚æ•°ï¼Œä½†æœ€ååŠ å›åŸå‚æ•°çš„å¯¹åº”éƒ¨åˆ†        </li>
</ol>
<h1 id="Freeze"><a href="#Freeze" class="headerlink" title="Freeze"></a>Freeze</h1><p>â€‹        å†»ç»“æŸäº›å‚æ•°æ˜¯æœ€å®¹æ˜“æƒ³åˆ°çš„æ–¹æ³•ï¼Œä»…ä»…è°ƒæ•´æŸäº›æœªè¢«å†»ç»“çš„å‚æ•°å°±å¯ä»¥å‡å°‘å¤§é‡æ˜¾å­˜å ç”¨ï¼Œä½†freezeæ–¹æ³•å¤§å¤§é™ä½äº†æ¨¡å‹çš„çµæ´»æ€§ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">for name ,param in model.parameters():
	if not any (n in name for n in [layers_name1,layer_name2...])
		param.requires_grad = False</code></pre>
<h1 id="Bitfit"><a href="#Bitfit" class="headerlink" title="Bitfit"></a>Bitfit</h1><p>å‡ºè‡ª<strong>BitFit: Simple Parameter-efficient Fine-tuning or Transformer-based Masked Language-models</strong>ã€‚</p>
<p>pass</p>
<h1 id="Prompt-tuning"><a href="#Prompt-tuning" class="headerlink" title="Prompt-tuning"></a>Prompt-tuning</h1><p>pass</p>
<h1 id="Lora"><a href="#Lora" class="headerlink" title="Lora"></a>Lora</h1><h2 id="åŸç†"><a href="#åŸç†" class="headerlink" title="åŸç†"></a>åŸç†</h2><p><img src="../images/image-20240410215838916.png" alt="image-20240410215838916" loading="lazy"></p>
<p>â€‹        åœ¨åŸå§‹æƒé‡ä¸­å¹¶ä¸Šä¸€ä¸ªæ—è·¯çš„åˆ†æ”¯ï¼Œä»¥Linearå±‚ä¸ºä¾‹å­ï¼ŒåŸæœ¬<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="11.481ex" height="1.927ex" role="img" focusable="false" viewBox="0 -694 5074.8 851.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(853.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(1909.6,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="TeXAtom" transform="translate(977,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(520,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1298,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mi" transform="translate(4222.8,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g></g></g></svg></mjx-container>ï¼Œ<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="2.371ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 1048 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container>æ˜¯æƒé‡ï¼Œæ—è·¯æœ‰ä¸¤ä¸ªä½ç§©çš„çŸ©é˜µï¼Œå…¶ä¸­<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="15.887ex" height="2.452ex" role="img" focusable="false" viewBox="0 -833.9 7022.2 1083.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="TeXAtom" transform="translate(783,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(451,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1229,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2348.2,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(3404,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(4292,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(4681,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(5181,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msup" transform="translate(5625.7,0)"><g data-mml-node="mi"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mn" transform="translate(604,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(6633.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>ï¼Œä¹Ÿå°±æ˜¯ä»¥é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–ï¼Œè€Œ<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="4.703ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 2078.7 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="TeXAtom" transform="translate(792,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(520,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1298,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></svg></mjx-container>çŸ©é˜µåˆ™ä»¥å…¨0åˆå§‹åŒ–ï¼Œå…¶ä¸­ç»´åº¦rè¿œå°äºdå’Œkã€‚<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="10.687ex" height="1.805ex" role="img" focusable="false" viewBox="0 -716 4723.6 798"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6E5" d="M574 715L582 716Q589 716 595 716Q612 716 616 714Q621 712 621 709Q622 707 705 359T788 8Q786 5 785 3L781 0H416Q52 0 50 2T48 6Q48 9 305 358T567 711Q572 712 574 715ZM599 346L538 602L442 474Q347 345 252 217T157 87T409 86T661 88L654 120Q646 151 629 220T599 346Z"></path></g><g data-mml-node="mi" transform="translate(833,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(2158.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(3214.6,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(3973.6,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g></g></g></svg></mjx-container> ,æœ€ç»ˆçš„æƒé‡ä¸º<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="9.392ex" height="1.805ex" role="img" focusable="false" viewBox="0 -716 4151.4 798"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(1270.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(2270.4,0)"><path data-c="1D6E5" d="M574 715L582 716Q589 716 595 716Q612 716 616 714Q621 712 621 709Q622 707 705 359T788 8Q786 5 785 3L781 0H416Q52 0 50 2T48 6Q48 9 305 358T567 711Q572 712 574 715ZM599 346L538 602L442 474Q347 345 252 217T157 87T409 86T661 88L654 120Q646 151 629 220T599 346Z"></path></g><g data-mml-node="mi" transform="translate(3103.4,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container>ã€‚å¯¹äº<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="4.256ex" height="1.67ex" role="img" focusable="false" viewBox="0 -716 1881 738"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6E5" d="M574 715L582 716Q589 716 595 716Q612 716 616 714Q621 712 621 709Q622 707 705 359T788 8Q786 5 785 3L781 0H416Q52 0 50 2T48 6Q48 9 305 358T567 711Q572 712 574 715ZM599 346L538 602L442 474Q347 345 252 217T157 87T409 86T661 88L654 120Q646 151 629 220T599 346Z"></path></g><g data-mml-node="mi" transform="translate(833,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container>å¯ä»¥ä½¿ç”¨ä¸€ä¸ªÎ±å‚æ•°æ¥æ§åˆ¶å€æ•°ï¼Œå³<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.798ex;" xmlns="http://www.w3.org/2000/svg" width="11.412ex" height="2.418ex" role="img" focusable="false" viewBox="0 -716 5044 1068.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(1270.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mfrac" transform="translate(2270.4,0)"><g data-mml-node="mi" transform="translate(220,394) scale(0.707)"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path></g><g data-mml-node="mi" transform="translate(286.8,-345) scale(0.707)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><rect width="652.5" height="60" x="120" y="220"></rect></g><g data-mml-node="mi" transform="translate(3163,0)"><path data-c="1D6E5" d="M574 715L582 716Q589 716 595 716Q612 716 616 714Q621 712 621 709Q622 707 705 359T788 8Q786 5 785 3L781 0H416Q52 0 50 2T48 6Q48 9 305 358T567 711Q572 712 574 715ZM599 346L538 602L442 474Q347 345 252 217T157 87T409 86T661 88L654 120Q646 151 629 220T599 346Z"></path></g><g data-mml-node="mi" transform="translate(3996,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container>ã€‚Bä¸ºå‡ç»´çŸ©é˜µï¼ŒAä¸ºé™ç»´çŸ©é˜µã€‚å®é™…ä¸Šï¼ŒLoRAä¸€èˆ¬ç”¨äºDenseå±‚ã€‚</p>
<p>â€‹        å¯¹äºæ¢¯åº¦è®¡ç®—ï¼Œå€Ÿç”¨çŸ¥ä¹ä¸ŠCodeLearnerç­”ä¸»çš„ä¸€å¼ å›¾ã€‚</p>
<p>â€‹        <img src="../images/image-20240411005445372.png" alt="image-20240411005445372" loading="lazy"></p>
<p>â€‹        æ‰€ä»¥ï¼Œåœ¨å¾®è°ƒæ—¶ï¼ŒæŸä¸€Loraå±‚åå‘ä¼ æ’­ä¸­çš„æ¢¯åº¦è®¡ç®—é‡æ˜¯è¦æ›´å¤šçš„ï¼Œä½†ç”±äºrè¿œå°äºåŸæƒé‡çš„ç»´åº¦då’Œkï¼Œæ‰€ä»¥ç›¸å¯¹äºå…¨é‡å¾®è°ƒä¿å­˜çš„æ¢¯åº¦å€¼å°±å°‘ã€‚åŒæ—¶ï¼ŒåŸè®ºæ–‡ä¸­ä¹Ÿåªå¯¹Transformer Blockä¸­çš„Q,K,Vè¿›è¡Œäº†Loraå¤„ç†ã€‚è‹¥rç­‰äºkï¼Œé‚£ä¹ˆæ­¤æ—¶å¯ä»¥ç­‰ä»·äºå…¨å‚æ•°å¾®è°ƒã€‚</p>
<p>â€‹        å¯¹äºåˆå§‹åŒ–é—®é¢˜ï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¿è¯æœ€å¼€å§‹çš„<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="8.404ex" height="1.805ex" role="img" focusable="false" viewBox="0 -716 3714.6 798"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6E5" d="M574 715L582 716Q589 716 595 716Q612 716 616 714Q621 712 621 709Q622 707 705 359T788 8Q786 5 785 3L781 0H416Q52 0 50 2T48 6Q48 9 305 358T567 711Q572 712 574 715ZM599 346L538 602L442 474Q347 345 252 217T157 87T409 86T661 88L654 120Q646 151 629 220T599 346Z"></path></g><g data-mml-node="mi" transform="translate(833,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(2158.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(3214.6,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></svg></mjx-container>ï¼Œæ‰€ä»¥éœ€è¦ABçš„å…¶ä¸­ä¸€è€…ä¸º0ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬çœ‹ä¸Šå›¾å¸¦ä¸ŠLoRAåçš„åå‘ä¼ æ’­æ¢¯åº¦è®¡ç®—ï¼Œè‹¥Aä¸º0ï¼Œé‚£ä¹ˆæ¢¯åº¦å°±ä¼šä¸€ç›´ä¸º0ï¼Œ<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="4.256ex" height="1.67ex" role="img" focusable="false" viewBox="0 -716 1881 738"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6E5" d="M574 715L582 716Q589 716 595 716Q612 716 616 714Q621 712 621 709Q622 707 705 359T788 8Q786 5 785 3L781 0H416Q52 0 50 2T48 6Q48 9 305 358T567 711Q572 712 574 715ZM599 346L538 602L442 474Q347 345 252 217T157 87T409 86T661 88L654 120Q646 151 629 220T599 346Z"></path></g><g data-mml-node="mi" transform="translate(833,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container>å°±ä¸ä¼šæ›´æ–°ã€‚</p>
<p>â€‹        åŸæ–‡ä¸­æåˆ°äº†LoRAçš„Limitationï¼Œå¦‚æœé€‰æ‹©å°†Aå’ŒBçŸ©é˜µå¸æ”¶(åˆå¹¶)åˆ°WåŸå§‹æƒé‡çŸ©é˜µä¸­,ä»¥æ¶ˆé™¤é¢å¤–çš„æ¨ç†å»¶è¿Ÿ,é‚£ä¹ˆåœ¨å•ä¸ªå‰å‘ä¼ é€’ä¸­ä¸ºä¸åŒä»»åŠ¡çš„ä¸åŒAå’ŒBæ‰¹é‡è¾“å…¥æ•°æ®å°±å˜å¾—ä¸ç›´è§‚äº†ã€‚æˆ‘æƒ³åŸæ–‡çš„æ„æ€æ˜¯ä¸€ä¸ªbatché‡Œå¯èƒ½æœ‰ä¸åŒçš„ä»»åŠ¡ï¼Œé‚£ä¹ˆä¸åŒçš„ä»»åŠ¡åº”è¯¥ç”¨ä¸åŒçš„æƒé‡ï¼Œé‚£ä¹ˆæœ€å¥½æ˜¯ä¸å°†<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="4.256ex" height="1.67ex" role="img" focusable="false" viewBox="0 -716 1881 738"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6E5" d="M574 715L582 716Q589 716 595 716Q612 716 616 714Q621 712 621 709Q622 707 705 359T788 8Q786 5 785 3L781 0H416Q52 0 50 2T48 6Q48 9 305 358T567 711Q572 712 574 715ZM599 346L538 602L442 474Q347 345 252 217T157 87T409 86T661 88L654 120Q646 151 629 220T599 346Z"></path></g><g data-mml-node="mi" transform="translate(833,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container>åˆå¹¶åˆ°åŸå§‹æƒé‡ï¼Œé’ˆå¯¹ä¸åŒä»»åŠ¡æ¥åŠ¨æ€é€‰æ‹©<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="4.256ex" height="1.67ex" role="img" focusable="false" viewBox="0 -716 1881 738"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6E5" d="M574 715L582 716Q589 716 595 716Q612 716 616 714Q621 712 621 709Q622 707 705 359T788 8Q786 5 785 3L781 0H416Q52 0 50 2T48 6Q48 9 305 358T567 711Q572 712 574 715ZM599 346L538 602L442 474Q347 345 252 217T157 87T409 86T661 88L654 120Q646 151 629 220T599 346Z"></path></g><g data-mml-node="mi" transform="translate(833,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container>ï¼Œè¿™éœ€è¦åœ¨æ¨ç†é€Ÿåº¦ä¸Šåšå–èˆã€‚</p>
<h2 id="æºç é€Ÿè¯»"><a href="#æºç é€Ÿè¯»" class="headerlink" title="æºç é€Ÿè¯»"></a>æºç é€Ÿè¯»</h2><p>â€‹        ä¸ºäº†æ–¹ä¾¿ç†è§£ï¼Œæˆ‘ä»¬ä»peft0.10.0çš„å®˜æ–¹ç¤ºä¾‹å‡ºå‘ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from transformers import AutoModelForSeq2SeqLM
from peft import get_peft_config, get_peft_model, LoraConfig, TaskType
model_name_or_path = "bigscience/mt0-large"
tokenizer_name_or_path = "bigscience/mt0-large"

peft_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
)

model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
#ç­‰ä»·äº model = PeftModelForSeq2SeqLM(model,peft_config)
#ä¹Ÿç­‰ä»·äºmodel = lora_model = LoraModel(model, config, "default")
model = get_peft_model(model, peft_config) 

model.print_trainable_parameters()
"trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282"</code></pre>
<p>â€‹        é¦–å…ˆä»LoraConfigå‡ºå‘ï¼Œè¿™é‡Œçš„task_typeæ˜¯PeftConfigçš„å‚æ•°ï¼Œå…¶ä½™åˆ™æ˜¯LoraConfigçš„å‚æ•°ï¼Œè¿™äº›å‚æ•°åœ¨ä¸Šæ–‡çš„åŸç†ä¸­éƒ½æœ‰æåˆ°ã€‚ä¸»è¦è¿˜æ˜¯æ ¹æ®ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡è¿”å›ä¸åŒçš„æ¨¡å‹ã€‚è¿™é‡Œä¹Ÿå¯ä»¥ç›´æ¥ç”¨get_peft_configå‡½æ•°æ¥è¯»å–ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">peft_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
)
----
config = {
    "task_type":"SEQ_2_SEQ_LM",
    "peft_type":"LORA",
    "inference_mode":False,
    "r":8,
    "lora_alpha":32,
    "lora_dropout":0.1
}

another_config = get_peft_config(config)</code></pre>
<h3 id="get-peft-model"><a href="#get-peft-model" class="headerlink" title="get_peft_model"></a>get_peft_model</h3><p>â€‹        æ¥ä¸‹æ¥çœ‹get_peft_modelã€‚ä¼ å…¥çš„å‚æ•°æœ‰æ¨¡å‹å’Œå¯¹åº”çš„configï¼Œæœ€åä¼šè¿”å›ä¸€ä¸ªPeftModelå®ä¾‹ã€‚è¿”å›çš„å®ä¾‹ç±»å‹ä¼šæ ¹æ®ä¼ å…¥çš„configæ¥ç¡®å®šã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model = get_peft_model(model, peft_config)
---------mapping.py----------------
def get_peft_model(
    model: PreTrainedModel, peft_config: PeftConfig, adapter_name: str = "default", mixed: bool = False
) -&gt; PeftModel | PeftMixedModel:
    """
    Returns a Peft model object from a model and a config.

    Args:
        model ([`transformers.PreTrainedModel`]):
            Model to be wrapped.
        peft_config ([`PeftConfig`]):
            Configuration object containing the parameters of the Peft model.
        adapter_name (`str`, `optional`, defaults to `"default"`):
            The name of the adapter to be injected, if not provided, the default adapter name is used ("default").
        mixed (`bool`, `optional`, defaults to `False`):
            Whether to allow mixing different (compatible) adapter types.
    """
    model_config = getattr(model, "config", {"model_type": "custom"})
    if hasattr(model_config, "to_dict"):
        model_config = model_config.to_dict()

    peft_config.base_model_name_or_path = model.__dict__.get("name_or_path", None)

    if mixed:
        return PeftMixedModel(model, peft_config, adapter_name=adapter_name)

    if peft_config.task_type not in MODEL_TYPE_TO_PEFT_MODEL_MAPPING.keys() and not peft_config.is_prompt_learning:
        return PeftModel(model, peft_config, adapter_name=adapter_name)

    if peft_config.is_prompt_learning:
        peft_config = _prepare_prompt_learning_config(peft_config, model_config)
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name=adapter_name)
----------------------
MODEL_TYPE_TO_PEFT_MODEL_MAPPING: dict[str, PeftModel] = {
    "SEQ_CLS": PeftModelForSequenceClassification,
    "SEQ_2_SEQ_LM": PeftModelForSeq2SeqLM,
    "CAUSAL_LM": PeftModelForCausalLM,
    "TOKEN_CLS": PeftModelForTokenClassification,
    "QUESTION_ANS": PeftModelForQuestionAnswering,
    "FEATURE_EXTRACTION": PeftModelForFeatureExtraction,
}
</code></pre>
<p>â€‹        å…³é”®åœ¨äºè¿™ä¸¤å¥ã€‚å¦‚æœä»»åŠ¡ç±»å‹ä¸åœ¨æ”¯æŒçš„ç‰¹å®šä»»åŠ¡ä¸­ï¼Œè¿”å›PeftModelï¼Œå¦åˆ™è¿”å›å¯¹åº”ä»»åŠ¡ç±»å‹çš„Modelï¼Œè¿™äº›Modelç»§æ‰¿äº†PeftModelã€‚è‹¥æ˜¯æç¤ºå­¦ä¹ ç±»å‹çš„ï¼Œå¦‚prompt-tuningï¼Œåˆ™éœ€è¦é¢å¤–çš„configä¿¡æ¯ï¼Œå¦‚éšè—å±‚çš„æ•°é‡,å¯èƒ½çš„é”®åŒ…æ‹¬<code>num_hidden_layers</code>ã€<code>num_layers</code>ã€<code>n_layer</code>ï¼Œ å¦‚æœæ— æ³•æ‰¾åˆ°,åˆ™éœ€è¦åœ¨peft_configä¸­æ‰‹åŠ¨æŒ‡å®šnum_layersï¼Œè¿™ä¸ªå‚æ•°æŒ‡å®šäº†promptå°†è¢«æ³¨å…¥åˆ°æ¨¡å‹çš„å“ªäº›å±‚ã€‚è¿˜éœ€è¦æ³¨æ„åŠ›çš„å¤´æ•°ï¼Œencoderéšè—å±‚çš„å¤§å°å’Œtokenembeddingçš„ç»´åº¦ç­‰ç­‰ã€‚è¿™äº›å‚æ•°è‹¥ä»æ¨¡å‹çš„configä¸­æ‰¾ä¸åˆ°ï¼Œåˆ™éœ€è¦åœ¨peftconfigä¸­è‡ªè¡ŒæŒ‡å®šã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">    if peft_config.task_type not in MODEL_TYPE_TO_PEFT_MODEL_MAPPING.keys() and not peft_config.is_prompt_learning:
        return PeftModel(model, peft_config, adapter_name=adapter_name)

    if peft_config.is_prompt_learning:
        peft_config = _prepare_prompt_learning_config(peft_config, model_config)
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name=adapter_name)
-------------------
def _prepare_prompt_learning_config(peft_config, model_config):
    if peft_config.num_layers is None:
        if "num_hidden_layers" in model_config:
            num_layers = model_config["num_hidden_layers"]
        elif "num_layers" in model_config:
            num_layers = model_config["num_layers"]
        elif "n_layer" in model_config:
            num_layers = model_config["n_layer"]
        else:
            raise ValueError("Please specify `num_layers` in `peft_config`")
        peft_config.num_layers = num_layers

    if peft_config.token_dim is None:
        if "hidden_size" in model_config:
            token_dim = model_config["hidden_size"]
        elif "n_embd" in model_config:
            token_dim = model_config["n_embd"]
        elif "d_model" in model_config:
            token_dim = model_config["d_model"]
        else:
            raise ValueError("Please specify `token_dim` in `peft_config`")
        peft_config.token_dim = token_dim

    if peft_config.num_attention_heads is None:
        if "num_attention_heads" in model_config:
            num_attention_heads = model_config["num_attention_heads"]
        elif "n_head" in model_config:
            num_attention_heads = model_config["n_head"]
        elif "num_heads" in model_config:
            num_attention_heads = model_config["num_heads"]
        elif "encoder_attention_heads" in model_config:
            num_attention_heads = model_config["encoder_attention_heads"]
        else:
            raise ValueError("Please specify `num_attention_heads` in `peft_config`")
        peft_config.num_attention_heads = num_attention_heads

    if getattr(peft_config, "encoder_hidden_size", None) is None:
        setattr(peft_config, "encoder_hidden_size", peft_config.token_dim)

    return peft_config</code></pre>
<p>â€‹        å›åˆ°é‡ç‚¹ï¼Œæˆ‘ä»¬ä»¥Seq2SeqModelçš„ä»£ç ä¸ºä¾‹ï¼Œç›¸å¯¹äºPeftModelï¼Œæ•´ä½“ç»“æ„ä¸€æ ·ï¼Œåªæ˜¯å¤šäº†ä¸¤ä¸ªå‚æ•°å˜é‡ï¼Œå…¶ä¸­prepare_inputs_for_generationå‡½æ•°éœ€è¦ç”Ÿæˆæ¨¡å‹åœ¨generateæ–¹æ³•ä¸­è‡ªè¡Œå®ç°ï¼Œbase_model_prepare_encoder_decoder_kwargs_for_generationå˜é‡åˆ™ä»base_modelä¸­æå–å‡ºä¸€äº›ç”Ÿæˆæ—¶çš„å‚æ•°ï¼Œå¦‚æ˜¯å¦è¦ä½¿ç”¨cacheçš„use_caheï¼Œå°†encoderçš„å‚æ•°å°è£…è¿›model_kwargsè¿”å›ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class PeftModelForSeq2SeqLM(PeftModel):
	def __init__(self, model: torch.nn.Module, peft_config: PeftConfig, adapter_name: str = "default") -&gt; None:
        super().__init__(model, peft_config, adapter_name)
        self.base_model_prepare_inputs_for_generation = self.base_model.prepare_inputs_for_generation
        self.base_model_prepare_encoder_decoder_kwargs_for_generation = (
            self.base_model._prepare_encoder_decoder_kwargs_for_generation
        )
</code></pre>
<h3 id="PeftModel"><a href="#PeftModel" class="headerlink" title="PeftModel"></a>PeftModel</h3><p>â€‹        è¿˜æ˜¯å…ˆè´´å‡ºæ•´ä½“çš„åˆå§‹åŒ–æºç ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class PeftModel(PushToHubMixin, torch.nn.Module):
    
    def __init__(self, model: PreTrainedModel, peft_config: PeftConfig, adapter_name: str = "default") -&gt; None:
        super().__init__()
        self.modules_to_save = None
        self.active_adapter = adapter_name
        self.peft_type = peft_config.peft_type
        # These args are special PEFT arguments that users can pass. They need to be removed before passing them to
        # forward.
        self.special_peft_forward_args = {"adapter_names"}

        self._is_prompt_learning = peft_config.is_prompt_learning
        if self._is_prompt_learning:
            self._peft_config = {adapter_name: peft_config}
            self.base_model = model
            self.add_adapter(adapter_name, peft_config)
        else:
            self._peft_config = None
            cls = PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type]
            self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
            self.set_additional_trainable_modules(peft_config, adapter_name)

        if getattr(model, "is_gradient_checkpointing", True):
            model = self._prepare_model_for_gradient_checkpointing(model)

        # the `pretraining_tp` is set for some models to simulate Tensor Parallelism during inference to avoid
        # numerical differences, https://github.com/pytorch/pytorch/issues/76232 - to avoid any unexpected
        # behavior we disable that in this line.
        if hasattr(self.base_model, "config") and hasattr(self.base_model.config, "pretraining_tp"):
            self.base_model.config.pretraining_tp = 1</code></pre>
<p>â€‹        PeftModelä¸­çš„æºç å¾ˆå¤§éƒ¨åˆ†æœåŠ¡äºæç¤ºå­¦ä¹ ï¼Œå…ˆä¸çœ‹ã€‚æˆ‘ä»¬åªçœ‹_is_prompt_learningæ˜¯Falseçš„æƒ…å†µã€‚</p>
<pre class="line-numbers language-none"><code class="language-none">else:
    self._peft_config = None
    cls = PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type]
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
    self.set_additional_trainable_modules(peft_config, adapter_name)</code></pre>
<p>â€‹        å¯ä»¥çœ‹åˆ°ï¼Œclsæœ€ç»ˆæ˜ å°„åˆ°å¯¹åº”peftæ–¹æ¡ˆçš„æ¨¡å‹å®ä¾‹ï¼Œè‹¥æ˜¯LORAï¼Œåˆ™è¿”å›ä¸€ä¸ªLoraModelå®ä¾‹ï¼Œçœ‹æ¥å…³é”®å°±åœ¨äºLoraModelä¸­ã€‚</p>
<h3 id="LoraModel"><a href="#LoraModel" class="headerlink" title="LoraModel"></a>LoraModel</h3><p>â€‹        LoraModelç»§æ‰¿äº†BaseTunerï¼Œè€ŒBaseTunerç»§æ‰¿nn.Moduleã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LoraModel(BaseTuner):
    prefix: str = "lora_"

    def __init__(self, model, config, adapter_name) -&gt; None:
        super().__init__(model, config, adapter_name)</code></pre>
<p>â€‹        ç”±äºä»£ç é‡éå¸¸å¤§ï¼Œæ‰€ä»¥åªè´´å‡ºæ ¸å¿ƒçš„ä»£ç ï¼Œè¿™ä¸€æ®µæ˜¯LoRAçš„æ ¸å¿ƒé€»è¾‘ã€‚é¦–å…ˆåˆ¤æ–­targetæ˜¯ä¸æ˜¯Loraå±‚ï¼Œè‹¥æ˜¯ï¼Œåˆ™æ ¹æ®loraconfigæ¥æ›´æ–°è¿™ä¸€å±‚ã€‚è‹¥ä¸æ˜¯ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„Loraå±‚æ¥æ›¿æ¢åŸæ¥çš„å±‚ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def _create_and_replace(
    self,
    lora_config,
    adapter_name,
    target,
    target_name,
    parent,
    current_key,
):
    """
    æ­¤å¤„çœç•¥éƒ¨åˆ†ä»£ç 
    """
    from peft.tuners.adalora import AdaLoraLayer

    if isinstance(target, LoraLayer) and not isinstance(target, AdaLoraLayer):
        target.update_layer(
            adapter_name,
            r,
            lora_alpha=alpha,
            lora_dropout=lora_config.lora_dropout,
            init_lora_weights=lora_config.init_lora_weights,
            use_rslora=lora_config.use_rslora,
            use_dora=lora_config.use_dora,
        )
    else:
        new_module = self._create_new_module(lora_config, adapter_name, target, **kwargs)
        if adapter_name != self.active_adapter:
            # adding an additional adapter: it is not automatically trainable
            new_module.requires_grad_(False)
        self._replace_module(parent, target_name, new_module, target)</code></pre>
<p>â€‹        ä¸‹é¢æ˜¯<strong>update_layer</strong>å‡½æ•°çš„é€»è¾‘ï¼Œé¦–å…ˆä¿è¯ç§©å¤§äº0ï¼Œç„¶ååˆ›å»ºdropoutå±‚ã€‚</p>
<p>â€‹        å†åˆ›å»ºABçŸ©é˜µï¼Œå¯ä»¥çœ‹åˆ°Aæ˜¯é™ç»´çŸ©é˜µï¼ŒBæ˜¯å‡ç»´çŸ©é˜µã€‚ä¹‹ååˆå§‹åŒ–è¿™ä¸¤ä¸ªçŸ©é˜µï¼Œå¯ä»¥çœ‹åˆ°AçŸ©é˜µå¯ä»¥é€‰æ‹©kaiming uniformæˆ–é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–ã€‚è‹¥æ˜¯ç”¨äºEmbeddingå±‚ï¼Œé‚£ä¹ˆAåˆå§‹åŒ–ä¸º0ï¼ŒBç”¨é«˜æ–¯åˆ†å¸ƒã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  def update_layer(
      self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights, use_rslora, use_dora: bool = False
  ):
      # This code works for linear layers, override for other layer types
      if r &lt;= 0:
          raise ValueError(f"`r` should be a positive integer value but the value passed is {r}")

      self.r[adapter_name] = r
      self.lora_alpha[adapter_name] = lora_alpha
      if lora_dropout &gt; 0.0:
          lora_dropout_layer = nn.Dropout(p=lora_dropout)
      else:
          lora_dropout_layer = nn.Identity()

      self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))
      # Actual trainable parameters
      self.lora_A[adapter_name] = nn.Linear(self.in_features, r, bias=False)
      self.lora_B[adapter_name] = nn.Linear(r, self.out_features, bias=False)
      if use_rslora:
          self.scaling[adapter_name] = lora_alpha / math.sqrt(r)
      else:
          self.scaling[adapter_name] = lora_alpha / r

      if init_lora_weights == "loftq":
          self.loftq_init(adapter_name)
      elif init_lora_weights:
          self.reset_lora_parameters(adapter_name, init_lora_weights)
"""
ä¸‹ç•¥
"""

      self.set_adapter(self.active_adapters)
    -----------------------------------------------------
  def reset_lora_parameters(self, adapter_name, init_lora_weights):
      if init_lora_weights is False:
          return

      if adapter_name in self.lora_A.keys():
          if init_lora_weights is True:
              # initialize A the same way as the default for nn.Linear and B to zero
              # https://github.com/microsoft/LoRA/blob/a0a92e0f26c067cf94747bdbf1ce73793fa44d19/loralib/layers.py#L124
              nn.init.kaiming_uniform_(self.lora_A[adapter_name].weight, a=math.sqrt(5))
          elif init_lora_weights.lower() == "gaussian":
              nn.init.normal_(self.lora_A[adapter_name].weight, std=1 / self.r[adapter_name])
          else:
              raise ValueError(f"Unknown initialization {init_lora_weights=}")
          nn.init.zeros_(self.lora_B[adapter_name].weight)
      if adapter_name in self.lora_embedding_A.keys():
          # initialize a the same way as the default for nn.linear and b to zero
          nn.init.zeros_(self.lora_embedding_A[adapter_name])
          nn.init.normal_(self.lora_embedding_B[adapter_name])</code></pre>
<h3 id="LoraLayer"><a href="#LoraLayer" class="headerlink" title="LoraLayer"></a>LoraLayer</h3><p>â€‹        å¯ä»¥çœ‹åˆ°layeréœ€è¦æ˜¯LoraLayerçš„å®ä¾‹æ‰ä¼šè¢«æ›´æ–°ï¼Œä¸‹é¢æ˜¯LoraLayerçš„ä»£ç ã€‚æœ€åˆçš„peft0.1.0ï¼ŒLoraåªèƒ½ç”¨äºLinearå±‚ï¼Œåæ¥åˆ™å°†ä¸€äº›denseå±‚ç»Ÿä¸€æŠ½è±¡æˆä¸ºäº†LoraLayerï¼Œå¯ä»¥çœ‹åˆ°æœ‰Linearï¼ŒEmbeddingï¼ŒConv1Då’ŒConv2Dã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LoraLayer(BaseTunerLayer):
    # All names of layers that may contain (trainable) adapter weights
    adapter_layer_names = ("lora_A", "lora_B", "lora_embedding_A", "lora_embedding_B")
    # All names of other parameters that may contain adapter-related parameters
    other_param_names = ("r", "lora_alpha", "scaling", "lora_dropout")

    def __init__(self, base_layer: nn.Module, **kwargs) -&gt; None:
        self.base_layer = base_layer
        self.r = {}
        self.lora_alpha = {}
        self.scaling = {}
        self.lora_dropout = nn.ModuleDict({})
        self.lora_A = nn.ModuleDict({})
        self.lora_B = nn.ModuleDict({})
        # For Embedding layer
        self.lora_embedding_A = nn.ParameterDict({})
        self.lora_embedding_B = nn.ParameterDict({})
        # Mark the weight as unmerged
        self._disable_adapters = False
        self.merged_adapters = []
        self.use_dora: dict[str, bool] = {}
        self.lora_magnitude_vector: Optional[torch.nn.ParameterDict] = None  # for DoRA
        self._caches: dict[str, Any] = {}
        self.kwargs = kwargs

        base_layer = self.get_base_layer()
        if isinstance(base_layer, nn.Linear):
            in_features, out_features = base_layer.in_features, base_layer.out_features
        elif isinstance(base_layer, nn.Conv2d):
            in_features, out_features = base_layer.in_channels, base_layer.out_channels
        elif isinstance(base_layer, nn.Embedding):
            in_features, out_features = base_layer.num_embeddings, base_layer.embedding_dim
        elif isinstance(base_layer, Conv1D):
            in_features, out_features = (
                base_layer.weight.ds_shape if hasattr(base_layer.weight, "ds_shape") else base_layer.weight.shape
            )
        elif hasattr(base_layer, "infeatures") and hasattr(base_layer, "outfeatures"):
            # QuantLinear
            in_features, out_features = base_layer.infeatures, base_layer.outfeatures
        elif hasattr(base_layer, "input_size") and hasattr(base_layer, "output_size"):
            # Megatron ColumnParallelLinear,RowParallelLinear
            in_features, out_features = base_layer.input_size, base_layer.output_size
        elif hasattr(base_layer, "codebooks") and base_layer.__class__.__name__ == "QuantizedLinear":
            # AQLM QuantLinear
            in_features, out_features = base_layer.in_features, base_layer.out_features
        elif hasattr(base_layer, "w_bit") and base_layer.__class__.__name__ == "WQLinear_GEMM":
            # Awq layers
            in_features, out_features = base_layer.in_features, base_layer.out_features
        else:
            raise ValueError(f"Unsupported layer type {type(base_layer)}")

        self.in_features = in_features
        self.out_features = out_features</code></pre>
<p>â€‹        åœ¨LoraModelä¸­çš„<strong>_create_new_module</strong>å‡½æ•°ä¸­æˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹åˆ°æŠ›å‡ºçš„å¼‚å¸¸ï¼Œåº”è¯äº†å½“targetä¸æ˜¯LoraLayerçš„æ—¶å€™ï¼Œå¦‚æœæƒ³åˆ›å»ºä¸€ä¸ªæ–°Loraæ¨¡å—å°†æ—§æ¨¡å—æ›¿æ¢ï¼Œé‚£ä¹ˆéœ€è¦åŸæ¨¡å—æ˜¯torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv2d, transformers.pytorch_utils.Conv1Dè¿™å‡ ç§ç±»å‹ï¼Œæœ€ç»ˆè¿™å‡ ä¸ªç±»åœ¨LoraLayer.pyä¸­éƒ½è¢«é‡å†™äº†ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if new_module is None:
    # no module could be matched
    raise ValueError(
        f"Target module {target} is not supported. Currently, only the following modules are supported: "
        "`torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`."
    )</code></pre>
<p>â€‹        åŒæ—¶ï¼Œå¯¹äºä¸åŒçš„æ¨¡å‹ï¼Œæ”¯æŒçš„Loraç­–ç•¥ä¹Ÿä¸åŒã€‚Transformeråº“ä¸­åˆ—ä¸¾äº†å“ªäº›æ¨¡å‹çš„å“ªäº›å±‚èƒ½å¤Ÿä½¿ç”¨å®˜æ–¹Loraæ–¹æ¡ˆã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING = {
    "t5": ["q", "v"],
    "mt5": ["q", "v"],
    "bart": ["q_proj", "v_proj"],
    "gpt2": ["c_attn"],
    "bloom": ["query_key_value"],
    "blip-2": ["q", "v", "q_proj", "v_proj"],
    "opt": ["q_proj", "v_proj"],
    "gptj": ["q_proj", "v_proj"],
    "gpt_neox": ["query_key_value"],
    "gpt_neo": ["q_proj", "v_proj"],
    "bert": ["query", "value"],
    "roberta": ["query", "value"],
    "xlm-roberta": ["query", "value"],
    "electra": ["query", "value"],
    "deberta-v2": ["query_proj", "value_proj"],
    "deberta": ["in_proj"],
    "layoutlm": ["query", "value"],
    "llama": ["q_proj", "v_proj"],
    "chatglm": ["query_key_value"],
    "gpt_bigcode": ["c_attn"],
    "mpt": ["Wqkv"],
    "RefinedWebModel": ["query_key_value"],
    "RefinedWeb": ["query_key_value"],
    "falcon": ["query_key_value"],
    "btlm": ["c_proj", "c_attn"],
    "codegen": ["qkv_proj"],
    "mistral": ["q_proj", "v_proj"],
    "mixtral": ["q_proj", "v_proj"],
    "stablelm": ["q_proj", "v_proj"],
    "phi": ["q_proj", "v_proj", "fc1", "fc2"],
    "gemma": ["q_proj", "v_proj"],
}</code></pre>
<h3 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h3><p>â€‹        æˆ‘ä»¬åªéœ€è¦çŸ¥é“Linearæœ€ç»ˆè¢«é‡å†™äº†ï¼Œç»§æ‰¿äº†LoraLayerï¼Œç„¶åé€šè¿‡LoraLayerçš„<strong>update_layer</strong>æ–¹æ³•æ ¹æ®configçš„å‚æ•°è¿›è¡Œäº†åˆå§‹åŒ–ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class Linear(nn.Module, LoraLayer):
    # Lora implemented in a dense layer
    def __init__(
        self,
        base_layer,
        adapter_name: str,
        r: int = 0,
        lora_alpha: int = 1,
        lora_dropout: float = 0.0,
        fan_in_fan_out: bool = False,  # Set this to True if the layer to replace stores weight like (fan_in, fan_out)
        is_target_conv_1d_layer: bool = False,
        init_lora_weights: Union[bool, str] = True,
        use_rslora: bool = False,
        use_dora: bool = False,
        **kwargs,
    ) -&gt; None:
        super().__init__()
        LoraLayer.__init__(self, base_layer, **kwargs)
        self.fan_in_fan_out = fan_in_fan_out

        self._active_adapter = adapter_name
        self.update_layer(
            adapter_name,
            r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            init_lora_weights=init_lora_weights,
            use_rslora=use_rslora,
            use_dora=use_dora,
        )
        self.is_target_conv_1d_layer = is_target_conv_1d_layer</code></pre>
<p>â€‹        æˆ‘ä»¬ç›´å¥”forwardæ–¹æ³•ã€‚å¯¹äºä¸åˆå¹¶çš„adapterï¼Œè‹¥å·²ç»åˆå¹¶ï¼ŒæŠŠå·²ç»åŠ å…¥åˆ°baselayerçš„æƒé‡ç§»å‡ºæ¥ã€‚å¯ä»¥åˆå¹¶çš„adapterè‹¥å·²ç»åˆå¹¶äº†å°±ç›´æ¥ç”¨è¿™ä¸ªæƒé‡ã€‚æœ€ç»ˆLoraä¸­çš„Linearå±‚çš„å‰å‘ä¼ æ’­ç»“æœæ˜¯ï¼š</p>
<script type="math/tex; mode=display">
X = XW + scale Ã— \frac{Î±}{r}(dropout(X))AB</script><pre class="line-numbers language-python" data-language="python"><code class="language-python">  def forward(self, x: torch.Tensor, *args: Any, **kwargs: Any) -&gt; torch.Tensor:
      self._check_forward_args(x, *args, **kwargs)
      adapter_names = kwargs.pop("adapter_names", None)
# è‹¥ä¹‹å‰æŠŠè¯¥adapterè®¾ç½®ä¸ºä¸åˆå¹¶ä½†åˆå¹¶äº†ï¼Œé©¬ä¸ŠæŠŠåŠ è¿›å»çš„æƒé‡æ‹¿å‡ºæ¥
      if self.disable_adapters:
          if self.merged:
              self.unmerge()
          result = self.base_layer(x, *args, **kwargs)
      elif adapter_names is not None:
          result = self._mixed_batch_forward(x, *args, adapter_names=adapter_names, **kwargs)
      # å·²ç»åˆå¹¶äº†å°±ç›´æ¥è¿”å›
      elif self.merged:
          result = self.base_layer(x, *args, **kwargs)
      else:
          result = self.base_layer(x, *args, **kwargs)
          torch_result_dtype = result.dtype
          for active_adapter in self.active_adapters:
              if active_adapter not in self.lora_A.keys():
                  continue
              lora_A = self.lora_A[active_adapter]
              lora_B = self.lora_B[active_adapter]
              dropout = self.lora_dropout[active_adapter]
              # self.scaling[adapter] = scale * self.lora_alpha[adapter] / self.r[adapter]
              scaling = self.scaling[active_adapter]
              x = x.to(lora_A.weight.dtype)

              if not self.use_dora[active_adapter]:
                  result = result + lora_B(lora_A(dropout(x))) * scaling
              else:
                  x = dropout(x)
                  result = result + self._apply_dora(x, lora_A, lora_B, scaling, active_adapter)

          result = result.to(torch_result_dtype)

      return result</code></pre>
<p>â€‹        å½“ç„¶è¿˜æœ‰å°‘ä¸äº†çš„mergeå’Œunmergeå‡½æ•°ï¼Œæœ€ç»ˆå¾®è°ƒå®Œæ¯•çš„æƒé‡æ˜¯å¯ä»¥é€‰æ‹©åŠ å›åŸæƒé‡æ¥æ¶ˆé™¤é¢å¤–çš„æ¨ç†å»¶è¿Ÿã€‚</p>
<p>â€‹        å…ˆçœ‹unmergeæ–¹æ³•ï¼Œå…¶å®å°±æ˜¯æŠŠåŠ åˆ°baselayerä¸Šçš„æƒé‡å‡å›å»ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def unmerge(self) -&gt; None:
    """
    This method unmerges all merged adapter layers from the base weights.
    """
    if not self.merged:
        warnings.warn("Already unmerged. Nothing to do.")
        return
    while len(self.merged_adapters) &gt; 0:
        active_adapter = self.merged_adapters.pop()
        if active_adapter in self.lora_A.keys():
            weight = self.get_base_layer().weight
            delta_weight = self.get_delta_weight(active_adapter)
            if not self.use_dora[active_adapter]:
                # åªçœ‹è¿™ä¸€å¥å°±è¡Œäº†
                weight.data -= delta_weight
            else:
                weight_norm = self._cache_pop(f"{active_adapter}-weight_norm")
                dora_factor = self.lora_magnitude_vector[active_adapter] / weight_norm
                weight_orig = weight.data / dora_factor.view(-1, 1) - delta_weight
                weight.data = weight_orig</code></pre>
<p>â€‹        å†çœ‹mergeæ“ä½œï¼Œé¦–å…ˆéœ€è¦æ‹¿åˆ°<strong>delta_weight</strong>ï¼Œå…·ä½“çš„å†…å®¹å°±ä¸å¤åˆ¶äº†ï¼Œå¦‚æœæ˜¯fp16çš„æƒé‡åœ¨cpuä¸Šè®¡ç®—ï¼Œé‚£ä¹ˆç”±äºcpuåŸç”Ÿä¸æ”¯æŒè¯¥ç±»å‹ï¼Œæ‰€ä»¥éœ€è¦è½¬æ¢æˆfp32å†è½¬æ¢å›å»ã€‚æœ€ç»ˆçš„<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="22.828ex" height="2.084ex" role="img" focusable="false" viewBox="0 -716 10090 921"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(1325.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2381.6,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(3651.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(4652,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(5121,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(5554,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(6083,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(6381,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(6726,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(7326,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(7803,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(8581,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(9340,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g></g></g></svg></mjx-container>ï¼Œå¯ä»¥çœ‹åˆ°ä»å¤´åˆ°å°¾æ˜¯æ²¡æœ‰åŸæ¥çš„Wå‚ä¸å‰å‘ä¼ æ’­çš„ï¼ŒåŸæ¥çš„Wç›´æ¥è¢«å†»ç»“ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def merge(self, safe_merge: bool = False, adapter_names: Optional[list[str]] = None) -&gt; None:
    adapter_names = check_adapters_to_merge(self, adapter_names)
    if not adapter_names:
        # no adapter to merge
        return

    for active_adapter in adapter_names:
        if active_adapter in self.lora_A.keys():
            base_layer = self.get_base_layer()
            if safe_merge:
                # Note that safe_merge will be slower than the normal merge
                # because of the copy operation.
	"""
	safe_mergeå…¶å®å°±æ˜¯æŸ¥çœ‹æ˜¯å¦æœ‰NaNå€¼ï¼Œæœ‰çš„è¯æŠ›å‡ºå¼‚å¸¸ã€‚
	å¹¶ä¸”ç”±äºå¤šå¤åˆ¶äº†ä¸€æ¬¡åŸæƒé‡ï¼Œæ‰€ä»¥æ•ˆç‡ä¼šæ›´ä½ï¼Œä»£ç ç•¥
	"""
            else:
                # è¯¥æ“ä½œæ ¸å¿ƒå°±ä¸€å¥ï¼Œè¿”å›scaling B@A
                # output_tensor = transpose(weight_B @ weight_A, self.fan_in_fan_out) * self.scaling[adapter]
                delta_weight = self.get_delta_weight(active_adapter)
                if not self.use_dora[active_adapter]:
                    base_layer.weight.data = base_layer.weight.data + delta_weight
                else:
                    # handle doraï¼Œæ­¤å¤„ç•¥
            self.merged_adapters.append(active_adapter)</code></pre>
<p>â€‹        å‰©ä¸‹çš„Embeddingï¼Œä¸€ç»´å·ç§¯å’ŒäºŒç»´å·ç§¯æ“ä½œå·®ä¸å¤šï¼Œè¿™é‡Œå°±ä¸å¤šèµ˜è¿°äº†ã€‚</p>
<h3 id="åˆå¹¶æ“ä½œ"><a href="#åˆå¹¶æ“ä½œ" class="headerlink" title="åˆå¹¶æ“ä½œ"></a>åˆå¹¶æ“ä½œ</h3><p>â€‹        é¦–å…ˆéœ€è¦åˆ¤æ–­æ˜¯å¦èƒ½å¤Ÿåˆå¹¶ã€‚æœ‰ä¸¤ç§æƒ…å†µä¸‹ä¸èƒ½åˆå¹¶ï¼Œä¸€ç§æ˜¯å½“å‰ä½¿ç”¨gptqé‡åŒ–æ¨¡å‹(QLoRAå¯ä»¥)ï¼Œå¦ä¸€ç§åˆ™æ˜¯å¼€å¯äº†peftconfigä¸­çš„layer_replicationæ“ä½œã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def _check_merge_allowed(self):
    """Verify that the configuration supports merging.

    Currently gptq quantization and replicated layers do not support merging.
    """
    if getattr(self.model, "quantization_method", None) == "gptq":
        raise ValueError("Cannot merge LORA layers when the model is gptq quantized")
    if self.peft_config.get("layer_replication"):
        raise ValueError("Cannot merge LORA layers when base model layers are replicated")</code></pre>
<p>â€‹        æœ€ç»ˆè°ƒç”¨çš„æ–¹æ³•æ˜¯<strong>merge_and_unload</strong>æ–¹æ³•ï¼Œ<code>progressbar</code>å°±æ˜¯æ˜¯å¦ç”¨tqdmæ˜¾ç¤ºè¿›è¡Œï¼Œ<code>safe_merge</code>ä¼šæ£€æŸ¥tensorä¸­æ˜¯å¦æœ‰NaNï¼Œ<code>adapter_names</code>ç”¨äºæŒ‡å®šå“ªäº›å±‚è¦åˆå¹¶ï¼Œé»˜è®¤æ˜¯æ‰€æœ‰éƒ½åˆå¹¶ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  def merge_and_unload(
      self, progressbar: bool = False, safe_merge: bool = False, adapter_names: Optional[list[str]] = None
  ) -&gt; torch.nn.Module:
# progressbarå°±æ˜¯æ˜¯å¦ç”¨tqdmæ˜¾ç¤ºè¿›è¡Œ
      return self._unload_and_optionally_merge(
          progressbar=progressbar, safe_merge=safe_merge, adapter_names=adapter_names
      )</code></pre>
<p>â€‹        ä¸‹é¢æ˜¯æ ¸å¿ƒæ–¹æ³•<strong>_unload_and_optionally_merge</strong>ã€‚é¦–å…ˆä¼šåˆ¤æ–­æ˜¯å¦æ»¡è¶³è¦æ±‚ï¼Œä¹Ÿå°±æ˜¯ä¸ä½¿ç”¨gptqä¸”ä¸é‡‡å–replicatedç­–ç•¥çš„æƒ…å†µä¸‹æ‰èƒ½åˆå¹¶ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def _unload_and_optionally_merge(
    self,
    merge=True,
    progressbar: bool = False,
    safe_merge: bool = False,
    adapter_names: Optional[list[str]] = None,
):
    if merge:
        self._check_merge_allowed()</code></pre>
<p>â€‹        ç„¶åæ‹¿åˆ°targetlayerè¿›è¡Œåˆå¹¶æ“ä½œåï¼Œæ›¿æ¢æ‰åŸæ¥çš„moduleã€‚è¿™é‡Œçš„target.mergeæ˜¯LoraLayerçš„å­ç±»éƒ½å®ç°çš„mergeæ“ä½œï¼Œä¸Šé¢å·²ç»ç»™å‡ºã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">if hasattr(target, "base_layer"):
    if merge:
        target.merge(safe_merge=safe_merge, adapter_names=adapter_names)
    self._replace_module(parent, target_name, target.get_base_layer(), target)
</code></pre>
<h1 id="å‚è€ƒèµ„æ–™"><a href="#å‚è€ƒèµ„æ–™" class="headerlink" title="å‚è€ƒèµ„æ–™"></a>å‚è€ƒèµ„æ–™</h1><p>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.15647">Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning</a></p>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>æœ¬æ–‡ä½œè€…ï¼š</strong>iroha</li><li class="post-copyright-link"><strong>æœ¬æ–‡é“¾æ¥ï¼š</strong><a href="http://example.com/post/ft_survey.html" title="å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ç»¼è¿°">http://example.com/post/ft_survey.html</a></li><li class="post-copyright-license"><strong>ç‰ˆæƒå£°æ˜ï¼š</strong>æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é»˜è®¤é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> è®¸å¯åè®®ã€‚</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/post/model_load.html" rel="prev" title="Huggingfaceçš„æ¨¡å‹åŠ è½½æµç¨‹"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">Huggingfaceçš„æ¨¡å‹åŠ è½½æµç¨‹</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/post/backtrace.html" rel="next" title="å›æº¯ä¹‹ç»„åˆé—®é¢˜"><span class="post-nav-text">å›æº¯ä¹‹ç»„åˆé—®é¢˜</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 â€“ 2024 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> iroha</span></div><div class="powered"><span>ç”± <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> é©±åŠ¨ v6.3.0</span><span class="footer-separator">|</span><span>ä¸»é¢˜ - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.9</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>