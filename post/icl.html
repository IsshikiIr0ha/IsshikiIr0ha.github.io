<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="iroha"><meta name="copyright" content="iroha"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>in-context learning | iroha</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"example.com","root":"/","title":"不想摆烂","version":"1.10.9","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><meta name="description" content="上下文学习​        in-context learning(上下文学习)由发表在NIPS上的《Language Models are Few-Shot Learners》提出。这篇文章证明了增加语言模型的规模可以显著提升任务无关、few-shot的性能，甚至能和采用fine-tuning的SOTA方法媲美。作者训练了GPT-3，一个175B参数的自回归模型，并在few-shot下测试它的性">
<meta property="og:type" content="article">
<meta property="og:title" content="in-context learning">
<meta property="og:url" content="http://example.com/post/icl.html">
<meta property="og:site_name" content="iroha">
<meta property="og:description" content="上下文学习​        in-context learning(上下文学习)由发表在NIPS上的《Language Models are Few-Shot Learners》提出。这篇文章证明了增加语言模型的规模可以显著提升任务无关、few-shot的性能，甚至能和采用fine-tuning的SOTA方法媲美。作者训练了GPT-3，一个175B参数的自回归模型，并在few-shot下测试它的性">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20231211191235294.png">
<meta property="og:image" content="http://example.com/images/image-20231211225513715.png">
<meta property="og:image" content="http://example.com/images/image-20231211231124699.png">
<meta property="article:published_time" content="2023-12-11T16:55:25.000Z">
<meta property="article:modified_time" content="2023-12-11T15:26:08.193Z">
<meta property="article:author" content="iroha">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20231211191235294.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="iroha"><img width="96" loading="lazy" src="/yun.png" alt="iroha"><span class="site-author-status" title="永远相信美好的事情即将发生">😊</span></a><div class="site-author-name"><a href="/about/">iroha</a></div><span class="site-name">iroha</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">21</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">9</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">16</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="友链" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">上下文学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">2.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">方法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%90%8E%E7%BB%AD%E5%B7%A5%E4%BD%9C"><span class="toc-number">4.</span> <span class="toc-text">后续工作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#warmup"><span class="toc-number">4.1.</span> <span class="toc-text">warmup</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://example.com/post/icl.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="iroha"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="iroha"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">in-context learning</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2023-12-12 00:55:25" itemprop="dateCreated datePublished" datetime="2023-12-12T00:55:25+08:00">2023-12-12</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="修改时间：2023-12-11 23:26:08" itemprop="dateModified" datetime="2023-12-11T23:26:08+08:00">2023-12-11</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">NLP</span></a></span> > <span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">论文笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/LLM/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">LLM</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><h1 id="上下文学习"><a href="#上下文学习" class="headerlink" title="上下文学习"></a>上下文学习</h1><p>​        <strong>in-context learning</strong>(上下文学习)由发表在NIPS上的《Language Models are Few-Shot Learners》提出。这篇文章证明了增加语言模型的规模可以显著提升任务无关、few-shot的性能，甚至能和采用fine-tuning的SOTA方法媲美。作者训练了GPT-3，一个175B参数的自回归模型，并在few-shot下测试它的性能。在所有的任务中，GPT-3都能在没有任何梯度更新和fine-tuning，只是与模型进行纯文本交互(包括任务描述和少样本演示)的情况下在多个任务中获得了优异的表现，包括机器翻译，问答，填空问题。</p>
<p>​        尽管如此，作者依旧提到，GPT-3在少样本学习中仍然存在挑战，以及GPT-3在训练大型网络语料库时面临方法上的问题。并且，ICL对prompt，上下文示例非常敏感。</p>
<p>​        在此给ICL下一个定义。ICL是一种只用一些格式化的示例让LLMs学习所给任务的范式，本质上是根据给定的查询从候选答案集中选出可能性最高的。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>​        NLP已经从为一个任务训练特定模型走到了为许多任务设计一种通用的模型这一步。如BERT等语言模型在大规模语料库上进行预训练已经让模型具有了一定的通用性，但大多数情况下我们仍然需要根据下游任务进行fine-tuing。通常，fine-tuning包括冻结预训练模型的一部分层（通常是底层），以保留先前学到的通用表示，然后在任务特定的数据上进行反向传播，更新与任务相关的层的权重。</p>
<p>​        之后，一些工作(如Language models are unsupervised multitask learners)说明了也许微调不是必要的，模型可以通过一些zero-shot转变为通用的NLP任务处理模型。然而，这些工作虽然很有前瞻性，但他们的最好性能在很多任务仅仅比Baseline好一些，甚至比baseline更差。于是作者大胆猜想，很可能是由于模型规模还不够大，所以zero-shot的结果不太好。</p>
<p>​        <img src="../images/image-20231211191235294.png" alt="image-20231211191235294" loading="lazy"></p>
<p>​        在上图中能得到许多信息。</p>
<p>​        对于第一张图，我们能够看到在GLUE任务中，zero-shot，one-shot和few-shot的性能随着LM参数量的增加显著提升，175B参数的GPT-3性能超过了使用125K个例子微调的BERT-Large和630K个例子微调的BERT++。</p>
<p>​        对于第二张图，随着使用的上下文增加，GPT-3在BLUE任务的表现也更强，而微调过的BERT-Large和BERT++，它们两者的差距可以在GPT-3中相当于7个上下文的差距。</p>
<p>​        对于第三张图，在42个benchmark中使用不同数量的shot，综合表现也随着参数量的增加而提高。</p>
<p>​        综合这三张图可以发现，哪怕是one-shot的性能也是要比zero-shot高出不少的，所以作者认为，LLM实际上可以是一个学习器，fine-tuning与上下文学习是可以结合起来使用的。在作者的实验中，巨大参数量的模型在经过fewshot后能媲美甚至超过许多任务的SOTA。</p>
<p>​        为了证明想法，作者训练了一系列的模型，参数量从125M到13B，发现参数量与zero-,one-,few-shot对各类任务的影响是很平滑的。同时随着参数量的上涨，zero-,one-,few-shot对各类任务的性能差距也在增大(第三张图)。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>​        进行实验时，作者使用的方法有，</p>
<ol>
<li><p>Fine-tuning，针对特定任务使用数千个数据在PLM上进行微调并更新权重。这种方法的优点是经过微调，都能在benchmark上取得不错的成绩。但缺点是为了适配每个任务需要大量的数据，在任务数据域外的泛化能力不佳。同时有可能因为数据中的一些spurious feature导致泛化能力差。</p>
</li>
<li><p>few-shot，给模型一些任务相关的描述作为条件，但不更新权重。一个示例通常由一个上下文和一个期望的结果，一般给模型10到100个例子。优点是极大减少了所需的数据，缺点是可能远远不及SOTA。</p>
</li>
<li><p>One-shot，类似于few-shot，只给模型一个例子。</p>
</li>
<li><p>zero-shot，不给模型例子，只给一个对任务的描述。</p>
<p>具体的in-context形式如下图所示，上下文最终和原查询生成一个新的查询输入给LLM。</p>
</li>
</ol>
<p><img src="../images/image-20231211225513715.png" alt="image-20231211225513715" loading="lazy"></p>
<p>​        ICL的优势是非常明显的，以自然语言编写，用户容易理解。其次无需训练模型，不修改权重。</p>
<h1 id="后续工作"><a href="#后续工作" class="headerlink" title="后续工作"></a>后续工作</h1><p>​        ICL在2020年提出，如今已经是2023年，这期间出现了许多相关的研究工作。</p>
<p>​        实验发现，ICL的强大性能取决于两个阶段：（1）培养LLMs的ICL能力的训练阶段，以及（2）在推理阶段，LLMs根据任务特定的演示进行预测。在训练阶段，LLMs直接在语言建模目标上进行训练，例如从左到右的生成。尽管这些模型没有专门针对上下文学习进行优化，它们仍然表现出ICL的能力。</p>
<p>​        下面这张来自于北大的ICL综述，详细阐述了针对ICL，训练与推理阶段为了增强ICL能力能做的工作。</p>
<p>​        <img src="../images/image-20231211231124699.png" alt="image-20231211231124699" loading="lazy"></p>
<h2 id="warmup"><a href="#warmup" class="headerlink" title="warmup"></a>warmup</h2><p>​        预热是ICL的一个可选步骤，它在ICL推理之前调整LLMs，包括修改LLMs的参数或添加额外的参数。与微调不同，预热并不旨在为特定任务训练LLM，而是增强模型的整体ICL能力。</p>
<p>​        在预训练中进行适应性训练可以显著的提升ICL的能力。<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2209.07661.pdf">On the Relation between Sensitivity and Accuracy in In-Context Learning</a>与<a target="_blank" rel="noopener" href="https://aclanthology.org/2022.naacl-main.201/">MetaICL: Learning to Learn In Context</a>中说明了adaption对ICL的增强。</p>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>iroha</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://example.com/post/icl.html" title="in-context learning">http://example.com/post/icl.html</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"></div><div class="post-nav-item"><a class="post-nav-next" href="/post/%E5%86%B3%E7%AD%96%E6%A0%91.html" rel="next" title=""><span class="post-nav-text"></span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2023 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> iroha</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.9</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>