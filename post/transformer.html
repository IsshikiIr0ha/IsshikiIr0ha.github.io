<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="iroha"><meta name="copyright" content="iroha"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>transformerç»“æ„ä¸ä»£ç  | iroha</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...undefined?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"example.com","root":"/","title":"ä¸æƒ³æ‘†çƒ‚","version":"1.10.9","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"æœç´¢...","empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹: ${query}","hits":"æ‰¾åˆ° ${hits} æ¡ç»“æœ","hits_time":"æ‰¾åˆ° ${hits} æ¡ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><meta name="description" content="ä»£ç æ€»è§ˆ class Transformer(nn.Module):      def __init__(self, src_pad_idx, trg_pad_idx, trg_sos_idx, enc_voc_size, dec_voc_size, d_model, n_head, max_len,                  ffn_hidden, n_layers, drop_pro">
<meta property="og:type" content="article">
<meta property="og:title" content="transformerç»“æ„ä¸ä»£ç ">
<meta property="og:url" content="http://example.com/post/transformer.html">
<meta property="og:site_name" content="iroha">
<meta property="og:description" content="ä»£ç æ€»è§ˆ class Transformer(nn.Module):      def __init__(self, src_pad_idx, trg_pad_idx, trg_sos_idx, enc_voc_size, dec_voc_size, d_model, n_head, max_len,                  ffn_hidden, n_layers, drop_pro">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20240316142251107.png">
<meta property="og:image" content="http://example.com/images/image-20240316154402911.png">
<meta property="og:image" content="http://example.com/images/image-20240315232326148.png">
<meta property="article:published_time" content="2024-04-01T08:08:00.000Z">
<meta property="article:modified_time" content="2024-07-22T09:15:38.348Z">
<meta property="article:author" content="iroha">
<meta property="article:tag" content="å¤§æ¨¡å‹">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20240316142251107.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="æ–‡ç« ç›®å½•"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="ç«™ç‚¹æ¦‚è§ˆ"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="iroha"><img width="96" loading="lazy" src="/yun.png" alt="iroha"><span class="site-author-status" title="æ°¸è¿œç›¸ä¿¡ç¾å¥½çš„äº‹æƒ…å³å°†å‘ç”Ÿ">ğŸ˜Š</span></a><div class="site-author-name"><a href="/about/">iroha</a></div><span class="site-name">iroha</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="é¦–é¡µ"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="å½’æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">14</span></a></div><div class="site-state-item"><a href="/categories/" title="åˆ†ç±»"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">4</span></a></div><div class="site-state-item"><a href="/tags/" title="æ ‡ç­¾"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">2</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="æ–‡æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="å‹é“¾" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E6%80%BB%E8%A7%88"><span class="toc-number">1.</span> <span class="toc-text">ä»£ç æ€»è§ˆ</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#embedding"><span class="toc-number">2.</span> <span class="toc-text">Embedding</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#encoder"><span class="toc-number">3.</span> <span class="toc-text">Encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#multiheadattention"><span class="toc-number">3.1.</span> <span class="toc-text">MultiHeadAttention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#scaled-dot-product-attention"><span class="toc-number">3.1.1.</span> <span class="toc-text">Scaled Dot-Product Attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#add-norm"><span class="toc-number">3.2.</span> <span class="toc-text">ADD &amp; NORM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#position-wise-feed-forward-networks"><span class="toc-number">3.3.</span> <span class="toc-text">Position-wise Feed-Forward
Networks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#add-norm-1"><span class="toc-number">3.4.</span> <span class="toc-text">ADD &amp; Norm</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#decoder"><span class="toc-number">4.</span> <span class="toc-text">Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#maskedmultiheadattention"><span class="toc-number">4.1.</span> <span class="toc-text">MaskedMultiHeadAttention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#multiheadattention-1"><span class="toc-number">4.2.</span> <span class="toc-text">MultiHeadAttention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#position-wise-feed-forward-networks-1"><span class="toc-number">4.3.</span> <span class="toc-text">Position-wise
Feed-Forward Networks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#linear"><span class="toc-number">4.4.</span> <span class="toc-text">Linear</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax"><span class="toc-number">4.5.</span> <span class="toc-text">softmax</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://example.com/post/transformer.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="iroha"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="iroha"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">transformerç»“æ„ä¸ä»£ç </h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="åˆ›å»ºæ—¶é—´ï¼š2024-04-01 16:08:00" itemprop="dateCreated datePublished" datetime="2024-04-01T16:08:00+08:00">2024-04-01</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="ä¿®æ”¹æ—¶é—´ï¼š2024-07-22 17:15:38" itemprop="dateModified" datetime="2024-07-22T17:15:38+08:00">2024-07-22</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">NLP</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">å¤§æ¨¡å‹</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><h1 id="ä»£ç æ€»è§ˆ">ä»£ç æ€»è§ˆ</h1>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class Transformer(nn.Module):

    def __init__(self, src_pad_idx, trg_pad_idx, trg_sos_idx, enc_voc_size, dec_voc_size, d_model, n_head, max_len,
                 ffn_hidden, n_layers, drop_prob, device):
        super().__init__()
        self.src_pad_idx &#x3D; src_pad_idx
        self.trg_pad_idx &#x3D; trg_pad_idx
        self.trg_sos_idx &#x3D; trg_sos_idx
        self.device &#x3D; device
        self.encoder &#x3D; Encoder(d_model&#x3D;d_model,
                               n_head&#x3D;n_head,
                               max_len&#x3D;max_len,
                               ffn_hidden&#x3D;ffn_hidden,
                               enc_voc_size&#x3D;enc_voc_size,
                               drop_prob&#x3D;drop_prob,
                               n_layers&#x3D;n_layers,
                               device&#x3D;device)

        self.decoder &#x3D; Decoder(d_model&#x3D;d_model,
                               n_head&#x3D;n_head,
                               max_len&#x3D;max_len,
                               ffn_hidden&#x3D;ffn_hidden,
                               dec_voc_size&#x3D;dec_voc_size,
                               drop_prob&#x3D;drop_prob,
                               n_layers&#x3D;n_layers,
                               device&#x3D;device)

    def forward(self, src, trg):
        src_mask &#x3D; self.make_src_mask(src)
        trg_mask &#x3D; self.make_trg_mask(trg)
        enc_src &#x3D; self.encoder(src, src_mask)
        output &#x3D; self.decoder(trg, enc_src, trg_mask, src_mask)
        return output

    def make_src_mask(self, src):
        src_mask &#x3D; (src !&#x3D; self.src_pad_idx).unsqueeze(1).unsqueeze(2)
        return src_mask

    def make_trg_mask(self, trg):
        trg_pad_mask &#x3D; (trg !&#x3D; self.trg_pad_idx).unsqueeze(1).unsqueeze(3)
        trg_len &#x3D; trg.shape[1]
        trg_sub_mask &#x3D; torch.tril(torch.ones(trg_len, trg_len)).type(torch.ByteTensor).to(self.device)
        trg_mask &#x3D; trg_pad_mask &amp; trg_sub_mask
        return trg_mask</code></pre>
<figure>
<img src="../images/image-20240316142251107.png"
alt="image-20240316142251107" />
<figcaption aria-hidden="true">image-20240316142251107</figcaption>
</figure>
<h1 id="embedding">Embedding</h1>
<p>Embeddingåˆ†ä¸ºToken Embeddingå’ŒPosition
Encodingï¼ŒäºŒè€…ç»“æœç›´æ¥ç›¸åŠ ï¼Œä¹‹åå…ˆlayernorm(åŸå§‹transformeré‡Œæ²¡ï¼Œbertæœ‰)å†dropoutã€‚</p>
<p>token Embeddingçš„å®ç°ä¸º</p>
<pre class="line-numbers language-none"><code class="language-none">tok_emb &#x3D; nn.Embedding(vocab_size,d_model,pad_id)
</code></pre>
<p>Pos embç¨å¾®æ¯”è¾ƒéº»çƒ¦ã€‚Transformerç”¨çš„æ˜¯ç»å¯¹ä½ç½®ç¼–ç ï¼Œä¸token
embç›´æ¥ç›¸åŠ ã€‚å…¶ä¸­iæ˜¯ <span class="math display">\[
\text{PE}_{(pos, 2i)} =
\sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \\
\text{PE}_{(pos, 2i+1)} =
\cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\]</span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class PositionEncoding(nn.Module):
	def __init__(self,d_model,max_len,device):
        super().__init__()
        self.encoding &#x3D; torch.zeros(max_len,d_model,device&#x3D;device)
        self.encoding.requires_grad &#x3D; False
        pos &#x3D; torch.arange(d_model,device&#x3D;device)
        pos &#x3D; pos.float().unsqueeze(dim&#x3D;1)
        _2i &#x3D; torch.arange(0,max_len,step&#x3D;2,device&#x3D;device).float() 
        self.encoding[:,0::2] &#x3D; torch.sin(pos&#x2F;10000**(_2i&#x2F;d_model))
        self.encoding[:,1::2] &#x3D; torch.cos(pos&#x2F;10000**(_2i&#x2F;d_model))</code></pre>
<h1 id="encoder">Encoder</h1>
<p>æ•´ä¸ªencoderå±‚æœ‰ä¸¤æ¬¡æ®‹å·®è¿æ¥ï¼Œå¹¶ä¸”éœ€è¦æŠŠè¢«paddingçš„éƒ¨åˆ†å¸¦æ©ç ä¸Šã€‚è¢«maskçš„åœ°æ–¹å¯ä»¥è®¾ç½®ä¸ºä¸€ä¸ªéå¸¸éå¸¸å°çš„è´Ÿæ•°ï¼Œè®©ä»–å¯¹softmaxè´¡çŒ®é™ä½ã€‚</p>
<figure>
<img src="../images/image-20240316154402911.png"
alt="image-20240316154402911" />
<figcaption aria-hidden="true">image-20240316154402911</figcaption>
</figure>
<h2 id="multiheadattention">MultiHeadAttention</h2>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class MultiHeadAttention(nn.Module):

    def __init__(self, d_model, n_head):
        super(MultiHeadAttention, self).__init__()
        self.n_head &#x3D; n_head
        self.attention &#x3D; ScaleDotProductAttention()
        self.w_q &#x3D; nn.Linear(d_model, d_model)
        self.w_k &#x3D; nn.Linear(d_model, d_model)
        self.w_v &#x3D; nn.Linear(d_model, d_model)
        self.w_concat &#x3D; nn.Linear(d_model, d_model) #  Wo

    def forward(self, q, k, v, mask&#x3D;None):
        # 1. dot product with weight matrices
        q, k, v &#x3D; self.w_q(q), self.w_k(k), self.w_v(v)

        # 2. split tensor by number of heads
        q, k, v &#x3D; self.split(q), self.split(k), self.split(v)

        # 3. do scale dot product to compute similarity
        out, attention &#x3D; self.attention(q, k, v, mask&#x3D;mask)

        # 4. concat and pass to linear layer
        out &#x3D; self.concat(out)
        out &#x3D; self.w_concat(out)

        # 5. visualize attention map
        # TODO : we should implement visualization

        return out</code></pre>
<figure>
<img src="../images/image-20240315232326148.png"
alt="image-20240315232326148" />
<figcaption aria-hidden="true">image-20240315232326148</figcaption>
</figure>
<p>ç”¨ç‚¹ç§¯æ³¨æ„åŠ›å¯ä»¥é«˜åº¦å¹¶è¡Œï¼Œå¯¹äºè¾ƒå¤§çš„dkå€¼ï¼ŒåŠ æ³•æ³¨æ„åŠ›åœ¨æ€§èƒ½ä¸Šä¼˜äºæ— ç¼©æ”¾çš„ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œé™¤ä»¥æ ¹å·dkçš„åŸå› æ˜¯æ€•softmaxè¾“å‡ºçš„æ¢¯åº¦å¤ªå°ã€‚</p>
<p>https://blog.csdn.net/qq_37430422/article/details/105042303</p>
<p>$ Attention(Q,K,V) = softmax()V$</p>
<h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3>
<p>Q,K,Vä¼šç»è¿‡ä¸€ä¸ªçº¿æ€§å±‚(d,d)ï¼Œç„¶åæ‹†åˆ†é€ç»™å¤šä¸ªAttentionï¼Œè¿™é‡Œè¯·æ³¨æ„ï¼Œæ˜¯æŠŠåºåˆ—ç»™æ‹†äº†ï¼ä¸æ˜¯æŠŠtensoré“ºå¼€æ¥ç„¶ååˆ’åˆ†ï¼Œæ‰€ä»¥éœ€è¦åtranspose</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">tensor &#x3D; tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)</code></pre>
<p>ä¹‹åå°†q,k,vä¼ å…¥ç‚¹ç§¯attention</p>
<pre class="line-numbers language-none"><code class="language-none">class ScaleDotProductAttention(nn.Module):
    &quot;&quot;&quot;
    compute scale dot product attention

    Query : given sentence that we focused on (decoder)
    Key : every sentence to check relationship with Qeury(encoder)
    Value : every sentence same with Key (encoder)
    &quot;&quot;&quot;

    def __init__(self):
        super(ScaleDotProductAttention, self).__init__()
        self.softmax &#x3D; nn.Softmax(dim&#x3D;-1)

    def forward(self, q, k, v, mask&#x3D;None, e&#x3D;1e-12):
        # input is 4 dimension tensor
        # [batch_size, head, length, d_tensor]
        batch_size, head, length, d_tensor &#x3D; k.size()

        # 1. dot product Query with Key^T to compute similarity
        k_t &#x3D; k.transpose(2, 3)  # transpose
        score &#x3D; (q @ k_t) &#x2F; math.sqrt(d_tensor)  # scaled dot product

        # 2. apply masking (opt)
        if mask is not None:
            score &#x3D; score.masked_fill(mask &#x3D;&#x3D; 0, -10000)

        # 3. pass them softmax to make [0, 1] range
        score &#x3D; self.softmax(score)

        # 4. multiply with Value
        v &#x3D; score @ v

        return v, score
</code></pre>
<p>æ¯ä¸€ä¸ªheadå¹¶è¡Œè®¡ç®—å®Œattentionåï¼Œé‡æ–°concatèµ·æ¥ï¼ŒWoå¦‚æœåœ¨headèƒ½è¢«dmodelæ•´é™¤çš„æƒ…å†µä¸‹ï¼Œæ‹¼èµ·æ¥å°±æ˜¯ä¸€ä¸ª512Ã—512çš„çŸ©é˜µã€‚</p>
<h2 id="add-norm">ADD &amp; NORM</h2>
<pre class="line-numbers language-none"><code class="language-none">x &#x3D; self.attention(q&#x3D;x, k&#x3D;x, v&#x3D;x, mask&#x3D;src_mask)

# 2. add and norm
x &#x3D; self.dropout1(x)
x &#x3D; self.norm1(x + _x)</code></pre>
<h2 id="position-wise-feed-forward-networks">Position-wise Feed-Forward
Networks</h2>
<p>è¿™ä¸ªåšæ³•å¯ä»¥å¢åŠ æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶ä¸”å¢åŠ äº†æ¨¡å‹å­¦ä¹ çš„çµæ´»æ€§ã€‚é€šè¿‡å°†ç»´åº¦æ˜ å°„åˆ°æ›´é«˜çš„ç»´åº¦ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´å¤æ‚çš„ç‰¹å¾ï¼Œç„¶åé€šè¿‡ç¬¬äºŒä¸ªçº¿æ€§å±‚å°†å…¶æ˜ å°„å›åŸå§‹çš„ç»´åº¦ï¼Œä»¥ä¿ç•™åŸå§‹è¾“å…¥çš„ä¿¡æ¯ã€‚è¿™ç§è®¾è®¡å¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°å­¦ä¹ è¾“å…¥åºåˆ—ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œå¹¶ä¸”æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚åŸå§‹çš„éšè—å±‚æ˜¯2048</p>
<pre class="line-numbers language-none"><code class="language-none">class PositionwiseFeedForward(nn.Module):

    def __init__(self, d_model, hidden, drop_prob&#x3D;0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.linear1 &#x3D; nn.Linear(d_model, hidden)
        self.linear2 &#x3D; nn.Linear(hidden, d_model)
        self.relu &#x3D; nn.ReLU()
        self.dropout &#x3D; nn.Dropout(p&#x3D;drop_prob)

    def forward(self, x):
        x &#x3D; self.linear1(x)
        x &#x3D; self.relu(x)
        x &#x3D; self.dropout(x)
        x &#x3D; self.linear2(x)
        return x</code></pre>
<h2 id="add-norm-1">ADD &amp; Norm</h2>
<pre class="line-numbers language-none"><code class="language-none"># 3. positionwise feed forward network
_x &#x3D; x
x &#x3D; self.ffn(x)
      
# 4. add and norm
x &#x3D; self.dropout2(x)
x &#x3D; self.norm2(x + _x)
return x</code></pre>
<h1 id="decoder">Decoder</h1>
<p>åœ¨è®­ç»ƒæ—¶ï¼ŒDecoderçš„embè¾“å…¥ä¸æ˜¯ä¸Šä¸€æ­¥ç”Ÿæˆçš„ï¼Œè€Œæ˜¯ç›®æ ‡åºåˆ—ã€‚Decoderæœ‰ä¸‰æ¬¡æ®‹å·®</p>
<pre class="line-numbers language-none"><code class="language-none">class Decoder(nn.Module):
    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):
        super().__init__()
        self.emb &#x3D; TransformerEmbedding(d_model&#x3D;d_model,
                                        drop_prob&#x3D;drop_prob,
                                        max_len&#x3D;max_len,
                                        vocab_size&#x3D;dec_voc_size,
                                        device&#x3D;device)

        self.layers &#x3D; nn.ModuleList([DecoderLayer(d_model&#x3D;d_model,
                                                  ffn_hidden&#x3D;ffn_hidden,
                                                  n_head&#x3D;n_head,
                                                  drop_prob&#x3D;drop_prob)
                                     for _ in range(n_layers)])

        self.linear &#x3D; nn.Linear(d_model, dec_voc_size)

    def forward(self, trg, enc_src, trg_mask, src_mask):
        trg &#x3D; self.emb(trg)

        for layer in self.layers:
            trg &#x3D; layer(trg, enc_src, trg_mask, src_mask)

        # pass to LM head
        output &#x3D; self.linear(trg)
        return output</code></pre>
<h2 id="maskedmultiheadattention">MaskedMultiHeadAttention</h2>
<p>æ•´ä½“å®ç°å’ŒEncoderçš„ä¸€æ ·ï¼Œä½†éœ€è¦å¸¦Maskedï¼Œå®é™…ä¸Šï¼Œåœ¨Encoderä¸­ï¼Œè¢«padçš„åœ°æ–¹ä¹Ÿè¦è¢«Maskedã€‚</p>
<pre class="line-numbers language-none"><code class="language-none">class DecoderLayer(nn.Module):

    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):
        super(DecoderLayer, self).__init__()
        self.self_attention &#x3D; MultiHeadAttention(d_model&#x3D;d_model, n_head&#x3D;n_head)
        self.norm1 &#x3D; LayerNorm(d_model&#x3D;d_model)
        self.dropout1 &#x3D; nn.Dropout(p&#x3D;drop_prob)

        self.enc_dec_attention &#x3D; MultiHeadAttention(d_model&#x3D;d_model, n_head&#x3D;n_head)
        self.norm2 &#x3D; LayerNorm(d_model&#x3D;d_model)
        self.dropout2 &#x3D; nn.Dropout(p&#x3D;drop_prob)

        self.ffn &#x3D; PositionwiseFeedForward(d_model&#x3D;d_model, hidden&#x3D;ffn_hidden, drop_prob&#x3D;drop_prob)
        self.norm3 &#x3D; LayerNorm(d_model&#x3D;d_model)
        self.dropout3 &#x3D; nn.Dropout(p&#x3D;drop_prob)
</code></pre>
<p>Decoderçš„maskéƒ¨åˆ†ç›¸å¯¹æ¥è¯´æ¯”è¾ƒå¤æ‚ã€‚é¦–å…ˆéœ€è¦æŠŠè¢«paddingçš„éƒ¨åˆ†å…ˆmaskä¸Šï¼Œç„¶åç”Ÿæˆä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µã€‚</p>
<p>è¿™é‡Œçš„QKVéƒ½æ¥è‡ªäºä¸Šä¸€æ­¥ï¼Œè®­ç»ƒä¸­åˆ™éƒ½æ˜¯ç›®æ ‡ã€‚</p>
<h2 id="multiheadattention-1">MultiHeadAttention</h2>
<p>è¿™é‡Œå’ŒEncoderå”¯ä¸€çš„åŒºåˆ«å°±æ˜¯Qæ¥è‡ªDecoderä¸­outputçš„embç»è¿‡ä¸€ä¸ªMaskedæ³¨æ„åŠ›æ¥çš„ï¼Œå…¶ä½™KVéƒ½æ˜¯ä»encoderè¿›æ¥çš„ï¼Œ</p>
<p>è¿™æ˜¯å› ä¸ºDecoder
éœ€è¦å…³æ³¨æºåºåˆ—çš„ä¿¡æ¯ï¼Œä»¥ä¾¿å¯¹å½“å‰ä½ç½®è¿›è¡Œåˆé€‚çš„é¢„æµ‹ã€‚è¿™ç§ä¿¡æ¯ä¸»è¦æ¥è‡ªäº
Encoder çš„è¾“å‡ºï¼Œå› ä¸º Encoder
èƒ½å¤Ÿå°†æºåºåˆ—çš„ä¿¡æ¯ç¼–ç ä¸ºä¸€ä¸ªä¸Šä¸‹æ–‡å‘é‡åºåˆ—ï¼ŒDecoder
é€šè¿‡å…³æ³¨è¿™äº›ä¸Šä¸‹æ–‡å‘é‡ï¼Œèƒ½å¤Ÿè·å–æºåºåˆ—çš„è¯­ä¹‰ä¿¡æ¯ã€‚Decoder
è¿˜éœ€è¦è€ƒè™‘å·²ç»ç”Ÿæˆçš„éƒ¨åˆ†ç›®æ ‡åºåˆ—ï¼Œä»¥ä¾¿ä¿æŒç”Ÿæˆçš„è¿è´¯æ€§å’Œåˆç†æ€§ã€‚è¿™ç§ä¿¡æ¯ä¸»è¦æ¥è‡ªäº
Decoder è‡ªèº«çš„å‰é¢å±‚è¾“å‡ºï¼Œå› ä¸º Decoder
åœ¨æ¯ä¸ªæ—¶é—´æ­¥éƒ½ä¼šç”Ÿæˆä¸€ä¸ªæ–°çš„ç›®æ ‡åºåˆ—æ ‡è®°ï¼Œè¿™äº›æ ‡è®°æ„æˆäº†ä¸€ä¸ªé€æ­¥ç”Ÿæˆçš„åºåˆ—ã€‚</p>
<h2 id="position-wise-feed-forward-networks-1">Position-wise
Feed-Forward Networks</h2>
<p>åŒEncoder</p>
<h2 id="linear">Linear</h2>
<pre class="line-numbers language-none"><code class="language-none">self.linear &#x3D; nn.Linear(d_model, dec_voc_size)</code></pre>
<h2 id="softmax">softmax</h2>
<p>è®­ç»ƒæ—¶ä¸éœ€è¦ï¼Œæ¨ç†æ—¶æ‰éœ€è¦</p>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>æœ¬æ–‡ä½œè€…ï¼š</strong>iroha</li><li class="post-copyright-link"><strong>æœ¬æ–‡é“¾æ¥ï¼š</strong><a href="http://example.com/post/transformer.html" title="transformerç»“æ„ä¸ä»£ç ">http://example.com/post/transformer.html</a></li><li class="post-copyright-license"><strong>ç‰ˆæƒå£°æ˜ï¼š</strong>æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é»˜è®¤é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> è®¸å¯åè®®ã€‚</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/post/T5.html" rel="prev" title="T5åŠç›¸å…³è§£æ"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">T5åŠç›¸å…³è§£æ</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/post/PE.html" rel="next" title="Position Encoding"><span class="post-nav-text">Position Encoding</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 â€“ 2024 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> iroha</span></div><div class="powered"><span>ç”± <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> é©±åŠ¨ v6.3.0</span><span class="footer-separator">|</span><span>ä¸»é¢˜ - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.9</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>