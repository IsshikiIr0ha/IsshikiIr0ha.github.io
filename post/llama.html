<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="iroha"><meta name="copyright" content="iroha"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>llamaç»“æ„æµ…æ | iroha</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"example.com","root":"/","title":"ä¸æƒ³æ‘†çƒ‚","version":"1.10.11","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"æœç´¢...","empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹: ${query}","hits":"æ‰¾åˆ° ${hits} æ¡ç»“æœ","hits_time":"æ‰¾åˆ° ${hits} æ¡ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js" defer></script><script src="/js/load-aplayer.js" defer></script><meta name="description" content="ä»¥ä¸‹çš„å†…å®¹æ¥è‡ªäºLLaMAgithubä»“åº“ä¸Huggingfaceä¸­çš„LLaMAå®ç°ã€‚ ç»“æ„ llamaçš„ç»“æ„è‡ªå§‹è‡³ç»ˆæ²¡æœ‰ä»€ä¹ˆæ”¹å˜ï¼Œæ— éå°±æ˜¯æ¯ä¸€ä»£éƒ½æŠŠGQAçš„å®ç°ä¸‹æ”¾åˆ°å°å‚æ•°ä¸Šã€‚   image-20240708001842029  å¤§ä½“çš„ç»“æ„å’ŒGPT-2è¿˜æ˜¯éå¸¸ç›¸ä¼¼çš„ï¼ŒDecoder-onlyï¼Œå‰ç½®Normç­‰ç­‰ã€‚å…·ä½“ç»“æ„å¯è§ä¸‹é¢çš„ä»£ç ï¼š class LlamaModel(Llam">
<meta property="og:type" content="article">
<meta property="og:title" content="llamaç»“æ„æµ…æ">
<meta property="og:url" content="http://example.com/post/llama.html">
<meta property="og:site_name" content="iroha">
<meta property="og:description" content="ä»¥ä¸‹çš„å†…å®¹æ¥è‡ªäºLLaMAgithubä»“åº“ä¸Huggingfaceä¸­çš„LLaMAå®ç°ã€‚ ç»“æ„ llamaçš„ç»“æ„è‡ªå§‹è‡³ç»ˆæ²¡æœ‰ä»€ä¹ˆæ”¹å˜ï¼Œæ— éå°±æ˜¯æ¯ä¸€ä»£éƒ½æŠŠGQAçš„å®ç°ä¸‹æ”¾åˆ°å°å‚æ•°ä¸Šã€‚   image-20240708001842029  å¤§ä½“çš„ç»“æ„å’ŒGPT-2è¿˜æ˜¯éå¸¸ç›¸ä¼¼çš„ï¼ŒDecoder-onlyï¼Œå‰ç½®Normç­‰ç­‰ã€‚å…·ä½“ç»“æ„å¯è§ä¸‹é¢çš„ä»£ç ï¼š class LlamaModel(Llam">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20240708001842029.png">
<meta property="og:image" content="http://example.com/images/image-20240718154653674.png">
<meta property="og:image" content="http://example.com/images/image-20240718154454439.png">
<meta property="article:published_time" content="2024-07-18T08:08:00.000Z">
<meta property="article:modified_time" content="2024-07-23T02:00:30.048Z">
<meta property="article:author" content="iroha">
<meta property="article:tag" content="å¤§æ¨¡å‹">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20240708001842029.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="æ–‡ç« ç›®å½•"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="ç«™ç‚¹æ¦‚è§ˆ"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="iroha"><img width="96" loading="lazy" src="/yun.png" alt="iroha"><span class="site-author-status" title="æ°¸è¿œç›¸ä¿¡ç¾å¥½çš„äº‹æƒ…å³å°†å‘ç”Ÿ">ğŸ˜Š</span></a><div class="site-author-name"><a href="/about/">iroha</a></div><span class="site-name">iroha</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="é¦–é¡µ"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="å½’æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">17</span></a></div><div class="site-state-item"><a href="/categories/" title="åˆ†ç±»"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">4</span></a></div><div class="site-state-item"><a href="/tags/" title="æ ‡ç­¾"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">2</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="æ–‡æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="å‹é“¾" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E6%9E%84"><span class="toc-number">1.</span> <span class="toc-text">ç»“æ„</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#embedding"><span class="toc-number">1.1.</span> <span class="toc-text">Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#decoder-layer"><span class="toc-number">1.2.</span> <span class="toc-text">Decoder Layer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#rmsnorm"><span class="toc-number">1.2.1.</span> <span class="toc-text">RMSNorm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#attention"><span class="toc-number">1.2.2.</span> <span class="toc-text">Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rmsnorm-1"><span class="toc-number">1.2.3.</span> <span class="toc-text">RMSNorm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mlp"><span class="toc-number">1.2.4.</span> <span class="toc-text">MLP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rmsnorm-2"><span class="toc-number">1.2.5.</span> <span class="toc-text">RMSNorm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#linear"><span class="toc-number">1.2.6.</span> <span class="toc-text">Linear</span></a></li></ol></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://example.com/post/llama.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="iroha"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="iroha"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">llamaç»“æ„æµ…æ</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="åˆ›å»ºæ—¶é—´ï¼š2024-07-18 16:08:00" itemprop="dateCreated datePublished" datetime="2024-07-18T16:08:00+08:00">2024-07-18</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="ä¿®æ”¹æ—¶é—´ï¼š2024-07-23 10:00:30" itemprop="dateModified" datetime="2024-07-23T10:00:30+08:00">2024-07-23</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">NLP</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">å¤§æ¨¡å‹</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>ä»¥ä¸‹çš„å†…å®¹æ¥è‡ªäº<a target="_blank" rel="noopener" href='https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md'>LLaMAgithubä»“åº“</a>ä¸Huggingfaceä¸­çš„LLaMAå®ç°ã€‚</p>
<h1 id="ç»“æ„">ç»“æ„</h1>
<p>llamaçš„ç»“æ„è‡ªå§‹è‡³ç»ˆæ²¡æœ‰ä»€ä¹ˆæ”¹å˜ï¼Œæ— éå°±æ˜¯æ¯ä¸€ä»£éƒ½æŠŠGQAçš„å®ç°ä¸‹æ”¾åˆ°å°å‚æ•°ä¸Šã€‚</p>
<figure>
<img src="../images/image-20240708001842029.png"
alt="image-20240708001842029" />
<figcaption aria-hidden="true">image-20240708001842029</figcaption>
</figure>
<p>å¤§ä½“çš„ç»“æ„å’ŒGPT-2è¿˜æ˜¯éå¸¸ç›¸ä¼¼çš„ï¼ŒDecoder-onlyï¼Œå‰ç½®Normç­‰ç­‰ã€‚å…·ä½“ç»“æ„å¯è§ä¸‹é¢çš„ä»£ç ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LlamaModel</span><span class="token punctuation">(</span>LlamaPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]

    Args:
        config: LlamaConfig
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> LlamaConfig<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>padding_idx <span class="token operator">=</span> config<span class="token punctuation">.</span>pad_token_id
        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> config<span class="token punctuation">.</span>vocab_size

        self<span class="token punctuation">.</span>embed_tokens <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>padding_idx<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>
            <span class="token punctuation">[</span>LlamaDecoderLayer<span class="token punctuation">(</span>config<span class="token punctuation">,</span> layer_idx<span class="token punctuation">)</span> <span class="token keyword">for</span> layer_idx <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>num_hidden_layers<span class="token punctuation">)</span><span class="token punctuation">]</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> LlamaRMSNorm<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>rms_norm_eps<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>gradient_checkpointing <span class="token operator">=</span> <span class="token boolean">False</span>

        <span class="token comment"># Initialize weights and apply final processing</span>
        self<span class="token punctuation">.</span>post_init<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
<span class="token keyword">class</span> <span class="token class-name">LlamaDecoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> LlamaConfig<span class="token punctuation">,</span> layer_idx<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size

        self<span class="token punctuation">.</span>self_attn <span class="token operator">=</span> LLAMA_ATTENTION_CLASSES<span class="token punctuation">[</span>config<span class="token punctuation">.</span>_attn_implementation<span class="token punctuation">]</span><span class="token punctuation">(</span>config<span class="token operator">=</span>config<span class="token punctuation">,</span> layer_idx<span class="token operator">=</span>layer_idx<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>mlp <span class="token operator">=</span> LlamaMLP<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>input_layernorm <span class="token operator">=</span> LlamaRMSNorm<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>rms_norm_eps<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>post_attention_layernorm <span class="token operator">=</span> LlamaRMSNorm<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>rms_norm_eps<span class="token punctuation">)</span>
        
     
<span class="token keyword">class</span> <span class="token class-name">LlamaForCausalLM</span><span class="token punctuation">(</span>LlamaPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    _tied_weights_keys <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"lm_head.weight"</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> LlamaModel<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> config<span class="token punctuation">.</span>vocab_size
        self<span class="token punctuation">.</span>lm_head <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

        <span class="token comment"># Initialize weights and apply final processing</span>
        self<span class="token punctuation">.</span>post_init<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="embedding">Embedding</h2>
<p>æ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼ŒBERTåŒæ¬¾ï¼Œå¯è®­ç»ƒçš„Embeddingã€‚</p>
<h2 id="decoder-layer">Decoder Layer</h2>
<p>Decoder Layerä¸»è¦ç”±æ³¨æ„åŠ›å±‚ï¼ŒMLPä¸RMSNormç»„æˆã€‚å‰å‘ä¼ æ’­ä»£ç å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">      residual <span class="token operator">=</span> hidden_states <span class="token comment"># 1. ä¿ç•™æ®‹å·®x</span>

      hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>input_layernorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span> <span class="token comment">#2. è®¡ç®—å‰ç½®RMSNorm</span>

      <span class="token comment">#3. Self Attention</span>
      hidden_states<span class="token punctuation">,</span> self_attn_weights<span class="token punctuation">,</span> present_key_value <span class="token operator">=</span> self<span class="token punctuation">.</span>self_attn<span class="token punctuation">(</span>
          hidden_states<span class="token operator">=</span>hidden_states<span class="token punctuation">,</span>
          attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
          position_ids<span class="token operator">=</span>position_ids<span class="token punctuation">,</span>
          past_key_value<span class="token operator">=</span>past_key_value<span class="token punctuation">,</span>
          output_attentions<span class="token operator">=</span>output_attentions<span class="token punctuation">,</span>
          use_cache<span class="token operator">=</span>use_cache<span class="token punctuation">,</span>
          cache_position<span class="token operator">=</span>cache_position<span class="token punctuation">,</span>
          <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
      <span class="token punctuation">)</span>
      <span class="token comment"># 4. è®¡ç®—æ®‹å·®</span>
      hidden_states <span class="token operator">=</span> residual <span class="token operator">+</span> hidden_states

      <span class="token comment"># ä¿ç•™æ®‹å·®</span>
      residual <span class="token operator">=</span> hidden_states
      <span class="token comment"># 6. MLPä¹‹å‰çš„RMSNorm</span>
      hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
      <span class="token comment"># 7.è®¡ç®—MLP</span>
      hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
      <span class="token comment"># 8.è®¡ç®—æ®‹å·®</span>
      hidden_states <span class="token operator">=</span> residual <span class="token operator">+</span> hidden_states

      outputs <span class="token operator">=</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span><span class="token punctuation">)</span>
<span class="token comment"># æ˜¯å¦è¾“å‡ºAttention Weight</span>
      <span class="token keyword">if</span> output_attentions<span class="token punctuation">:</span>
          outputs <span class="token operator">+=</span> <span class="token punctuation">(</span>self_attn_weights<span class="token punctuation">,</span><span class="token punctuation">)</span>
<span class="token comment"># KV Cache</span>
      <span class="token keyword">if</span> use_cache<span class="token punctuation">:</span>
          outputs <span class="token operator">+=</span> <span class="token punctuation">(</span>present_key_value<span class="token punctuation">,</span><span class="token punctuation">)</span>

      <span class="token keyword">return</span> outputs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="rmsnorm">RMSNorm</h3>
<p>åŸè®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/pdf/1910.07467.pdf</p>
<p>æ€»ç»“ä¸€ä¸‹å°±æ˜¯çœç•¥è®¡ç®—meançš„æ“ä½œä¸ä¼šå½±å“æ€§èƒ½ï¼Œä½†å¯ä»¥èŠ‚çœå¤§é‡è®¡ç®—å¼€é”€ã€‚</p>
<p>ä½œä¸ºå¯¹æ¯”ï¼Œæˆ‘ä»¬é¦–å…ˆå›é¡¾ä¸€ä¸‹LayerNormã€‚ <span class="math display">\[
y = \frac{x-E(x)}{\sqrt{var(x)+\epsilon}}
\]</span></p>
<p>å…¶ä¸­xä¸ºå¯¹åº”dimä¸Šçš„ä¸€ç»„æ•°ã€‚</p>
<p>ä»£ç å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LayerNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>d_model<span class="token punctuation">,</span>eps<span class="token operator">=</span><span class="token number">1e-12</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
		self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>beta <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>eps <span class="token operator">=</span> <span class="token number">1e-12</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        var<span class="token punctuation">,</span>mean <span class="token operator">=</span> torch<span class="token punctuation">.</span>var_mean<span class="token punctuation">(</span>x<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> <span class="token punctuation">(</span>x<span class="token operator">-</span>mean<span class="token punctuation">)</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>var<span class="token operator">+</span>self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> out <span class="token operator">+</span> self<span class="token punctuation">.</span>beta
        <span class="token keyword">return</span> out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>LayerNormé€šè¿‡å½’ä¸€åŒ–å¤„ç†ï¼Œé˜²æ­¢æ¯ä¸€å±‚çš„åˆ†å¸ƒå‘ç”Ÿå‰§çƒˆå˜åŒ–ï¼Œå‡å°‘Internal
Covariate
Shiftï¼Œè®©åˆ†å¸ƒè¶‹äºç¨³å®šï¼Œä½¿å¾—æ¢¯åº¦ä¼ æ’­æ›´åŠ ç¨³å®šï¼Œæœ‰åŠ©äºå‡å°‘æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸ã€‚ä¹Ÿå‡å°‘äº†å¯¹è®­ç»ƒæ•°æ®åˆ†å¸ƒçš„ä¾èµ–ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p>RMSNormå…¨åä¸º Root Mean Square
Normï¼Œå»é™¤äº†LayerNormä¸­çš„å‡å€¼éƒ¨åˆ†ï¼ŒåŒ…æ‹¬åˆ†å­ä¸­å‡å»å‡å€¼çš„æ“ä½œå’Œåˆ†æ¯ä¸­è®¡ç®—æ–¹å·®æ—¶å‡å»å‡å€¼çš„æ“ä½œã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåˆ†æ¯å˜æˆäº†å‡æ–¹æ ¹ã€‚
<span class="math display">\[
y = \frac{x}{RMS(x)} ,RMS(x) = \sqrt{\frac{\sum_{i=1}^Nx_i^2}{N}}
\]</span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">RMSNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>d_model<span class="token punctuation">,</span>eps<span class="token operator">=</span><span class="token number">1e-12</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
		self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span><span class="token punctuation">)</span>
		self<span class="token punctuation">.</span>beta <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span><span class="token punctuation">)</span>
	<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
		rms <span class="token operator">=</span> x<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
		out <span class="token operator">=</span> x <span class="token operator">*</span> torch<span class="token punctuation">.</span>rsqrt<span class="token punctuation">(</span>rms <span class="token operator">+</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span>
		<span class="token keyword">return</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> out <span class="token operator">+</span> self<span class="token punctuation">.</span>beta<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="attention">Attention</h3>
<p>Attentionå¯é€‰ä¸‰ç§ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">LLAMA_ATTENTION_CLASSES <span class="token operator">=</span> <span class="token punctuation">&#123;</span>
    <span class="token string">"eager"</span><span class="token punctuation">:</span> LlamaAttention<span class="token punctuation">,</span>
    <span class="token string">"flash_attention_2"</span><span class="token punctuation">:</span> LlamaFlashAttention2<span class="token punctuation">,</span>
    <span class="token string">"sdpa"</span><span class="token punctuation">:</span> LlamaSdpaAttention<span class="token punctuation">,</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>å…¶ä¸­<code>LlamaAttention</code>ä¸<code>LlamaSdpaAttention</code>çš„è®¡ç®—ç›¸åŒï¼Œåè€…ä½¿ç”¨äº†torchå®˜æ–¹çš„sdpa
APIã€‚è€Œ<code>LlamaFlashAttention2</code>æ˜¯å¯¹FlashAttentionV2çš„å®ç°ã€‚æˆ‘ä»¬è¿™é‡Œåªçœ‹<code>LlamaAttention</code>ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LlamaAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Multi-headed attention from 'Attention Is All You Need' paper"""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> LlamaConfig<span class="token punctuation">,</span> layer_idx<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config
        self<span class="token punctuation">.</span>layer_idx <span class="token operator">=</span> layer_idx
        <span class="token keyword">if</span> layer_idx <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            logger<span class="token punctuation">.</span>warning_once<span class="token punctuation">(</span>
                <span class="token string-interpolation"><span class="token string">f"Instantiating </span><span class="token interpolation"><span class="token punctuation">&#123;</span>self<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">&#125;</span></span><span class="token string"> without passing a `layer_idx` is not recommended and will "</span></span>
                <span class="token string">"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` "</span>
                <span class="token string">"when creating this class."</span>
            <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>attention_dropout <span class="token operator">=</span> config<span class="token punctuation">.</span>attention_dropout
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> config<span class="token punctuation">.</span>num_attention_heads
        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> self<span class="token punctuation">.</span>hidden_size <span class="token operator">//</span> self<span class="token punctuation">.</span>num_heads
        self<span class="token punctuation">.</span>num_key_value_heads <span class="token operator">=</span> config<span class="token punctuation">.</span>num_key_value_heads
        self<span class="token punctuation">.</span>num_key_value_groups <span class="token operator">=</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">//</span> self<span class="token punctuation">.</span>num_key_value_heads
        self<span class="token punctuation">.</span>max_position_embeddings <span class="token operator">=</span> config<span class="token punctuation">.</span>max_position_embeddings
        self<span class="token punctuation">.</span>rope_theta <span class="token operator">=</span> config<span class="token punctuation">.</span>rope_theta
        self<span class="token punctuation">.</span>is_causal <span class="token operator">=</span> <span class="token boolean">True</span>

        <span class="token keyword">if</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>head_dim <span class="token operator">*</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span> <span class="token operator">!=</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
                <span class="token string-interpolation"><span class="token string">f"hidden_size must be divisible by num_heads (got `hidden_size`: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>
                <span class="token string-interpolation"><span class="token string">f" and `num_heads`: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">&#125;</span></span><span class="token string">)."</span></span>
            <span class="token punctuation">)</span>
		<span class="token comment"># æ³¨æ„ï¼Œè¿™é‡Œåªæœ‰Qæ˜¯å®Œæ•´çš„4096Ã—4096ç»´ï¼Œè€ŒKVéƒ½æ˜¯1024Ã—1024ç»´</span>
        self<span class="token punctuation">.</span>q_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>attention_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>k_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>attention_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>v_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>attention_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>o_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>attention_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_init_rope<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>åœ¨åˆå§‹åŒ–ä¸­ï¼Œå¤§ä½“å’ŒåŸå§‹çš„Attentionæ˜¯ä¸€æ ·çš„ã€‚åŒºåˆ«çš„åœ°æ–¹åœ¨äºä½¿ç”¨äº†GQAï¼Œæ‰€ä»¥KVçš„headæ•°ä¸Qä¼šä¸åŒã€‚</p>
<p>åœ¨llama3-8Bä¸­ï¼Œå®Œæ•´çš„configå¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>
  <span class="token property">"architectures"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
    <span class="token string">"LlamaForCausalLM"</span>
  <span class="token punctuation">]</span><span class="token punctuation">,</span>
  <span class="token property">"attention_bias"</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>
  <span class="token property">"attention_dropout"</span><span class="token operator">:</span> <span class="token number">0.0</span><span class="token punctuation">,</span>
  <span class="token property">"bos_token_id"</span><span class="token operator">:</span> <span class="token number">128000</span><span class="token punctuation">,</span>
  <span class="token property">"eos_token_id"</span><span class="token operator">:</span> <span class="token number">128009</span><span class="token punctuation">,</span>
  <span class="token property">"hidden_act"</span><span class="token operator">:</span> <span class="token string">"silu"</span><span class="token punctuation">,</span>
  <span class="token property">"hidden_size"</span><span class="token operator">:</span> <span class="token number">4096</span><span class="token punctuation">,</span>
  <span class="token property">"initializer_range"</span><span class="token operator">:</span> <span class="token number">0.02</span><span class="token punctuation">,</span>
  <span class="token property">"intermediate_size"</span><span class="token operator">:</span> <span class="token number">14336</span><span class="token punctuation">,</span>
  <span class="token property">"max_position_embeddings"</span><span class="token operator">:</span> <span class="token number">8192</span><span class="token punctuation">,</span>
  <span class="token property">"model_type"</span><span class="token operator">:</span> <span class="token string">"llama"</span><span class="token punctuation">,</span>
  <span class="token property">"num_attention_heads"</span><span class="token operator">:</span> <span class="token number">32</span><span class="token punctuation">,</span>
  <span class="token property">"num_hidden_layers"</span><span class="token operator">:</span> <span class="token number">32</span><span class="token punctuation">,</span>
  <span class="token property">"num_key_value_heads"</span><span class="token operator">:</span> <span class="token number">8</span><span class="token punctuation">,</span>
  <span class="token property">"pretraining_tp"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
  <span class="token property">"rms_norm_eps"</span><span class="token operator">:</span> <span class="token number">1e-05</span><span class="token punctuation">,</span>
  <span class="token property">"rope_scaling"</span><span class="token operator">:</span> <span class="token null keyword">null</span><span class="token punctuation">,</span>
  <span class="token property">"rope_theta"</span><span class="token operator">:</span> <span class="token number">500000.0</span><span class="token punctuation">,</span>
  <span class="token property">"tie_word_embeddings"</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>
  <span class="token property">"torch_dtype"</span><span class="token operator">:</span> <span class="token string">"bfloat16"</span><span class="token punctuation">,</span>
  <span class="token property">"transformers_version"</span><span class="token operator">:</span> <span class="token string">"4.40.0.dev0"</span><span class="token punctuation">,</span>
  <span class="token property">"use_cache"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>
  <span class="token property">"vocab_size"</span><span class="token operator">:</span> <span class="token number">128256</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>å¯ä»¥çœ‹åˆ°Qçš„headæ•°æ˜¯32ï¼Œæ¯ä¸€ä¸ªheadåŒ…å«128ç»´çš„å‘é‡ã€‚è€ŒKVçš„headæ•°åªæœ‰8ï¼Œæ„å‘³ç€æ¯4ä¸ªQä¸ºä¸€ç»„ï¼Œå…±äº«ä¸€ç»„KVã€‚</p>
<p>åœ¨è®¡ç®—Attention
Scoreä¹‹å‰ï¼Œéœ€è¦è®¡ç®—å‡ºæ—‹è½¬çŸ©é˜µï¼Œä¸è¾“å…¥çš„xç›¸ä¹˜ã€‚ç”±äºé»˜è®¤æƒ…å†µä¸‹ï¼Œ<code>rope_scaling=null</code>ï¼Œæ‰€ä»¥ä¼šé‡‡ç”¨æœ€æ™®é€šçš„æ—‹è½¬çŸ©é˜µã€‚</p>
<figure>
<img src="../images/image-20240718154653674.png"
alt="image-20240718154653674" />
<figcaption aria-hidden="true">image-20240718154653674</figcaption>
</figure>
<figure>
<img src="../images/image-20240718154454439.png"
alt="image-20240718154454439" />
<figcaption aria-hidden="true">image-20240718154454439</figcaption>
</figure>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LlamaRotaryEmbedding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> max_position_embeddings<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> base<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> scaling_factor<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>scaling_factor <span class="token operator">=</span> scaling_factor
        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> dim <span class="token comment"># head_dim = 128</span>
        self<span class="token punctuation">.</span>max_position_embeddings <span class="token operator">=</span> max_position_embeddings
        self<span class="token punctuation">.</span>base <span class="token operator">=</span> base <span class="token comment"># Î¸</span>
        <span class="token comment"># 10000^(i/d) i = [0,2,4,...,126]</span>
        inv_freq <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>base <span class="token operator">**</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int64<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"inv_freq"</span><span class="token punctuation">,</span> inv_freq<span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token comment"># æœ€å¤§åºåˆ—é•¿åº¦ä¸º8192</span>
        self<span class="token punctuation">.</span>max_seq_len_cached <span class="token operator">=</span> max_position_embeddings
        <span class="token comment"># å…¬å¼ä¸­çš„mï¼Œä¹Ÿå°±æ˜¯pos</span>
        t <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>self<span class="token punctuation">.</span>max_seq_len_cached<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int64<span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>self<span class="token punctuation">.</span>inv_freq<span class="token punctuation">)</span>
        t <span class="token operator">=</span> t <span class="token operator">/</span> self<span class="token punctuation">.</span>scaling_factor
        <span class="token comment"># è®¡ç®—å¤–ç§¯ï¼Œä¹Ÿå°±æ˜¯[pos*10000^(i/d)],å…¶å®å°±æ˜¯cosä¸sinä¸­çš„è§’åº¦</span>
        freqs <span class="token operator">=</span> torch<span class="token punctuation">.</span>outer<span class="token punctuation">(</span>t<span class="token punctuation">,</span> self<span class="token punctuation">.</span>inv_freq<span class="token punctuation">)</span>
        <span class="token comment"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span>
        <span class="token comment"># åœ¨æœ€åä¸€ç»´è¿›è¡Œæ‹¼æ¥ [seq_len,2*head_dim]</span>
        emb <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>freqs<span class="token punctuation">,</span> freqs<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># ç™»è®°cos(emb)ä¸sin(emb)ï¼Œä¹Ÿå°±æ˜¯cosmÎ¸ä¸sinmÎ¸</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"_cos_cached"</span><span class="token punctuation">,</span> emb<span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>get_default_dtype<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"_sin_cached"</span><span class="token punctuation">,</span> emb<span class="token punctuation">.</span>sin<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>get_default_dtype<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        
    <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>no_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> position_ids<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># x: [bs, num_attention_heads, seq_len, head_size]</span>
        inv_freq_expanded <span class="token operator">=</span> self<span class="token punctuation">.</span>inv_freq<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>position_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        position_ids_expanded <span class="token operator">=</span> position_ids<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># Force float32 since bfloat16 loses precision on long contexts</span>
        <span class="token comment"># See https://github.com/huggingface/transformers/pull/29285</span>
        device_type <span class="token operator">=</span> x<span class="token punctuation">.</span>device<span class="token punctuation">.</span><span class="token builtin">type</span>
        device_type <span class="token operator">=</span> device_type <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>device_type<span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token keyword">and</span> device_type <span class="token operator">!=</span> <span class="token string">"mps"</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>autocast<span class="token punctuation">(</span>device_type<span class="token operator">=</span>device_type<span class="token punctuation">,</span> enabled<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            freqs <span class="token operator">=</span> <span class="token punctuation">(</span>inv_freq_expanded<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> @ position_ids_expanded<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
            emb <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>freqs<span class="token punctuation">,</span> freqs<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            cos <span class="token operator">=</span> emb<span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token punctuation">)</span>
            sin <span class="token operator">=</span> emb<span class="token punctuation">.</span>sin<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> cos<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> sin<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
    
<span class="token keyword">def</span> <span class="token function">apply_rotary_pos_emb</span><span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> cos<span class="token punctuation">,</span> sin<span class="token punctuation">,</span> position_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> unsqueeze_dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """</span>
    cos <span class="token operator">=</span> cos<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>unsqueeze_dim<span class="token punctuation">)</span>
    sin <span class="token operator">=</span> sin<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>unsqueeze_dim<span class="token punctuation">)</span>
    q_embed <span class="token operator">=</span> <span class="token punctuation">(</span>q <span class="token operator">*</span> cos<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>rotate_half<span class="token punctuation">(</span>q<span class="token punctuation">)</span> <span class="token operator">*</span> sin<span class="token punctuation">)</span>
    k_embed <span class="token operator">=</span> <span class="token punctuation">(</span>k <span class="token operator">*</span> cos<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>rotate_half<span class="token punctuation">(</span>k<span class="token punctuation">)</span> <span class="token operator">*</span> sin<span class="token punctuation">)</span>
    <span class="token keyword">return</span> q_embed<span class="token punctuation">,</span> k_embed<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>ä¸‹é¢çœ‹å‰å‘ä¼ æ’­ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
      self<span class="token punctuation">,</span>
      hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>
      attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
      position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
      past_key_value<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Cache<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
      output_attentions<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
      use_cache<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
      cache_position<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
      <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
  <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
      bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> _ <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># å…³äºå¼ é‡å¹¶è¡Œçš„å†…å®¹</span>
      <span class="token keyword">if</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>
          key_value_slicing <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_key_value_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp
          query_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span>
              <span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span>
          <span class="token punctuation">)</span>
          key_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span>key_value_slicing<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
          value_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span>key_value_slicing<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

          query_states <span class="token operator">=</span> <span class="token punctuation">[</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span> query_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span><span class="token punctuation">]</span>
          query_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>query_states<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

          key_states <span class="token operator">=</span> <span class="token punctuation">[</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span> key_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span><span class="token punctuation">]</span>
          key_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

          value_states <span class="token operator">=</span> <span class="token punctuation">[</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span> value_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span><span class="token punctuation">]</span>
          value_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>value_states<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

      <span class="token keyword">else</span><span class="token punctuation">:</span><span class="token comment"># è·å–æŠ•å½±çŸ©é˜µW_Q,W_K,W_V</span>
          query_states <span class="token operator">=</span> self<span class="token punctuation">.</span>q_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
          key_states <span class="token operator">=</span> self<span class="token punctuation">.</span>k_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
          value_states <span class="token operator">=</span> self<span class="token punctuation">.</span>v_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
<span class="token comment"># å¸¸è§„æ“ä½œï¼Œä¸ºäº†å¹¶è¡Œå°†seqç»´åº¦ä¸headç»´åº¦äº’æ¢</span>
      query_states <span class="token operator">=</span> query_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
      key_states <span class="token operator">=</span> key_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
      value_states <span class="token operator">=</span> value_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>

      past_key_value <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token string">"past_key_value"</span><span class="token punctuation">,</span> past_key_value<span class="token punctuation">)</span>
      cos<span class="token punctuation">,</span> sin <span class="token operator">=</span> self<span class="token punctuation">.</span>rotary_emb<span class="token punctuation">(</span>value_states<span class="token punctuation">,</span> position_ids<span class="token punctuation">)</span>
      <span class="token comment"># å¯¹Q Kè¿ç”¨æ—‹è½¬çŸ©é˜µ</span>
      query_states<span class="token punctuation">,</span> key_states <span class="token operator">=</span> apply_rotary_pos_emb<span class="token punctuation">(</span>query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">,</span> cos<span class="token punctuation">,</span> sin<span class="token punctuation">)</span>

      <span class="token keyword">if</span> past_key_value <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
          <span class="token comment"># sin and cos are specific to RoPE models; cache_position needed for the static cache</span>
          cache_kwargs <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">"sin"</span><span class="token punctuation">:</span> sin<span class="token punctuation">,</span> <span class="token string">"cos"</span><span class="token punctuation">:</span> cos<span class="token punctuation">,</span> <span class="token string">"cache_position"</span><span class="token punctuation">:</span> cache_position<span class="token punctuation">&#125;</span>
          key_states<span class="token punctuation">,</span> value_states <span class="token operator">=</span> past_key_value<span class="token punctuation">.</span>update<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> value_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>layer_idx<span class="token punctuation">,</span> cache_kwargs<span class="token punctuation">)</span>

      <span class="token comment"># ç”±äºè¿ç”¨äº†GQAï¼Œæ‰€ä»¥éœ€è¦é‡å¤ï¼Œæ¯4ä¸ªQå…¬ç”¨ä¸€ç»„KV</span>
      key_states <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_groups<span class="token punctuation">)</span>
      value_states <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>value_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_groups<span class="token punctuation">)</span>
<span class="token comment"># æ¥ä¸‹æ¥çš„æ“ä½œå°±ä¸Attentionçš„æ“ä½œä¸€æ¨¡ä¸€æ ·äº†</span>
      attn_weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>

      <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>  <span class="token comment"># no matter the length, we just slice it</span>
          causal_mask <span class="token operator">=</span> attention_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span> key_states<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
          attn_weights <span class="token operator">=</span> attn_weights <span class="token operator">+</span> causal_mask

      <span class="token comment"># upcast attention to fp32</span>
      attn_weights <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn_weights<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>query_states<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
      attn_weights <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>attn_weights<span class="token punctuation">,</span> p<span class="token operator">=</span>self<span class="token punctuation">.</span>attention_dropout<span class="token punctuation">,</span> training<span class="token operator">=</span>self<span class="token punctuation">.</span>training<span class="token punctuation">)</span>
      attn_output <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attn_weights<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span>

      <span class="token keyword">if</span> attn_output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
          <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
              <span class="token string-interpolation"><span class="token string">f"`attn_output` should be of size </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">, but is"</span></span>
              <span class="token string-interpolation"><span class="token string">f" </span><span class="token interpolation"><span class="token punctuation">&#123;</span>attn_output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>
          <span class="token punctuation">)</span>

      attn_output <span class="token operator">=</span> attn_output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>

      attn_output <span class="token operator">=</span> attn_output<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>

      <span class="token keyword">if</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>
          attn_output <span class="token operator">=</span> attn_output<span class="token punctuation">.</span>split<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size <span class="token operator">//</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
          o_proj_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size <span class="token operator">//</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
          attn_output <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">[</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>attn_output<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> o_proj_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
      <span class="token keyword">else</span><span class="token punctuation">:</span>
          attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>o_proj<span class="token punctuation">(</span>attn_output<span class="token punctuation">)</span>

      <span class="token keyword">if</span> <span class="token keyword">not</span> output_attentions<span class="token punctuation">:</span>
          attn_weights <span class="token operator">=</span> <span class="token boolean">None</span>

      <span class="token keyword">return</span> attn_output<span class="token punctuation">,</span> attn_weights<span class="token punctuation">,</span> past_key_value<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>å…³äºrepeat_kvçš„å®ç°å¦‚ä¸‹ï¼Œå…¶å®å°±æ˜¯å¯¹KVè¿›è¡Œå¤åˆ¶ï¼Œæ‰©å±•æˆä¸Qçš„å½¢çŠ¶ç›¸åŒçš„tensorã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">repeat_kv</span><span class="token punctuation">(</span>hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> n_rep<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """</span>
    batch<span class="token punctuation">,</span> num_key_value_heads<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>shape
    <span class="token comment"># æ¯ä¸€ä¸ªQå…±ç”¨ä¸€ä¸ªKVï¼Œé‚£å°±æ˜¯MHA</span>
    <span class="token keyword">if</span> n_rep <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> hidden_states
    <span class="token comment"># å…ˆæ‰©å±•ä¸€ä¸ªç»´åº¦ï¼Œè®©å…¶åœ¨dim3ä¸Šé‡å¤4æ¬¡ï¼Œå…ˆå˜æˆ[bcz,head,1,seq,head_dim]ï¼Œç„¶åexpandæˆ[bcz,head,4,seq,head_dim]</span>
    hidden_states <span class="token operator">=</span> hidden_states<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> num_key_value_heads<span class="token punctuation">,</span> n_rep<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim<span class="token punctuation">)</span>
    <span class="token comment"># reshapeæˆ[bcz,num_head,slen,head_dim]ï¼Œè¿™æ ·å°±ä¸Qçš„ç»´åº¦ç›¸åŒäº†</span>
    <span class="token keyword">return</span> hidden_states<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> num_key_value_heads <span class="token operator">*</span> n_rep<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="rmsnorm-1">RMSNorm</h3>
<p>åŒä¸Šï¼Œç”¨äºå½’ä¸€åŒ–æ³¨æ„åŠ›æ¨¡å—çš„è¾“å‡ºã€‚</p>
<h3 id="mlp">MLP</h3>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LlamaMLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size
        self<span class="token punctuation">.</span>intermediate_size <span class="token operator">=</span> config<span class="token punctuation">.</span>intermediate_size
        self<span class="token punctuation">.</span>gate_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>up_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>down_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>act_fn <span class="token operator">=</span> ACT2FN<span class="token punctuation">[</span>config<span class="token punctuation">.</span>hidden_act<span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">#çœç•¥äº†åœ¨å¼ é‡å¹¶è¡Œè®­ç»ƒä¸‹çš„ä»£ç </span>
        down_proj <span class="token operator">=</span> self<span class="token punctuation">.</span>down_proj<span class="token punctuation">(</span>self<span class="token punctuation">.</span>act_fn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>gate_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>up_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> down_proj<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>xä¼šæœ‰ä¸¤ä»½ï¼Œå…¶ä¸­ä¸€ä»½ä¼šç»è¿‡ä¸€ä¸ªé—¨æ§ä¿¡å·ï¼Œä¹Ÿå°±æ˜¯gate_projï¼Œåœ¨ç»è¿‡<code>silu</code>æ¿€æ´»å‡½æ•°åä¸å¦ä¸€ä»½ç»è¿‡å‡ç»´æ“ä½œçš„xè¿›è¡Œå“ˆè¾¾ç›ç§¯ã€‚å…¶ä¸­<span
class="math inline">\(silu(x) =
xâ‹…Ïƒ(x)\)</span>ï¼ŒÏƒ(x)æ˜¯sigmoidå‡½æ•°ã€‚æœ€åå†è¿›è¡Œé™ç»´ã€‚</p>
<h3 id="rmsnorm-2">RMSNorm</h3>
<p>ç”¨äºå½’ä¸€åŒ–LlamaMLPçš„è¾“å‡ºã€‚</p>
<h3 id="linear">Linear</h3>
<p>æœ¬æ–‡ä»¥<code>LlamaForCausalLM</code>ä¸ºä¾‹ï¼Œæ‰€ä»¥åœ¨æœ€åä¼šè¾“å…¥ä¸€ä¸ªLinearå±‚å¾—åˆ°logitsï¼Œä¹Ÿå°±æ˜¯æ½œåœ¨ç”Ÿæˆè¯çš„åŸå§‹åˆ†æ•°ã€‚</p>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">class LlamaForCausalLM(LlamaPreTrainedModel):
    _tied_weights_keys &#x3D; [&quot;lm_head.weight&quot;]

    def __init__(self, config):
        super().__init__(config)
        self.model &#x3D; LlamaModel(config)
        self.vocab_size &#x3D; config.vocab_size
        self.lm_head &#x3D; nn.Linear(config.hidden_size, config.vocab_size, bias&#x3D;False) # æ­¤å¤„<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>æœ¬æ–‡ä½œè€…ï¼š</strong>iroha</li><li class="post-copyright-link"><strong>æœ¬æ–‡é“¾æ¥ï¼š</strong><a href="http://example.com/post/llama.html" title="llamaç»“æ„æµ…æ">http://example.com/post/llama.html</a></li><li class="post-copyright-license"><strong>ç‰ˆæƒå£°æ˜ï¼š</strong>æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é»˜è®¤é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> è®¸å¯åè®®ã€‚</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/post/pytorch_cg.html" rel="prev" title="Pytorchçš„è®¡ç®—å›¾"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">Pytorchçš„è®¡ç®—å›¾</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/post/data_collator.html" rel="next" title="Transformersä¸­çš„DataCollator"><span class="post-nav-text">Transformersä¸­çš„DataCollator</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 â€“ 2024 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> iroha</span></div><div class="powered"><span>ç”± <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> é©±åŠ¨ v6.3.0</span><span class="footer-separator">|</span><span>ä¸»é¢˜ - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.11</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>