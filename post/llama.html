<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="iroha"><meta name="copyright" content="iroha"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>llama结构浅析 | iroha</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"example.com","root":"/","title":"不想摆烂","version":"1.10.11","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js" defer></script><script src="/js/load-aplayer.js" defer></script><meta name="description" content="以下的内容来自于LLaMAgithub仓库与Huggingface中的LLaMA实现。 结构 llama的结构自始至终没有什么改变，无非就是每一代都把GQA的实现下放到小参数上。   image-20240708001842029  大体的结构和GPT-2还是非常相似的，Decoder-only，前置Norm等等。具体结构可见下面的代码： class LlamaModel(Llam">
<meta property="og:type" content="article">
<meta property="og:title" content="llama结构浅析">
<meta property="og:url" content="http://example.com/post/llama.html">
<meta property="og:site_name" content="iroha">
<meta property="og:description" content="以下的内容来自于LLaMAgithub仓库与Huggingface中的LLaMA实现。 结构 llama的结构自始至终没有什么改变，无非就是每一代都把GQA的实现下放到小参数上。   image-20240708001842029  大体的结构和GPT-2还是非常相似的，Decoder-only，前置Norm等等。具体结构可见下面的代码： class LlamaModel(Llam">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20240708001842029.png">
<meta property="og:image" content="http://example.com/images/image-20240718154653674.png">
<meta property="og:image" content="http://example.com/images/image-20240718154454439.png">
<meta property="article:published_time" content="2024-07-18T08:08:00.000Z">
<meta property="article:modified_time" content="2024-07-23T02:00:30.048Z">
<meta property="article:author" content="iroha">
<meta property="article:tag" content="大模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20240708001842029.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="iroha"><img width="96" loading="lazy" src="/yun.png" alt="iroha"><span class="site-author-status" title="永远相信美好的事情即将发生">😊</span></a><div class="site-author-name"><a href="/about/">iroha</a></div><span class="site-name">iroha</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">17</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">4</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">2</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="友链" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E6%9E%84"><span class="toc-number">1.</span> <span class="toc-text">结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#embedding"><span class="toc-number">1.1.</span> <span class="toc-text">Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#decoder-layer"><span class="toc-number">1.2.</span> <span class="toc-text">Decoder Layer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#rmsnorm"><span class="toc-number">1.2.1.</span> <span class="toc-text">RMSNorm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#attention"><span class="toc-number">1.2.2.</span> <span class="toc-text">Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rmsnorm-1"><span class="toc-number">1.2.3.</span> <span class="toc-text">RMSNorm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mlp"><span class="toc-number">1.2.4.</span> <span class="toc-text">MLP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rmsnorm-2"><span class="toc-number">1.2.5.</span> <span class="toc-text">RMSNorm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#linear"><span class="toc-number">1.2.6.</span> <span class="toc-text">Linear</span></a></li></ol></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://example.com/post/llama.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="iroha"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="iroha"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">llama结构浅析</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2024-07-18 16:08:00" itemprop="dateCreated datePublished" datetime="2024-07-18T16:08:00+08:00">2024-07-18</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="修改时间：2024-07-23 10:00:30" itemprop="dateModified" datetime="2024-07-23T10:00:30+08:00">2024-07-23</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">NLP</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">大模型</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>以下的内容来自于<a target="_blank" rel="noopener" href='https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md'>LLaMAgithub仓库</a>与Huggingface中的LLaMA实现。</p>
<h1 id="结构">结构</h1>
<p>llama的结构自始至终没有什么改变，无非就是每一代都把GQA的实现下放到小参数上。</p>
<figure>
<img src="../images/image-20240708001842029.png"
alt="image-20240708001842029" />
<figcaption aria-hidden="true">image-20240708001842029</figcaption>
</figure>
<p>大体的结构和GPT-2还是非常相似的，Decoder-only，前置Norm等等。具体结构可见下面的代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LlamaModel</span><span class="token punctuation">(</span>LlamaPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]

    Args:
        config: LlamaConfig
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> LlamaConfig<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>padding_idx <span class="token operator">=</span> config<span class="token punctuation">.</span>pad_token_id
        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> config<span class="token punctuation">.</span>vocab_size

        self<span class="token punctuation">.</span>embed_tokens <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>padding_idx<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>
            <span class="token punctuation">[</span>LlamaDecoderLayer<span class="token punctuation">(</span>config<span class="token punctuation">,</span> layer_idx<span class="token punctuation">)</span> <span class="token keyword">for</span> layer_idx <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>num_hidden_layers<span class="token punctuation">)</span><span class="token punctuation">]</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> LlamaRMSNorm<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>rms_norm_eps<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>gradient_checkpointing <span class="token operator">=</span> <span class="token boolean">False</span>

        <span class="token comment"># Initialize weights and apply final processing</span>
        self<span class="token punctuation">.</span>post_init<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
<span class="token keyword">class</span> <span class="token class-name">LlamaDecoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> LlamaConfig<span class="token punctuation">,</span> layer_idx<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size

        self<span class="token punctuation">.</span>self_attn <span class="token operator">=</span> LLAMA_ATTENTION_CLASSES<span class="token punctuation">[</span>config<span class="token punctuation">.</span>_attn_implementation<span class="token punctuation">]</span><span class="token punctuation">(</span>config<span class="token operator">=</span>config<span class="token punctuation">,</span> layer_idx<span class="token operator">=</span>layer_idx<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>mlp <span class="token operator">=</span> LlamaMLP<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>input_layernorm <span class="token operator">=</span> LlamaRMSNorm<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>rms_norm_eps<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>post_attention_layernorm <span class="token operator">=</span> LlamaRMSNorm<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>rms_norm_eps<span class="token punctuation">)</span>
        
     
<span class="token keyword">class</span> <span class="token class-name">LlamaForCausalLM</span><span class="token punctuation">(</span>LlamaPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    _tied_weights_keys <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"lm_head.weight"</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> LlamaModel<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> config<span class="token punctuation">.</span>vocab_size
        self<span class="token punctuation">.</span>lm_head <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

        <span class="token comment"># Initialize weights and apply final processing</span>
        self<span class="token punctuation">.</span>post_init<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="embedding">Embedding</h2>
<p>没什么好说的，BERT同款，可训练的Embedding。</p>
<h2 id="decoder-layer">Decoder Layer</h2>
<p>Decoder Layer主要由注意力层，MLP与RMSNorm组成。前向传播代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">      residual <span class="token operator">=</span> hidden_states <span class="token comment"># 1. 保留残差x</span>

      hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>input_layernorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span> <span class="token comment">#2. 计算前置RMSNorm</span>

      <span class="token comment">#3. Self Attention</span>
      hidden_states<span class="token punctuation">,</span> self_attn_weights<span class="token punctuation">,</span> present_key_value <span class="token operator">=</span> self<span class="token punctuation">.</span>self_attn<span class="token punctuation">(</span>
          hidden_states<span class="token operator">=</span>hidden_states<span class="token punctuation">,</span>
          attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
          position_ids<span class="token operator">=</span>position_ids<span class="token punctuation">,</span>
          past_key_value<span class="token operator">=</span>past_key_value<span class="token punctuation">,</span>
          output_attentions<span class="token operator">=</span>output_attentions<span class="token punctuation">,</span>
          use_cache<span class="token operator">=</span>use_cache<span class="token punctuation">,</span>
          cache_position<span class="token operator">=</span>cache_position<span class="token punctuation">,</span>
          <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
      <span class="token punctuation">)</span>
      <span class="token comment"># 4. 计算残差</span>
      hidden_states <span class="token operator">=</span> residual <span class="token operator">+</span> hidden_states

      <span class="token comment"># 保留残差</span>
      residual <span class="token operator">=</span> hidden_states
      <span class="token comment"># 6. MLP之前的RMSNorm</span>
      hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
      <span class="token comment"># 7.计算MLP</span>
      hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
      <span class="token comment"># 8.计算残差</span>
      hidden_states <span class="token operator">=</span> residual <span class="token operator">+</span> hidden_states

      outputs <span class="token operator">=</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span><span class="token punctuation">)</span>
<span class="token comment"># 是否输出Attention Weight</span>
      <span class="token keyword">if</span> output_attentions<span class="token punctuation">:</span>
          outputs <span class="token operator">+=</span> <span class="token punctuation">(</span>self_attn_weights<span class="token punctuation">,</span><span class="token punctuation">)</span>
<span class="token comment"># KV Cache</span>
      <span class="token keyword">if</span> use_cache<span class="token punctuation">:</span>
          outputs <span class="token operator">+=</span> <span class="token punctuation">(</span>present_key_value<span class="token punctuation">,</span><span class="token punctuation">)</span>

      <span class="token keyword">return</span> outputs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="rmsnorm">RMSNorm</h3>
<p>原论文地址：https://arxiv.org/pdf/1910.07467.pdf</p>
<p>总结一下就是省略计算mean的操作不会影响性能，但可以节省大量计算开销。</p>
<p>作为对比，我们首先回顾一下LayerNorm。 <span class="math display">\[
y = \frac{x-E(x)}{\sqrt{var(x)+\epsilon}}
\]</span></p>
<p>其中x为对应dim上的一组数。</p>
<p>代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LayerNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>d_model<span class="token punctuation">,</span>eps<span class="token operator">=</span><span class="token number">1e-12</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
		self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>beta <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>eps <span class="token operator">=</span> <span class="token number">1e-12</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        var<span class="token punctuation">,</span>mean <span class="token operator">=</span> torch<span class="token punctuation">.</span>var_mean<span class="token punctuation">(</span>x<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> <span class="token punctuation">(</span>x<span class="token operator">-</span>mean<span class="token punctuation">)</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>var<span class="token operator">+</span>self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> out <span class="token operator">+</span> self<span class="token punctuation">.</span>beta
        <span class="token keyword">return</span> out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>LayerNorm通过归一化处理，防止每一层的分布发生剧烈变化，减少Internal
Covariate
Shift，让分布趋于稳定，使得梯度传播更加稳定，有助于减少梯度消失和梯度爆炸。也减少了对训练数据分布的依赖，提高模型的泛化能力。</p>
<p>RMSNorm全名为 Root Mean Square
Norm，去除了LayerNorm中的均值部分，包括分子中减去均值的操作和分母中计算方差时减去均值的操作。也就是说，分母变成了均方根。
<span class="math display">\[
y = \frac{x}{RMS(x)} ,RMS(x) = \sqrt{\frac{\sum_{i=1}^Nx_i^2}{N}}
\]</span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">RMSNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>d_model<span class="token punctuation">,</span>eps<span class="token operator">=</span><span class="token number">1e-12</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
		self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span><span class="token punctuation">)</span>
		self<span class="token punctuation">.</span>beta <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span><span class="token punctuation">)</span>
	<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
		rms <span class="token operator">=</span> x<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
		out <span class="token operator">=</span> x <span class="token operator">*</span> torch<span class="token punctuation">.</span>rsqrt<span class="token punctuation">(</span>rms <span class="token operator">+</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span>
		<span class="token keyword">return</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> out <span class="token operator">+</span> self<span class="token punctuation">.</span>beta<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="attention">Attention</h3>
<p>Attention可选三种。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">LLAMA_ATTENTION_CLASSES <span class="token operator">=</span> <span class="token punctuation">&#123;</span>
    <span class="token string">"eager"</span><span class="token punctuation">:</span> LlamaAttention<span class="token punctuation">,</span>
    <span class="token string">"flash_attention_2"</span><span class="token punctuation">:</span> LlamaFlashAttention2<span class="token punctuation">,</span>
    <span class="token string">"sdpa"</span><span class="token punctuation">:</span> LlamaSdpaAttention<span class="token punctuation">,</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>其中<code>LlamaAttention</code>与<code>LlamaSdpaAttention</code>的计算相同，后者使用了torch官方的sdpa
API。而<code>LlamaFlashAttention2</code>是对FlashAttentionV2的实现。我们这里只看<code>LlamaAttention</code>。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LlamaAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Multi-headed attention from 'Attention Is All You Need' paper"""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> LlamaConfig<span class="token punctuation">,</span> layer_idx<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config
        self<span class="token punctuation">.</span>layer_idx <span class="token operator">=</span> layer_idx
        <span class="token keyword">if</span> layer_idx <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            logger<span class="token punctuation">.</span>warning_once<span class="token punctuation">(</span>
                <span class="token string-interpolation"><span class="token string">f"Instantiating </span><span class="token interpolation"><span class="token punctuation">&#123;</span>self<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">&#125;</span></span><span class="token string"> without passing a `layer_idx` is not recommended and will "</span></span>
                <span class="token string">"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` "</span>
                <span class="token string">"when creating this class."</span>
            <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>attention_dropout <span class="token operator">=</span> config<span class="token punctuation">.</span>attention_dropout
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> config<span class="token punctuation">.</span>num_attention_heads
        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> self<span class="token punctuation">.</span>hidden_size <span class="token operator">//</span> self<span class="token punctuation">.</span>num_heads
        self<span class="token punctuation">.</span>num_key_value_heads <span class="token operator">=</span> config<span class="token punctuation">.</span>num_key_value_heads
        self<span class="token punctuation">.</span>num_key_value_groups <span class="token operator">=</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">//</span> self<span class="token punctuation">.</span>num_key_value_heads
        self<span class="token punctuation">.</span>max_position_embeddings <span class="token operator">=</span> config<span class="token punctuation">.</span>max_position_embeddings
        self<span class="token punctuation">.</span>rope_theta <span class="token operator">=</span> config<span class="token punctuation">.</span>rope_theta
        self<span class="token punctuation">.</span>is_causal <span class="token operator">=</span> <span class="token boolean">True</span>

        <span class="token keyword">if</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>head_dim <span class="token operator">*</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span> <span class="token operator">!=</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
                <span class="token string-interpolation"><span class="token string">f"hidden_size must be divisible by num_heads (got `hidden_size`: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>
                <span class="token string-interpolation"><span class="token string">f" and `num_heads`: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">&#125;</span></span><span class="token string">)."</span></span>
            <span class="token punctuation">)</span>
		<span class="token comment"># 注意，这里只有Q是完整的4096×4096维，而KV都是1024×1024维</span>
        self<span class="token punctuation">.</span>q_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>attention_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>k_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>attention_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>v_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>attention_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>o_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> bias<span class="token operator">=</span>config<span class="token punctuation">.</span>attention_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_init_rope<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>在初始化中，大体和原始的Attention是一样的。区别的地方在于使用了GQA，所以KV的head数与Q会不同。</p>
<p>在llama3-8B中，完整的config如下：</p>
<pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>
  <span class="token property">"architectures"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
    <span class="token string">"LlamaForCausalLM"</span>
  <span class="token punctuation">]</span><span class="token punctuation">,</span>
  <span class="token property">"attention_bias"</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>
  <span class="token property">"attention_dropout"</span><span class="token operator">:</span> <span class="token number">0.0</span><span class="token punctuation">,</span>
  <span class="token property">"bos_token_id"</span><span class="token operator">:</span> <span class="token number">128000</span><span class="token punctuation">,</span>
  <span class="token property">"eos_token_id"</span><span class="token operator">:</span> <span class="token number">128009</span><span class="token punctuation">,</span>
  <span class="token property">"hidden_act"</span><span class="token operator">:</span> <span class="token string">"silu"</span><span class="token punctuation">,</span>
  <span class="token property">"hidden_size"</span><span class="token operator">:</span> <span class="token number">4096</span><span class="token punctuation">,</span>
  <span class="token property">"initializer_range"</span><span class="token operator">:</span> <span class="token number">0.02</span><span class="token punctuation">,</span>
  <span class="token property">"intermediate_size"</span><span class="token operator">:</span> <span class="token number">14336</span><span class="token punctuation">,</span>
  <span class="token property">"max_position_embeddings"</span><span class="token operator">:</span> <span class="token number">8192</span><span class="token punctuation">,</span>
  <span class="token property">"model_type"</span><span class="token operator">:</span> <span class="token string">"llama"</span><span class="token punctuation">,</span>
  <span class="token property">"num_attention_heads"</span><span class="token operator">:</span> <span class="token number">32</span><span class="token punctuation">,</span>
  <span class="token property">"num_hidden_layers"</span><span class="token operator">:</span> <span class="token number">32</span><span class="token punctuation">,</span>
  <span class="token property">"num_key_value_heads"</span><span class="token operator">:</span> <span class="token number">8</span><span class="token punctuation">,</span>
  <span class="token property">"pretraining_tp"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
  <span class="token property">"rms_norm_eps"</span><span class="token operator">:</span> <span class="token number">1e-05</span><span class="token punctuation">,</span>
  <span class="token property">"rope_scaling"</span><span class="token operator">:</span> <span class="token null keyword">null</span><span class="token punctuation">,</span>
  <span class="token property">"rope_theta"</span><span class="token operator">:</span> <span class="token number">500000.0</span><span class="token punctuation">,</span>
  <span class="token property">"tie_word_embeddings"</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>
  <span class="token property">"torch_dtype"</span><span class="token operator">:</span> <span class="token string">"bfloat16"</span><span class="token punctuation">,</span>
  <span class="token property">"transformers_version"</span><span class="token operator">:</span> <span class="token string">"4.40.0.dev0"</span><span class="token punctuation">,</span>
  <span class="token property">"use_cache"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>
  <span class="token property">"vocab_size"</span><span class="token operator">:</span> <span class="token number">128256</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>可以看到Q的head数是32，每一个head包含128维的向量。而KV的head数只有8，意味着每4个Q为一组，共享一组KV。</p>
<p>在计算Attention
Score之前，需要计算出旋转矩阵，与输入的x相乘。由于默认情况下，<code>rope_scaling=null</code>，所以会采用最普通的旋转矩阵。</p>
<figure>
<img src="../images/image-20240718154653674.png"
alt="image-20240718154653674" />
<figcaption aria-hidden="true">image-20240718154653674</figcaption>
</figure>
<figure>
<img src="../images/image-20240718154454439.png"
alt="image-20240718154454439" />
<figcaption aria-hidden="true">image-20240718154454439</figcaption>
</figure>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LlamaRotaryEmbedding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> max_position_embeddings<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> base<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> scaling_factor<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>scaling_factor <span class="token operator">=</span> scaling_factor
        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> dim <span class="token comment"># head_dim = 128</span>
        self<span class="token punctuation">.</span>max_position_embeddings <span class="token operator">=</span> max_position_embeddings
        self<span class="token punctuation">.</span>base <span class="token operator">=</span> base <span class="token comment"># θ</span>
        <span class="token comment"># 10000^(i/d) i = [0,2,4,...,126]</span>
        inv_freq <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>base <span class="token operator">**</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int64<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"inv_freq"</span><span class="token punctuation">,</span> inv_freq<span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token comment"># 最大序列长度为8192</span>
        self<span class="token punctuation">.</span>max_seq_len_cached <span class="token operator">=</span> max_position_embeddings
        <span class="token comment"># 公式中的m，也就是pos</span>
        t <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>self<span class="token punctuation">.</span>max_seq_len_cached<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int64<span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>self<span class="token punctuation">.</span>inv_freq<span class="token punctuation">)</span>
        t <span class="token operator">=</span> t <span class="token operator">/</span> self<span class="token punctuation">.</span>scaling_factor
        <span class="token comment"># 计算外积，也就是[pos*10000^(i/d)],其实就是cos与sin中的角度</span>
        freqs <span class="token operator">=</span> torch<span class="token punctuation">.</span>outer<span class="token punctuation">(</span>t<span class="token punctuation">,</span> self<span class="token punctuation">.</span>inv_freq<span class="token punctuation">)</span>
        <span class="token comment"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span>
        <span class="token comment"># 在最后一维进行拼接 [seq_len,2*head_dim]</span>
        emb <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>freqs<span class="token punctuation">,</span> freqs<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 登记cos(emb)与sin(emb)，也就是cosmθ与sinmθ</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"_cos_cached"</span><span class="token punctuation">,</span> emb<span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>get_default_dtype<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"_sin_cached"</span><span class="token punctuation">,</span> emb<span class="token punctuation">.</span>sin<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>get_default_dtype<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        
    <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>no_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> position_ids<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># x: [bs, num_attention_heads, seq_len, head_size]</span>
        inv_freq_expanded <span class="token operator">=</span> self<span class="token punctuation">.</span>inv_freq<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>position_ids<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        position_ids_expanded <span class="token operator">=</span> position_ids<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># Force float32 since bfloat16 loses precision on long contexts</span>
        <span class="token comment"># See https://github.com/huggingface/transformers/pull/29285</span>
        device_type <span class="token operator">=</span> x<span class="token punctuation">.</span>device<span class="token punctuation">.</span><span class="token builtin">type</span>
        device_type <span class="token operator">=</span> device_type <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>device_type<span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token keyword">and</span> device_type <span class="token operator">!=</span> <span class="token string">"mps"</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>autocast<span class="token punctuation">(</span>device_type<span class="token operator">=</span>device_type<span class="token punctuation">,</span> enabled<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            freqs <span class="token operator">=</span> <span class="token punctuation">(</span>inv_freq_expanded<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> @ position_ids_expanded<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
            emb <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>freqs<span class="token punctuation">,</span> freqs<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            cos <span class="token operator">=</span> emb<span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token punctuation">)</span>
            sin <span class="token operator">=</span> emb<span class="token punctuation">.</span>sin<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> cos<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> sin<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
    
<span class="token keyword">def</span> <span class="token function">apply_rotary_pos_emb</span><span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> cos<span class="token punctuation">,</span> sin<span class="token punctuation">,</span> position_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> unsqueeze_dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """</span>
    cos <span class="token operator">=</span> cos<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>unsqueeze_dim<span class="token punctuation">)</span>
    sin <span class="token operator">=</span> sin<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>unsqueeze_dim<span class="token punctuation">)</span>
    q_embed <span class="token operator">=</span> <span class="token punctuation">(</span>q <span class="token operator">*</span> cos<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>rotate_half<span class="token punctuation">(</span>q<span class="token punctuation">)</span> <span class="token operator">*</span> sin<span class="token punctuation">)</span>
    k_embed <span class="token operator">=</span> <span class="token punctuation">(</span>k <span class="token operator">*</span> cos<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>rotate_half<span class="token punctuation">(</span>k<span class="token punctuation">)</span> <span class="token operator">*</span> sin<span class="token punctuation">)</span>
    <span class="token keyword">return</span> q_embed<span class="token punctuation">,</span> k_embed<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>下面看前向传播。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
      self<span class="token punctuation">,</span>
      hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>
      attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
      position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
      past_key_value<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Cache<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
      output_attentions<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
      use_cache<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
      cache_position<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
      <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
  <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
      bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> _ <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 关于张量并行的内容</span>
      <span class="token keyword">if</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>
          key_value_slicing <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_key_value_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp
          query_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span>
              <span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span>
          <span class="token punctuation">)</span>
          key_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span>key_value_slicing<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
          value_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span>key_value_slicing<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

          query_states <span class="token operator">=</span> <span class="token punctuation">[</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span> query_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span><span class="token punctuation">]</span>
          query_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>query_states<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

          key_states <span class="token operator">=</span> <span class="token punctuation">[</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span> key_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span><span class="token punctuation">]</span>
          key_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

          value_states <span class="token operator">=</span> <span class="token punctuation">[</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span> value_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span><span class="token punctuation">]</span>
          value_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>value_states<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

      <span class="token keyword">else</span><span class="token punctuation">:</span><span class="token comment"># 获取投影矩阵W_Q,W_K,W_V</span>
          query_states <span class="token operator">=</span> self<span class="token punctuation">.</span>q_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
          key_states <span class="token operator">=</span> self<span class="token punctuation">.</span>k_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
          value_states <span class="token operator">=</span> self<span class="token punctuation">.</span>v_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
<span class="token comment"># 常规操作，为了并行将seq维度与head维度互换</span>
      query_states <span class="token operator">=</span> query_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
      key_states <span class="token operator">=</span> key_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
      value_states <span class="token operator">=</span> value_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>

      past_key_value <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token string">"past_key_value"</span><span class="token punctuation">,</span> past_key_value<span class="token punctuation">)</span>
      cos<span class="token punctuation">,</span> sin <span class="token operator">=</span> self<span class="token punctuation">.</span>rotary_emb<span class="token punctuation">(</span>value_states<span class="token punctuation">,</span> position_ids<span class="token punctuation">)</span>
      <span class="token comment"># 对Q K运用旋转矩阵</span>
      query_states<span class="token punctuation">,</span> key_states <span class="token operator">=</span> apply_rotary_pos_emb<span class="token punctuation">(</span>query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">,</span> cos<span class="token punctuation">,</span> sin<span class="token punctuation">)</span>

      <span class="token keyword">if</span> past_key_value <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
          <span class="token comment"># sin and cos are specific to RoPE models; cache_position needed for the static cache</span>
          cache_kwargs <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">"sin"</span><span class="token punctuation">:</span> sin<span class="token punctuation">,</span> <span class="token string">"cos"</span><span class="token punctuation">:</span> cos<span class="token punctuation">,</span> <span class="token string">"cache_position"</span><span class="token punctuation">:</span> cache_position<span class="token punctuation">&#125;</span>
          key_states<span class="token punctuation">,</span> value_states <span class="token operator">=</span> past_key_value<span class="token punctuation">.</span>update<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> value_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>layer_idx<span class="token punctuation">,</span> cache_kwargs<span class="token punctuation">)</span>

      <span class="token comment"># 由于运用了GQA，所以需要重复，每4个Q公用一组KV</span>
      key_states <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>key_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_groups<span class="token punctuation">)</span>
      value_states <span class="token operator">=</span> repeat_kv<span class="token punctuation">(</span>value_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_groups<span class="token punctuation">)</span>
<span class="token comment"># 接下来的操作就与Attention的操作一模一样了</span>
      attn_weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>

      <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>  <span class="token comment"># no matter the length, we just slice it</span>
          causal_mask <span class="token operator">=</span> attention_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span> key_states<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
          attn_weights <span class="token operator">=</span> attn_weights <span class="token operator">+</span> causal_mask

      <span class="token comment"># upcast attention to fp32</span>
      attn_weights <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn_weights<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>query_states<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
      attn_weights <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>attn_weights<span class="token punctuation">,</span> p<span class="token operator">=</span>self<span class="token punctuation">.</span>attention_dropout<span class="token punctuation">,</span> training<span class="token operator">=</span>self<span class="token punctuation">.</span>training<span class="token punctuation">)</span>
      attn_output <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attn_weights<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span>

      <span class="token keyword">if</span> attn_output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
          <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
              <span class="token string-interpolation"><span class="token string">f"`attn_output` should be of size </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">, but is"</span></span>
              <span class="token string-interpolation"><span class="token string">f" </span><span class="token interpolation"><span class="token punctuation">&#123;</span>attn_output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>
          <span class="token punctuation">)</span>

      attn_output <span class="token operator">=</span> attn_output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>

      attn_output <span class="token operator">=</span> attn_output<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> q_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>

      <span class="token keyword">if</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>
          attn_output <span class="token operator">=</span> attn_output<span class="token punctuation">.</span>split<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size <span class="token operator">//</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
          o_proj_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size <span class="token operator">//</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
          attn_output <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">[</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>attn_output<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> o_proj_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
      <span class="token keyword">else</span><span class="token punctuation">:</span>
          attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>o_proj<span class="token punctuation">(</span>attn_output<span class="token punctuation">)</span>

      <span class="token keyword">if</span> <span class="token keyword">not</span> output_attentions<span class="token punctuation">:</span>
          attn_weights <span class="token operator">=</span> <span class="token boolean">None</span>

      <span class="token keyword">return</span> attn_output<span class="token punctuation">,</span> attn_weights<span class="token punctuation">,</span> past_key_value<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>关于repeat_kv的实现如下，其实就是对KV进行复制，扩展成与Q的形状相同的tensor。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">repeat_kv</span><span class="token punctuation">(</span>hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> n_rep<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """</span>
    batch<span class="token punctuation">,</span> num_key_value_heads<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>shape
    <span class="token comment"># 每一个Q共用一个KV，那就是MHA</span>
    <span class="token keyword">if</span> n_rep <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> hidden_states
    <span class="token comment"># 先扩展一个维度，让其在dim3上重复4次，先变成[bcz,head,1,seq,head_dim]，然后expand成[bcz,head,4,seq,head_dim]</span>
    hidden_states <span class="token operator">=</span> hidden_states<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> num_key_value_heads<span class="token punctuation">,</span> n_rep<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim<span class="token punctuation">)</span>
    <span class="token comment"># reshape成[bcz,num_head,slen,head_dim]，这样就与Q的维度相同了</span>
    <span class="token keyword">return</span> hidden_states<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> num_key_value_heads <span class="token operator">*</span> n_rep<span class="token punctuation">,</span> slen<span class="token punctuation">,</span> head_dim<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="rmsnorm-1">RMSNorm</h3>
<p>同上，用于归一化注意力模块的输出。</p>
<h3 id="mlp">MLP</h3>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LlamaMLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size
        self<span class="token punctuation">.</span>intermediate_size <span class="token operator">=</span> config<span class="token punctuation">.</span>intermediate_size
        self<span class="token punctuation">.</span>gate_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>up_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>down_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>act_fn <span class="token operator">=</span> ACT2FN<span class="token punctuation">[</span>config<span class="token punctuation">.</span>hidden_act<span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">#省略了在张量并行训练下的代码</span>
        down_proj <span class="token operator">=</span> self<span class="token punctuation">.</span>down_proj<span class="token punctuation">(</span>self<span class="token punctuation">.</span>act_fn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>gate_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>up_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> down_proj<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>x会有两份，其中一份会经过一个门控信号，也就是gate_proj，在经过<code>silu</code>激活函数后与另一份经过升维操作的x进行哈达玛积。其中<span
class="math inline">\(silu(x) =
x⋅σ(x)\)</span>，σ(x)是sigmoid函数。最后再进行降维。</p>
<h3 id="rmsnorm-2">RMSNorm</h3>
<p>用于归一化LlamaMLP的输出。</p>
<h3 id="linear">Linear</h3>
<p>本文以<code>LlamaForCausalLM</code>为例，所以在最后会输入一个Linear层得到logits，也就是潜在生成词的原始分数。</p>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">class LlamaForCausalLM(LlamaPreTrainedModel):
    _tied_weights_keys &#x3D; [&quot;lm_head.weight&quot;]

    def __init__(self, config):
        super().__init__(config)
        self.model &#x3D; LlamaModel(config)
        self.vocab_size &#x3D; config.vocab_size
        self.lm_head &#x3D; nn.Linear(config.hidden_size, config.vocab_size, bias&#x3D;False) # 此处<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>iroha</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://example.com/post/llama.html" title="llama结构浅析">http://example.com/post/llama.html</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/post/pytorch_cg.html" rel="prev" title="Pytorch的计算图"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">Pytorch的计算图</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/post/data_collator.html" rel="next" title="Transformers中的DataCollator"><span class="post-nav-text">Transformers中的DataCollator</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2024 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> iroha</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.11</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>