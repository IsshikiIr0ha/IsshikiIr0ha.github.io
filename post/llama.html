<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="iroha"><meta name="copyright" content="iroha"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>llama结构浅析 | iroha</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"example.com","root":"/","title":"不想摆烂","version":"1.10.9","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><meta name="description" content="以下的内容来自于LLaMAgithub仓库与Huggingface中的LLaMA实现。 结构 llama的结构自始至终没有什么改变，无非就是每一代都把GQA的实现下放到小参数上。   image-20240708001842029  大体的结构和GPT-2还是非常相似的，Decoder-only，前置Norm等等。具体结构可见下面的代码： class LlamaModel(Llam">
<meta property="og:type" content="article">
<meta property="og:title" content="llama结构浅析">
<meta property="og:url" content="http://example.com/post/llama.html">
<meta property="og:site_name" content="iroha">
<meta property="og:description" content="以下的内容来自于LLaMAgithub仓库与Huggingface中的LLaMA实现。 结构 llama的结构自始至终没有什么改变，无非就是每一代都把GQA的实现下放到小参数上。   image-20240708001842029  大体的结构和GPT-2还是非常相似的，Decoder-only，前置Norm等等。具体结构可见下面的代码： class LlamaModel(Llam">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20240708001842029.png">
<meta property="og:image" content="http://example.com/images/image-20240718154653674.png">
<meta property="og:image" content="http://example.com/images/image-20240718154454439.png">
<meta property="article:published_time" content="2024-07-18T08:08:00.000Z">
<meta property="article:modified_time" content="2024-07-22T09:33:37.060Z">
<meta property="article:author" content="iroha">
<meta property="article:tag" content="大模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20240708001842029.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="iroha"><img width="96" loading="lazy" src="/yun.png" alt="iroha"><span class="site-author-status" title="永远相信美好的事情即将发生">😊</span></a><div class="site-author-name"><a href="/about/">iroha</a></div><span class="site-name">iroha</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">14</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">4</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">2</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="友链" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E6%9E%84"><span class="toc-number">1.</span> <span class="toc-text">结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#embedding"><span class="toc-number">1.1.</span> <span class="toc-text">Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#decoder-layer"><span class="toc-number">1.2.</span> <span class="toc-text">Decoder Layer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#rmsnorm"><span class="toc-number">1.2.1.</span> <span class="toc-text">RMSNorm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#attention"><span class="toc-number">1.2.2.</span> <span class="toc-text">Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rmsnorm-1"><span class="toc-number">1.2.3.</span> <span class="toc-text">RMSNorm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mlp"><span class="toc-number">1.2.4.</span> <span class="toc-text">MLP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rmsnorm-2"><span class="toc-number">1.2.5.</span> <span class="toc-text">RMSNorm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#linear"><span class="toc-number">1.2.6.</span> <span class="toc-text">Linear</span></a></li></ol></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://example.com/post/llama.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="iroha"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="iroha"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">llama结构浅析</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2024-07-18 16:08:00" itemprop="dateCreated datePublished" datetime="2024-07-18T16:08:00+08:00">2024-07-18</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="修改时间：2024-07-22 17:33:37" itemprop="dateModified" datetime="2024-07-22T17:33:37+08:00">2024-07-22</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">NLP</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">大模型</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>以下的内容来自于<a target="_blank" rel="noopener" href='https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md'>LLaMAgithub仓库</a>与Huggingface中的LLaMA实现。</p>
<h1 id="结构">结构</h1>
<p>llama的结构自始至终没有什么改变，无非就是每一代都把GQA的实现下放到小参数上。</p>
<figure>
<img src="../images/image-20240708001842029.png"
alt="image-20240708001842029" />
<figcaption aria-hidden="true">image-20240708001842029</figcaption>
</figure>
<p>大体的结构和GPT-2还是非常相似的，Decoder-only，前置Norm等等。具体结构可见下面的代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LlamaModel(LlamaPreTrainedModel):
    &quot;&quot;&quot;
    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [&#96;LlamaDecoderLayer&#96;]

    Args:
        config: LlamaConfig
    &quot;&quot;&quot;

    def __init__(self, config: LlamaConfig):
        super().__init__(config)
        self.padding_idx &#x3D; config.pad_token_id
        self.vocab_size &#x3D; config.vocab_size

        self.embed_tokens &#x3D; nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers &#x3D; nn.ModuleList(
            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm &#x3D; LlamaRMSNorm(config.hidden_size, eps&#x3D;config.rms_norm_eps)
        self.gradient_checkpointing &#x3D; False

        # Initialize weights and apply final processing
        self.post_init()
        
class LlamaDecoderLayer(nn.Module):
    def __init__(self, config: LlamaConfig, layer_idx: int):
        super().__init__()
        self.hidden_size &#x3D; config.hidden_size

        self.self_attn &#x3D; LLAMA_ATTENTION_CLASSES[config._attn_implementation](config&#x3D;config, layer_idx&#x3D;layer_idx)

        self.mlp &#x3D; LlamaMLP(config)
        self.input_layernorm &#x3D; LlamaRMSNorm(config.hidden_size, eps&#x3D;config.rms_norm_eps)
        self.post_attention_layernorm &#x3D; LlamaRMSNorm(config.hidden_size, eps&#x3D;config.rms_norm_eps)
        
     
class LlamaForCausalLM(LlamaPreTrainedModel):
    _tied_weights_keys &#x3D; [&quot;lm_head.weight&quot;]

    def __init__(self, config):
        super().__init__(config)
        self.model &#x3D; LlamaModel(config)
        self.vocab_size &#x3D; config.vocab_size
        self.lm_head &#x3D; nn.Linear(config.hidden_size, config.vocab_size, bias&#x3D;False)

        # Initialize weights and apply final processing
        self.post_init()</code></pre>
<h2 id="embedding">Embedding</h2>
<p>没什么好说的，BERT同款，可训练的Embedding。</p>
<h2 id="decoder-layer">Decoder Layer</h2>
<p>Decoder Layer主要由注意力层，MLP与RMSNorm组成。前向传播代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">      residual &#x3D; hidden_states # 1. 保留残差x

      hidden_states &#x3D; self.input_layernorm(hidden_states) #2. 计算前置RMSNorm

      #3. Self Attention
      hidden_states, self_attn_weights, present_key_value &#x3D; self.self_attn(
          hidden_states&#x3D;hidden_states,
          attention_mask&#x3D;attention_mask,
          position_ids&#x3D;position_ids,
          past_key_value&#x3D;past_key_value,
          output_attentions&#x3D;output_attentions,
          use_cache&#x3D;use_cache,
          cache_position&#x3D;cache_position,
          **kwargs,
      )
      # 4. 计算残差
      hidden_states &#x3D; residual + hidden_states

      # 保留残差
      residual &#x3D; hidden_states
      # 6. MLP之前的RMSNorm
      hidden_states &#x3D; self.post_attention_layernorm(hidden_states)
      # 7.计算MLP
      hidden_states &#x3D; self.mlp(hidden_states)
      # 8.计算残差
      hidden_states &#x3D; residual + hidden_states

      outputs &#x3D; (hidden_states,)
# 是否输出Attention Weight
      if output_attentions:
          outputs +&#x3D; (self_attn_weights,)
# KV Cache
      if use_cache:
          outputs +&#x3D; (present_key_value,)

      return outputs</code></pre>
<h3 id="rmsnorm">RMSNorm</h3>
<p>原论文地址：https://arxiv.org/pdf/1910.07467.pdf</p>
<p>总结一下就是省略计算mean的操作不会影响性能，但可以节省大量计算开销。</p>
<p>作为对比，我们首先回顾一下LayerNorm。 <span class="math display">\[
y = \frac{x-E(x)}{\sqrt{var(x)+\epsilon}}
\]</span></p>
<p>其中x为对应dim上的一组数。</p>
<p>代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LayerNorm(nn.Module):
	def __init__(self,d_model,eps&#x3D;1e-12):
		super().__init__()
		self.gamma &#x3D; nn.Parameter(torch.ones(d_model))
        self.beta &#x3D; nn.Parameter(torch.zeros(d_model))
        self.eps &#x3D; 1e-12
    def forward(self,x):
        var,mean &#x3D; torch.var_mean(x,dim&#x3D;-1,keepdim&#x3D;True)
        out &#x3D; (x-mean) &#x2F; torch.sqrt(var+self.eps)
        out &#x3D; self.gamma * out + self.beta
        return out</code></pre>
<p>LayerNorm通过归一化处理，防止每一层的分布发生剧烈变化，减少Internal
Covariate
Shift，让分布趋于稳定，使得梯度传播更加稳定，有助于减少梯度消失和梯度爆炸。也减少了对训练数据分布的依赖，提高模型的泛化能力。</p>
<p>RMSNorm全名为 Root Mean Square
Norm，去除了LayerNorm中的均值部分，包括分子中减去均值的操作和分母中计算方差时减去均值的操作。也就是说，分母变成了均方根。
<span class="math display">\[
y = \frac{x}{RMS(x)} ,RMS(x) = \sqrt{\frac{\sum_{i=1}^Nx_i^2}{N}}
\]</span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class RMSNorm(nn.Module):
	def __init__(self,d_model,eps&#x3D;1e-12):
		super().__init__()
		self.gamma &#x3D; nn.Parameter(torch.ones(d_model))
		self.beta &#x3D; nn.Parameter(torch.zeros(d_model))
	def forward(self,x):
		rms &#x3D; x.pow(2).mean(dim&#x3D;-1,keepdim&#x3D;True)
		out &#x3D; x * torch.rsqrt(rms + self.eps)
		return self.gamma * out + self.beta</code></pre>
<h3 id="attention">Attention</h3>
<p>Attention可选三种。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">LLAMA_ATTENTION_CLASSES &#x3D; &#123;
    &quot;eager&quot;: LlamaAttention,
    &quot;flash_attention_2&quot;: LlamaFlashAttention2,
    &quot;sdpa&quot;: LlamaSdpaAttention,
&#125;</code></pre>
<p>其中<code>LlamaAttention</code>与<code>LlamaSdpaAttention</code>的计算相同，后者使用了torch官方的sdpa
API。而<code>LlamaFlashAttention2</code>是对FlashAttentionV2的实现。我们这里只看<code>LlamaAttention</code>。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LlamaAttention(nn.Module):
    &quot;&quot;&quot;Multi-headed attention from &#39;Attention Is All You Need&#39; paper&quot;&quot;&quot;

    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] &#x3D; None):
        super().__init__()
        self.config &#x3D; config
        self.layer_idx &#x3D; layer_idx
        if layer_idx is None:
            logger.warning_once(
                f&quot;Instantiating &#123;self.__class__.__name__&#125; without passing a &#96;layer_idx&#96; is not recommended and will &quot;
                &quot;lead to errors during the forward call if caching is used. Please make sure to provide a &#96;layer_idx&#96; &quot;
                &quot;when creating this class.&quot;
            )

        self.attention_dropout &#x3D; config.attention_dropout
        self.hidden_size &#x3D; config.hidden_size
        self.num_heads &#x3D; config.num_attention_heads
        self.head_dim &#x3D; self.hidden_size &#x2F;&#x2F; self.num_heads
        self.num_key_value_heads &#x3D; config.num_key_value_heads
        self.num_key_value_groups &#x3D; self.num_heads &#x2F;&#x2F; self.num_key_value_heads
        self.max_position_embeddings &#x3D; config.max_position_embeddings
        self.rope_theta &#x3D; config.rope_theta
        self.is_causal &#x3D; True

        if (self.head_dim * self.num_heads) !&#x3D; self.hidden_size:
            raise ValueError(
                f&quot;hidden_size must be divisible by num_heads (got &#96;hidden_size&#96;: &#123;self.hidden_size&#125;&quot;
                f&quot; and &#96;num_heads&#96;: &#123;self.num_heads&#125;).&quot;
            )
		# 注意，这里只有Q是完整的4096×4096维，而KV都是1024×1024维
        self.q_proj &#x3D; nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias&#x3D;config.attention_bias)
        self.k_proj &#x3D; nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias&#x3D;config.attention_bias)
        self.v_proj &#x3D; nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias&#x3D;config.attention_bias)
        self.o_proj &#x3D; nn.Linear(self.hidden_size, self.hidden_size, bias&#x3D;config.attention_bias)
        self._init_rope()</code></pre>
<p>在初始化中，大体和原始的Attention是一样的。区别的地方在于使用了GQA，所以KV的head数与Q会不同。</p>
<p>在llama3-8B中，完整的config如下：</p>
<pre class="line-numbers language-json" data-language="json"><code class="language-json">&#123;
  &quot;architectures&quot;: [
    &quot;LlamaForCausalLM&quot;
  ],
  &quot;attention_bias&quot;: false,
  &quot;attention_dropout&quot;: 0.0,
  &quot;bos_token_id&quot;: 128000,
  &quot;eos_token_id&quot;: 128009,
  &quot;hidden_act&quot;: &quot;silu&quot;,
  &quot;hidden_size&quot;: 4096,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 14336,
  &quot;max_position_embeddings&quot;: 8192,
  &quot;model_type&quot;: &quot;llama&quot;,
  &quot;num_attention_heads&quot;: 32,
  &quot;num_hidden_layers&quot;: 32,
  &quot;num_key_value_heads&quot;: 8,
  &quot;pretraining_tp&quot;: 1,
  &quot;rms_norm_eps&quot;: 1e-05,
  &quot;rope_scaling&quot;: null,
  &quot;rope_theta&quot;: 500000.0,
  &quot;tie_word_embeddings&quot;: false,
  &quot;torch_dtype&quot;: &quot;bfloat16&quot;,
  &quot;transformers_version&quot;: &quot;4.40.0.dev0&quot;,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 128256
&#125;</code></pre>
<p>可以看到Q的head数是32，每一个head包含128维的向量。而KV的head数只有8，意味着每4个Q为一组，共享一组KV。</p>
<p>在计算Attention
Score之前，需要计算出旋转矩阵，与输入的x相乘。由于默认情况下，<code>rope_scaling=null</code>，所以会采用最普通的旋转矩阵。</p>
<figure>
<img src="../images/image-20240718154653674.png"
alt="image-20240718154653674" />
<figcaption aria-hidden="true">image-20240718154653674</figcaption>
</figure>
<figure>
<img src="../images/image-20240718154454439.png"
alt="image-20240718154454439" />
<figcaption aria-hidden="true">image-20240718154454439</figcaption>
</figure>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LlamaRotaryEmbedding(nn.Module):
    def __init__(self, dim, max_position_embeddings&#x3D;2048, base&#x3D;10000, device&#x3D;None, scaling_factor&#x3D;1.0):
        super().__init__()
        self.scaling_factor &#x3D; scaling_factor
        self.dim &#x3D; dim # head_dim &#x3D; 128
        self.max_position_embeddings &#x3D; max_position_embeddings
        self.base &#x3D; base # θ
        # 10000^(i&#x2F;d) i &#x3D; [0,2,4,...,126]
        inv_freq &#x3D; 1.0 &#x2F; (self.base ** (torch.arange(0, self.dim, 2, dtype&#x3D;torch.int64).float().to(device) &#x2F; self.dim))
        self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent&#x3D;False)
        # 最大序列长度为8192
        self.max_seq_len_cached &#x3D; max_position_embeddings
        # 公式中的m，也就是pos
        t &#x3D; torch.arange(self.max_seq_len_cached, device&#x3D;device, dtype&#x3D;torch.int64).type_as(self.inv_freq)
        t &#x3D; t &#x2F; self.scaling_factor
        # 计算外积，也就是[pos*10000^(i&#x2F;d)],其实就是cos与sin中的角度
        freqs &#x3D; torch.outer(t, self.inv_freq)
        # Different from paper, but it uses a different permutation in order to obtain the same calculation
        # 在最后一维进行拼接 [seq_len,2*head_dim]
        emb &#x3D; torch.cat((freqs, freqs), dim&#x3D;-1)
        # 登记cos(emb)与sin(emb)，也就是cosmθ与sinmθ
        self.register_buffer(&quot;_cos_cached&quot;, emb.cos().to(torch.get_default_dtype()), persistent&#x3D;False)
        self.register_buffer(&quot;_sin_cached&quot;, emb.sin().to(torch.get_default_dtype()), persistent&#x3D;False)
        
    @torch.no_grad()
    def forward(self, x, position_ids):
        # x: [bs, num_attention_heads, seq_len, head_size]
        inv_freq_expanded &#x3D; self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)
        position_ids_expanded &#x3D; position_ids[:, None, :].float()
        # Force float32 since bfloat16 loses precision on long contexts
        # See https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;pull&#x2F;29285
        device_type &#x3D; x.device.type
        device_type &#x3D; device_type if isinstance(device_type, str) and device_type !&#x3D; &quot;mps&quot; else &quot;cpu&quot;
        with torch.autocast(device_type&#x3D;device_type, enabled&#x3D;False):
            freqs &#x3D; (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb &#x3D; torch.cat((freqs, freqs), dim&#x3D;-1)
            cos &#x3D; emb.cos()
            sin &#x3D; emb.sin()
        return cos.to(dtype&#x3D;x.dtype), sin.to(dtype&#x3D;x.dtype)
    
def apply_rotary_pos_emb(q, k, cos, sin, position_ids&#x3D;None, unsqueeze_dim&#x3D;1):
    &quot;&quot;&quot;Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (&#96;torch.Tensor&#96;): The query tensor.
        k (&#96;torch.Tensor&#96;): The key tensor.
        cos (&#96;torch.Tensor&#96;): The cosine part of the rotary embedding.
        sin (&#96;torch.Tensor&#96;): The sine part of the rotary embedding.
        position_ids (&#96;torch.Tensor&#96;, *optional*):
            Deprecated and unused.
        unsqueeze_dim (&#96;int&#96;, *optional*, defaults to 1):
            The &#39;unsqueeze_dim&#39; argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim&#x3D;1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim&#x3D;2.
    Returns:
        &#96;tuple(torch.Tensor)&#96; comprising of the query and key tensors rotated using the Rotary Position Embedding.
    &quot;&quot;&quot;
    cos &#x3D; cos.unsqueeze(unsqueeze_dim)
    sin &#x3D; sin.unsqueeze(unsqueeze_dim)
    q_embed &#x3D; (q * cos) + (rotate_half(q) * sin)
    k_embed &#x3D; (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed</code></pre>
<p>下面看前向传播。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  def forward(
      self,
      hidden_states: torch.Tensor,
      attention_mask: Optional[torch.Tensor] &#x3D; None,
      position_ids: Optional[torch.LongTensor] &#x3D; None,
      past_key_value: Optional[Cache] &#x3D; None,
      output_attentions: bool &#x3D; False,
      use_cache: bool &#x3D; False,
      cache_position: Optional[torch.LongTensor] &#x3D; None,
      **kwargs,
  ) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
      bsz, q_len, _ &#x3D; hidden_states.size()
# 关于张量并行的内容
      if self.config.pretraining_tp &gt; 1:
          key_value_slicing &#x3D; (self.num_key_value_heads * self.head_dim) &#x2F;&#x2F; self.config.pretraining_tp
          query_slices &#x3D; self.q_proj.weight.split(
              (self.num_heads * self.head_dim) &#x2F;&#x2F; self.config.pretraining_tp, dim&#x3D;0
          )
          key_slices &#x3D; self.k_proj.weight.split(key_value_slicing, dim&#x3D;0)
          value_slices &#x3D; self.v_proj.weight.split(key_value_slicing, dim&#x3D;0)

          query_states &#x3D; [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]
          query_states &#x3D; torch.cat(query_states, dim&#x3D;-1)

          key_states &#x3D; [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]
          key_states &#x3D; torch.cat(key_states, dim&#x3D;-1)

          value_states &#x3D; [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]
          value_states &#x3D; torch.cat(value_states, dim&#x3D;-1)

      else:# 获取投影矩阵W_Q,W_K,W_V
          query_states &#x3D; self.q_proj(hidden_states)
          key_states &#x3D; self.k_proj(hidden_states)
          value_states &#x3D; self.v_proj(hidden_states)
# 常规操作，为了并行将seq维度与head维度互换
      query_states &#x3D; query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
      key_states &#x3D; key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
      value_states &#x3D; value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

      past_key_value &#x3D; getattr(self, &quot;past_key_value&quot;, past_key_value)
      cos, sin &#x3D; self.rotary_emb(value_states, position_ids)
      # 对Q K运用旋转矩阵
      query_states, key_states &#x3D; apply_rotary_pos_emb(query_states, key_states, cos, sin)

      if past_key_value is not None:
          # sin and cos are specific to RoPE models; cache_position needed for the static cache
          cache_kwargs &#x3D; &#123;&quot;sin&quot;: sin, &quot;cos&quot;: cos, &quot;cache_position&quot;: cache_position&#125;
          key_states, value_states &#x3D; past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

      # 由于运用了GQA，所以需要重复，每4个Q公用一组KV
      key_states &#x3D; repeat_kv(key_states, self.num_key_value_groups)
      value_states &#x3D; repeat_kv(value_states, self.num_key_value_groups)
# 接下来的操作就与Attention的操作一模一样了
      attn_weights &#x3D; torch.matmul(query_states, key_states.transpose(2, 3)) &#x2F; math.sqrt(self.head_dim)

      if attention_mask is not None:  # no matter the length, we just slice it
          causal_mask &#x3D; attention_mask[:, :, :, : key_states.shape[-2]]
          attn_weights &#x3D; attn_weights + causal_mask

      # upcast attention to fp32
      attn_weights &#x3D; nn.functional.softmax(attn_weights, dim&#x3D;-1, dtype&#x3D;torch.float32).to(query_states.dtype)
      attn_weights &#x3D; nn.functional.dropout(attn_weights, p&#x3D;self.attention_dropout, training&#x3D;self.training)
      attn_output &#x3D; torch.matmul(attn_weights, value_states)

      if attn_output.size() !&#x3D; (bsz, self.num_heads, q_len, self.head_dim):
          raise ValueError(
              f&quot;&#96;attn_output&#96; should be of size &#123;(bsz, self.num_heads, q_len, self.head_dim)&#125;, but is&quot;
              f&quot; &#123;attn_output.size()&#125;&quot;
          )

      attn_output &#x3D; attn_output.transpose(1, 2).contiguous()

      attn_output &#x3D; attn_output.reshape(bsz, q_len, self.hidden_size)

      if self.config.pretraining_tp &gt; 1:
          attn_output &#x3D; attn_output.split(self.hidden_size &#x2F;&#x2F; self.config.pretraining_tp, dim&#x3D;2)
          o_proj_slices &#x3D; self.o_proj.weight.split(self.hidden_size &#x2F;&#x2F; self.config.pretraining_tp, dim&#x3D;1)
          attn_output &#x3D; sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])
      else:
          attn_output &#x3D; self.o_proj(attn_output)

      if not output_attentions:
          attn_weights &#x3D; None

      return attn_output, attn_weights, past_key_value</code></pre>
<p>关于repeat_kv的实现如下，其实就是对KV进行复制，扩展成与Q的形状相同的tensor。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -&gt; torch.Tensor:
    &quot;&quot;&quot;
    This is the equivalent of torch.repeat_interleave(x, dim&#x3D;1, repeats&#x3D;n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    &quot;&quot;&quot;
    batch, num_key_value_heads, slen, head_dim &#x3D; hidden_states.shape
    # 每一个Q共用一个KV，那就是MHA
    if n_rep &#x3D;&#x3D; 1:
        return hidden_states
    # 先扩展一个维度，让其在dim3上重复4次，先变成[bcz,head,1,seq,head_dim]，然后expand成[bcz,head,4,seq,head_dim]
    hidden_states &#x3D; hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    # reshape成[bcz,num_head,slen,head_dim]，这样就与Q的维度相同了
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)</code></pre>
<h3 id="rmsnorm-1">RMSNorm</h3>
<p>同上，用于归一化注意力模块的输出。</p>
<h3 id="mlp">MLP</h3>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LlamaMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config &#x3D; config
        self.hidden_size &#x3D; config.hidden_size
        self.intermediate_size &#x3D; config.intermediate_size
        self.gate_proj &#x3D; nn.Linear(self.hidden_size, self.intermediate_size, bias&#x3D;False)
        self.up_proj &#x3D; nn.Linear(self.hidden_size, self.intermediate_size, bias&#x3D;False)
        self.down_proj &#x3D; nn.Linear(self.intermediate_size, self.hidden_size, bias&#x3D;False)
        self.act_fn &#x3D; ACT2FN[config.hidden_act]

    def forward(self, x):
        #省略了在张量并行训练下的代码
        down_proj &#x3D; self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))

        return down_proj</code></pre>
<p>x会有两份，其中一份会经过一个门控信号，也就是gate_proj，在经过<code>silu</code>激活函数后与另一份经过升维操作的x进行哈达玛积。其中<span
class="math inline">\(silu(x) =
x⋅σ(x)\)</span>，σ(x)是sigmoid函数。最后再进行降维。</p>
<h3 id="rmsnorm-2">RMSNorm</h3>
<p>用于归一化LlamaMLP的输出。</p>
<h3 id="linear">Linear</h3>
<p>本文以<code>LlamaForCausalLM</code>为例，所以在最后会输入一个Linear层得到logits，也就是潜在生成词的原始分数。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LlamaForCausalLM(LlamaPreTrainedModel):
    _tied_weights_keys &#x3D; [&quot;lm_head.weight&quot;]

    def __init__(self, config):
        super().__init__(config)
        self.model &#x3D; LlamaModel(config)
        self.vocab_size &#x3D; config.vocab_size
        self.lm_head &#x3D; nn.Linear(config.hidden_size, config.vocab_size, bias&#x3D;False) # 此处
</code></pre>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>iroha</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://example.com/post/llama.html" title="llama结构浅析">http://example.com/post/llama.html</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"></div><div class="post-nav-item"><a class="post-nav-next" href="/post/data_collator.html" rel="next" title="Transformers中的DataCollator"><span class="post-nav-text">Transformers中的DataCollator</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2024 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> iroha</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.9</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>