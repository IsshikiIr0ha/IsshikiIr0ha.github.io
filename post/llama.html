<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="iroha"><meta name="copyright" content="iroha"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>llamaç»“æ„æµ…æ | iroha</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"example.com","root":"/","title":"ä¸æƒ³æ‘†çƒ‚","version":"1.10.9","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"æœç´¢...","empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹: ${query}","hits":"æ‰¾åˆ° ${hits} æ¡ç»“æœ","hits_time":"æ‰¾åˆ° ${hits} æ¡ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><meta name="description" content="ä»¥ä¸‹çš„å†…å®¹æ¥è‡ªäºLLaMAgithubä»“åº“ä¸Huggingfaceä¸­çš„LLaMAå®ç°ã€‚ ç»“æ„llamaçš„ç»“æ„è‡ªå§‹è‡³ç»ˆæ²¡æœ‰ä»€ä¹ˆæ”¹å˜ï¼Œæ— éå°±æ˜¯æ¯ä¸€ä»£éƒ½æŠŠGQAçš„å®ç°ä¸‹æ”¾åˆ°å°å‚æ•°ä¸Šã€‚  å¤§ä½“çš„ç»“æ„å’ŒGPT-2è¿˜æ˜¯éå¸¸ç›¸ä¼¼çš„ï¼ŒDecoder-onlyï¼Œå‰ç½®Normç­‰ç­‰ã€‚å…·ä½“ç»“æ„å¯è§ä¸‹é¢çš„ä»£ç ï¼š class LlamaModel(LlamaPreTrainedModel):     &quot;&quot;&quot;     Tran">
<meta property="og:type" content="article">
<meta property="og:title" content="llamaç»“æ„æµ…æ">
<meta property="og:url" content="http://example.com/post/llama.html">
<meta property="og:site_name" content="iroha">
<meta property="og:description" content="ä»¥ä¸‹çš„å†…å®¹æ¥è‡ªäºLLaMAgithubä»“åº“ä¸Huggingfaceä¸­çš„LLaMAå®ç°ã€‚ ç»“æ„llamaçš„ç»“æ„è‡ªå§‹è‡³ç»ˆæ²¡æœ‰ä»€ä¹ˆæ”¹å˜ï¼Œæ— éå°±æ˜¯æ¯ä¸€ä»£éƒ½æŠŠGQAçš„å®ç°ä¸‹æ”¾åˆ°å°å‚æ•°ä¸Šã€‚  å¤§ä½“çš„ç»“æ„å’ŒGPT-2è¿˜æ˜¯éå¸¸ç›¸ä¼¼çš„ï¼ŒDecoder-onlyï¼Œå‰ç½®Normç­‰ç­‰ã€‚å…·ä½“ç»“æ„å¯è§ä¸‹é¢çš„ä»£ç ï¼š class LlamaModel(LlamaPreTrainedModel):     &quot;&quot;&quot;     Tran">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20240708001842029.png">
<meta property="og:image" content="http://example.com/images/image-20240718154653674.png">
<meta property="og:image" content="http://example.com/images/image-20240718154454439.png">
<meta property="article:published_time" content="2024-07-18T08:08:00.000Z">
<meta property="article:modified_time" content="2024-07-18T10:06:00.920Z">
<meta property="article:author" content="iroha">
<meta property="article:tag" content="å¤§æ¨¡å‹">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20240708001842029.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="æ–‡ç« ç›®å½•"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="ç«™ç‚¹æ¦‚è§ˆ"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="iroha"><img width="96" loading="lazy" src="/yun.png" alt="iroha"><span class="site-author-status" title="æ°¸è¿œç›¸ä¿¡ç¾å¥½çš„äº‹æƒ…å³å°†å‘ç”Ÿ">ğŸ˜Š</span></a><div class="site-author-name"><a href="/about/">iroha</a></div><span class="site-name">iroha</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="é¦–é¡µ"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="å½’æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">14</span></a></div><div class="site-state-item"><a href="/categories/" title="åˆ†ç±»"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">4</span></a></div><div class="site-state-item"><a href="/tags/" title="æ ‡ç­¾"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">2</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="æ–‡æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="å‹é“¾" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E6%9E%84"><span class="toc-number">1.</span> <span class="toc-text">ç»“æ„</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Embedding"><span class="toc-number">1.1.</span> <span class="toc-text">Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Decoder-Layer"><span class="toc-number">1.2.</span> <span class="toc-text">Decoder Layer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RMSNorm"><span class="toc-number">1.2.1.</span> <span class="toc-text">RMSNorm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention"><span class="toc-number">1.2.2.</span> <span class="toc-text">Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RMSNorm-1"><span class="toc-number">1.2.3.</span> <span class="toc-text">RMSNorm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MLP"><span class="toc-number">1.2.4.</span> <span class="toc-text">MLP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RMSNorm-2"><span class="toc-number">1.2.5.</span> <span class="toc-text">RMSNorm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Linear"><span class="toc-number">1.2.6.</span> <span class="toc-text">Linear</span></a></li></ol></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://example.com/post/llama.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="iroha"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="iroha"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">llamaç»“æ„æµ…æ</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="åˆ›å»ºæ—¶é—´ï¼š2024-07-18 16:08:00" itemprop="dateCreated datePublished" datetime="2024-07-18T16:08:00+08:00">2024-07-18</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">NLP</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">å¤§æ¨¡å‹</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>ä»¥ä¸‹çš„å†…å®¹æ¥è‡ªäº<a target="_blank" rel="noopener" href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md">LLaMAgithubä»“åº“</a>ä¸Huggingfaceä¸­çš„LLaMAå®ç°ã€‚</p>
<h1 id="ç»“æ„"><a href="#ç»“æ„" class="headerlink" title="ç»“æ„"></a>ç»“æ„</h1><p>llamaçš„ç»“æ„è‡ªå§‹è‡³ç»ˆæ²¡æœ‰ä»€ä¹ˆæ”¹å˜ï¼Œæ— éå°±æ˜¯æ¯ä¸€ä»£éƒ½æŠŠGQAçš„å®ç°ä¸‹æ”¾åˆ°å°å‚æ•°ä¸Šã€‚</p>
<p><img src="../images/image-20240708001842029.png" alt="image-20240708001842029" loading="lazy"></p>
<p>å¤§ä½“çš„ç»“æ„å’ŒGPT-2è¿˜æ˜¯éå¸¸ç›¸ä¼¼çš„ï¼ŒDecoder-onlyï¼Œå‰ç½®Normç­‰ç­‰ã€‚å…·ä½“ç»“æ„å¯è§ä¸‹é¢çš„ä»£ç ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LlamaModel(LlamaPreTrainedModel):
    """
    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]

    Args:
        config: LlamaConfig
    """

    def __init__(self, config: LlamaConfig):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList(
            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.gradient_checkpointing = False

        # Initialize weights and apply final processing
        self.post_init()
        
class LlamaDecoderLayer(nn.Module):
    def __init__(self, config: LlamaConfig, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)

        self.mlp = LlamaMLP(config)
        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        
     
class LlamaForCausalLM(LlamaPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]

    def __init__(self, config):
        super().__init__(config)
        self.model = LlamaModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()</code></pre>
<h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>æ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼ŒBERTåŒæ¬¾ï¼Œå¯è®­ç»ƒçš„Embeddingã€‚</p>
<h2 id="Decoder-Layer"><a href="#Decoder-Layer" class="headerlink" title="Decoder Layer"></a>Decoder Layer</h2><p>Decoder Layerä¸»è¦ç”±æ³¨æ„åŠ›å±‚ï¼ŒMLPä¸RMSNormç»„æˆã€‚å‰å‘ä¼ æ’­ä»£ç å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">      residual = hidden_states # 1. ä¿ç•™æ®‹å·®x

      hidden_states = self.input_layernorm(hidden_states) #2. è®¡ç®—å‰ç½®RMSNorm

      #3. Self Attention
      hidden_states, self_attn_weights, present_key_value = self.self_attn(
          hidden_states=hidden_states,
          attention_mask=attention_mask,
          position_ids=position_ids,
          past_key_value=past_key_value,
          output_attentions=output_attentions,
          use_cache=use_cache,
          cache_position=cache_position,
          **kwargs,
      )
      # 4. è®¡ç®—æ®‹å·®
      hidden_states = residual + hidden_states

      # ä¿ç•™æ®‹å·®
      residual = hidden_states
      # 6. MLPä¹‹å‰çš„RMSNorm
      hidden_states = self.post_attention_layernorm(hidden_states)
      # 7.è®¡ç®—MLP
      hidden_states = self.mlp(hidden_states)
      # 8.è®¡ç®—æ®‹å·®
      hidden_states = residual + hidden_states

      outputs = (hidden_states,)
# æ˜¯å¦è¾“å‡ºAttention Weight
      if output_attentions:
          outputs += (self_attn_weights,)
# KV Cache
      if use_cache:
          outputs += (present_key_value,)

      return outputs</code></pre>
<h3 id="RMSNorm"><a href="#RMSNorm" class="headerlink" title="RMSNorm"></a>RMSNorm</h3><p>åŸè®ºæ–‡åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.07467.pdf">https://arxiv.org/pdf/1910.07467.pdf</a></p>
<p>æ€»ç»“ä¸€ä¸‹å°±æ˜¯çœç•¥è®¡ç®—meançš„æ“ä½œä¸ä¼šå½±å“æ€§èƒ½ï¼Œä½†å¯ä»¥èŠ‚çœå¤§é‡è®¡ç®—å¼€é”€ã€‚</p>
<p>ä½œä¸ºå¯¹æ¯”ï¼Œæˆ‘ä»¬é¦–å…ˆå›é¡¾ä¸€ä¸‹LayerNormã€‚</p>
<script type="math/tex; mode=display">
y = \frac{x-E(x)}{\sqrt{var(x)+\epsilon}}</script><p>å…¶ä¸­xä¸ºå¯¹åº”dimä¸Šçš„ä¸€ç»„æ•°ã€‚</p>
<p>ä»£ç å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LayerNorm(nn.Module):
	def __init__(self,d_model,eps=1e-12):
		super().__init__()
		self.gamma = nn.Parameter(torch.ones(d_model))
        self.beta = nn.Parameter(torch.zeros(d_model))
        self.eps = 1e-12
    def forward(self,x):
        var,mean = torch.var_mean(x,dim=-1,keepdim=True)
        out = (x-mean) / torch.sqrt(var+self.eps)
        out = self.gamma * out + self.beta
        return out</code></pre>
<p>LayerNormé€šè¿‡å½’ä¸€åŒ–å¤„ç†ï¼Œé˜²æ­¢æ¯ä¸€å±‚çš„åˆ†å¸ƒå‘ç”Ÿå‰§çƒˆå˜åŒ–ï¼Œå‡å°‘Internal Covariate Shiftï¼Œè®©åˆ†å¸ƒè¶‹äºç¨³å®šï¼Œä½¿å¾—æ¢¯åº¦ä¼ æ’­æ›´åŠ ç¨³å®šï¼Œæœ‰åŠ©äºå‡å°‘æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸ã€‚ä¹Ÿå‡å°‘äº†å¯¹è®­ç»ƒæ•°æ®åˆ†å¸ƒçš„ä¾èµ–ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p>RMSNormå…¨åä¸º Root Mean Square Normï¼Œå»é™¤äº†LayerNormä¸­çš„å‡å€¼éƒ¨åˆ†ï¼ŒåŒ…æ‹¬åˆ†å­ä¸­å‡å»å‡å€¼çš„æ“ä½œå’Œåˆ†æ¯ä¸­è®¡ç®—æ–¹å·®æ—¶å‡å»å‡å€¼çš„æ“ä½œã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåˆ†æ¯å˜æˆäº†å‡æ–¹æ ¹ã€‚</p>
<script type="math/tex; mode=display">
y = \frac{x}{RMS(x)} ,RMS(x) = \sqrt{\frac{\sum_{i=1}^Nx_i^2}{N}}</script><pre class="line-numbers language-python" data-language="python"><code class="language-python">class RMSNorm(nn.Module):
	def __init__(self,d_model,eps=1e-12):
		super().__init__()
		self.gamma = nn.Parameter(torch.ones(d_model))
		self.beta = nn.Parameter(torch.zeros(d_model))
	def forward(self,x):
		rms = x.pow(2).mean(dim=-1,keepdim=True)
		out = x * torch.rsqrt(rms + self.eps)
		return self.gamma * out + self.beta</code></pre>
<h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>Attentionå¯é€‰ä¸‰ç§ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">LLAMA_ATTENTION_CLASSES = {
    "eager": LlamaAttention,
    "flash_attention_2": LlamaFlashAttention2,
    "sdpa": LlamaSdpaAttention,
}</code></pre>
<p>å…¶ä¸­<code>LlamaAttention</code>ä¸<code>LlamaSdpaAttention</code>çš„è®¡ç®—ç›¸åŒï¼Œåè€…ä½¿ç”¨äº†torchå®˜æ–¹çš„sdpa APIã€‚è€Œ<code>LlamaFlashAttention2</code>æ˜¯å¯¹FlashAttentionV2çš„å®ç°ã€‚æˆ‘ä»¬è¿™é‡Œåªçœ‹<code>LlamaAttention</code>ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LlamaAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        if layer_idx is None:
            logger.warning_once(
                f"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will "
                "lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` "
                "when creating this class."
            )

        self.attention_dropout = config.attention_dropout
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.max_position_embeddings = config.max_position_embeddings
        self.rope_theta = config.rope_theta
        self.is_causal = True

        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )
		# æ³¨æ„ï¼Œè¿™é‡Œåªæœ‰Qæ˜¯å®Œæ•´çš„4096Ã—4096ç»´ï¼Œè€ŒKVéƒ½æ˜¯1024Ã—1024ç»´
        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)
        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)
        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)
        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)
        self._init_rope()</code></pre>
<p>åœ¨åˆå§‹åŒ–ä¸­ï¼Œå¤§ä½“å’ŒåŸå§‹çš„Attentionæ˜¯ä¸€æ ·çš„ã€‚åŒºåˆ«çš„åœ°æ–¹åœ¨äºä½¿ç”¨äº†GQAï¼Œæ‰€ä»¥KVçš„headæ•°ä¸Qä¼šä¸åŒã€‚</p>
<p>åœ¨llama3-8Bä¸­ï¼Œå®Œæ•´çš„configå¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-json" data-language="json"><code class="language-json">{
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "vocab_size": 128256
}</code></pre>
<p>å¯ä»¥çœ‹åˆ°Qçš„headæ•°æ˜¯32ï¼Œæ¯ä¸€ä¸ªheadåŒ…å«128ç»´çš„å‘é‡ã€‚è€ŒKVçš„headæ•°åªæœ‰8ï¼Œæ„å‘³ç€æ¯4ä¸ªQä¸ºä¸€ç»„ï¼Œå…±äº«ä¸€ç»„KVã€‚</p>
<p>åœ¨è®¡ç®—Attention Scoreä¹‹å‰ï¼Œéœ€è¦è®¡ç®—å‡ºæ—‹è½¬çŸ©é˜µï¼Œä¸è¾“å…¥çš„xç›¸ä¹˜ã€‚ç”±äºé»˜è®¤æƒ…å†µä¸‹ï¼Œ<code>rope_scaling=null</code>ï¼Œæ‰€ä»¥ä¼šé‡‡ç”¨æœ€æ™®é€šçš„æ—‹è½¬çŸ©é˜µã€‚</p>
<p><img src="../images/image-20240718154653674.png" alt="image-20240718154653674" loading="lazy"></p>
<p><img src="../images/image-20240718154454439.png" alt="image-20240718154454439" loading="lazy"></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LlamaRotaryEmbedding(nn.Module):
    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):
        super().__init__()
        self.scaling_factor = scaling_factor
        self.dim = dim # head_dim = 128
        self.max_position_embeddings = max_position_embeddings
        self.base = base # Î¸
        # 10000^(i/d) i = [0,2,4,...,126]
        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        # æœ€å¤§åºåˆ—é•¿åº¦ä¸º8192
        self.max_seq_len_cached = max_position_embeddings
        # å…¬å¼ä¸­çš„mï¼Œä¹Ÿå°±æ˜¯pos
        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)
        t = t / self.scaling_factor
        # è®¡ç®—å¤–ç§¯ï¼Œä¹Ÿå°±æ˜¯[pos*10000^(i/d)],å…¶å®å°±æ˜¯cosä¸sinä¸­çš„è§’åº¦
        freqs = torch.outer(t, self.inv_freq)
        # Different from paper, but it uses a different permutation in order to obtain the same calculation
        # åœ¨æœ€åä¸€ç»´è¿›è¡Œæ‹¼æ¥ [seq_len,2*head_dim]
        emb = torch.cat((freqs, freqs), dim=-1)
        # ç™»è®°cos(emb)ä¸sin(emb)ï¼Œä¹Ÿå°±æ˜¯cosmÎ¸ä¸sinmÎ¸
        self.register_buffer("_cos_cached", emb.cos().to(torch.get_default_dtype()), persistent=False)
        self.register_buffer("_sin_cached", emb.sin().to(torch.get_default_dtype()), persistent=False)
        
    @torch.no_grad()
    def forward(self, x, position_ids):
        # x: [bs, num_attention_heads, seq_len, head_size]
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)
        position_ids_expanded = position_ids[:, None, :].float()
        # Force float32 since bfloat16 loses precision on long contexts
        # See https://github.com/huggingface/transformers/pull/29285
        device_type = x.device.type
        device_type = device_type if isinstance(device_type, str) and device_type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos()
            sin = emb.sin()
        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
    
def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed</code></pre>
<p>ä¸‹é¢çœ‹å‰å‘ä¼ æ’­ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  def forward(
      self,
      hidden_states: torch.Tensor,
      attention_mask: Optional[torch.Tensor] = None,
      position_ids: Optional[torch.LongTensor] = None,
      past_key_value: Optional[Cache] = None,
      output_attentions: bool = False,
      use_cache: bool = False,
      cache_position: Optional[torch.LongTensor] = None,
      **kwargs,
  ) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
      bsz, q_len, _ = hidden_states.size()
# å…³äºå¼ é‡å¹¶è¡Œçš„å†…å®¹
      if self.config.pretraining_tp &gt; 1:
          key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp
          query_slices = self.q_proj.weight.split(
              (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0
          )
          key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)
          value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)

          query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]
          query_states = torch.cat(query_states, dim=-1)

          key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]
          key_states = torch.cat(key_states, dim=-1)

          value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]
          value_states = torch.cat(value_states, dim=-1)

      else:# è·å–æŠ•å½±çŸ©é˜µW_Q,W_K,W_V
          query_states = self.q_proj(hidden_states)
          key_states = self.k_proj(hidden_states)
          value_states = self.v_proj(hidden_states)
# å¸¸è§„æ“ä½œï¼Œä¸ºäº†å¹¶è¡Œå°†seqç»´åº¦ä¸headç»´åº¦äº’æ¢
      query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
      key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
      value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

      past_key_value = getattr(self, "past_key_value", past_key_value)
      cos, sin = self.rotary_emb(value_states, position_ids)
      # å¯¹Q Kè¿ç”¨æ—‹è½¬çŸ©é˜µ
      query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

      if past_key_value is not None:
          # sin and cos are specific to RoPE models; cache_position needed for the static cache
          cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
          key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

      # ç”±äºè¿ç”¨äº†GQAï¼Œæ‰€ä»¥éœ€è¦é‡å¤ï¼Œæ¯4ä¸ªQå…¬ç”¨ä¸€ç»„KV
      key_states = repeat_kv(key_states, self.num_key_value_groups)
      value_states = repeat_kv(value_states, self.num_key_value_groups)
# æ¥ä¸‹æ¥çš„æ“ä½œå°±ä¸Attentionçš„æ“ä½œä¸€æ¨¡ä¸€æ ·äº†
      attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)

      if attention_mask is not None:  # no matter the length, we just slice it
          causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
          attn_weights = attn_weights + causal_mask

      # upcast attention to fp32
      attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
      attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)
      attn_output = torch.matmul(attn_weights, value_states)

      if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
          raise ValueError(
              f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
              f" {attn_output.size()}"
          )

      attn_output = attn_output.transpose(1, 2).contiguous()

      attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)

      if self.config.pretraining_tp &gt; 1:
          attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)
          o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)
          attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])
      else:
          attn_output = self.o_proj(attn_output)

      if not output_attentions:
          attn_weights = None

      return attn_output, attn_weights, past_key_value</code></pre>
<p>å…³äºrepeat_kvçš„å®ç°å¦‚ä¸‹ï¼Œå…¶å®å°±æ˜¯å¯¹KVè¿›è¡Œå¤åˆ¶ï¼Œæ‰©å±•æˆä¸Qçš„å½¢çŠ¶ç›¸åŒçš„tensorã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -&gt; torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    # æ¯ä¸€ä¸ªQå…±ç”¨ä¸€ä¸ªKVï¼Œé‚£å°±æ˜¯MHA
    if n_rep == 1:
        return hidden_states
    # å…ˆæ‰©å±•ä¸€ä¸ªç»´åº¦ï¼Œè®©å…¶åœ¨dim3ä¸Šé‡å¤4æ¬¡ï¼Œå…ˆå˜æˆ[bcz,head,1,seq,head_dim]ï¼Œç„¶åexpandæˆ[bcz,head,4,seq,head_dim]
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    # reshapeæˆ[bcz,num_head,slen,head_dim]ï¼Œè¿™æ ·å°±ä¸Qçš„ç»´åº¦ç›¸åŒäº†
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)</code></pre>
<h3 id="RMSNorm-1"><a href="#RMSNorm-1" class="headerlink" title="RMSNorm"></a>RMSNorm</h3><p>åŒä¸Šï¼Œç”¨äºå½’ä¸€åŒ–æ³¨æ„åŠ›æ¨¡å—çš„è¾“å‡ºã€‚</p>
<h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">class LlamaMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        #çœç•¥äº†åœ¨å¼ é‡å¹¶è¡Œè®­ç»ƒä¸‹çš„ä»£ç 
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))

        return down_proj</code></pre>
<p>xä¼šæœ‰ä¸¤ä»½ï¼Œå…¶ä¸­ä¸€ä»½ä¼šç»è¿‡ä¸€ä¸ªé—¨æ§ä¿¡å·ï¼Œä¹Ÿå°±æ˜¯gate_projï¼Œåœ¨ç»è¿‡<code>silu</code>æ¿€æ´»å‡½æ•°åä¸å¦ä¸€ä»½ç»è¿‡å‡ç»´æ“ä½œçš„xè¿›è¡Œå“ˆè¾¾ç›ç§¯ã€‚å…¶ä¸­<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="17.156ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 7583 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(469,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(814,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(1112,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1684,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2073,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2645,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3311.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(4367.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(5161.8,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(5662,0)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(6233,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(6622,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(7194,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>ï¼ŒÏƒ(x)æ˜¯sigmoidå‡½æ•°ã€‚æœ€åå†è¿›è¡Œé™ç»´ã€‚</p>
<h3 id="RMSNorm-2"><a href="#RMSNorm-2" class="headerlink" title="RMSNorm"></a>RMSNorm</h3><p>ç”¨äºå½’ä¸€åŒ–LlamaMLPçš„è¾“å‡ºã€‚</p>
<h3 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h3><p>æœ¬æ–‡ä»¥<code>LlamaForCausalLM</code>ä¸ºä¾‹ï¼Œæ‰€ä»¥åœ¨æœ€åä¼šè¾“å…¥ä¸€ä¸ªLinearå±‚å¾—åˆ°logitsï¼Œä¹Ÿå°±æ˜¯æ½œåœ¨ç”Ÿæˆè¯çš„åŸå§‹åˆ†æ•°ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LlamaForCausalLM(LlamaPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]

    def __init__(self, config):
        super().__init__(config)
        self.model = LlamaModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False) # æ­¤å¤„
</code></pre>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>æœ¬æ–‡ä½œè€…ï¼š</strong>iroha</li><li class="post-copyright-link"><strong>æœ¬æ–‡é“¾æ¥ï¼š</strong><a href="http://example.com/post/llama.html" title="llamaç»“æ„æµ…æ">http://example.com/post/llama.html</a></li><li class="post-copyright-license"><strong>ç‰ˆæƒå£°æ˜ï¼š</strong>æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é»˜è®¤é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> è®¸å¯åè®®ã€‚</li></ul></article><div class="post-nav"><div class="post-nav-item"></div><div class="post-nav-item"><a class="post-nav-next" href="/post/data_collator.html" rel="next" title="Transformersä¸­çš„DataCollator"><span class="post-nav-text">Transformersä¸­çš„DataCollator</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 â€“ 2024 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> iroha</span></div><div class="powered"><span>ç”± <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> é©±åŠ¨ v6.3.0</span><span class="footer-separator">|</span><span>ä¸»é¢˜ - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.9</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>