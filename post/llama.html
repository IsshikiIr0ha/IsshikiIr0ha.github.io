<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="iroha"><meta name="copyright" content="iroha"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>llamaç»“æ„æµ…æ | iroha</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"example.com","root":"/","title":"ä¸æƒ³æ‘†çƒ‚","version":"1.10.9","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"æœç´¢...","empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹: ${query}","hits":"æ‰¾åˆ° ${hits} æ¡ç»“æœ","hits_time":"æ‰¾åˆ° ${hits} æ¡ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><meta name="description" content="ä»¥ä¸‹çš„å†…å®¹æ¥è‡ªäºLLaMAgithubä»“åº“ä¸Huggingfaceä¸­çš„LLaMAå®ç°ã€‚ ç»“æ„ llamaçš„ç»“æ„è‡ªå§‹è‡³ç»ˆæ²¡æœ‰ä»€ä¹ˆæ”¹å˜ï¼Œæ— éå°±æ˜¯æ¯ä¸€ä»£éƒ½æŠŠGQAçš„å®ç°ä¸‹æ”¾åˆ°å°å‚æ•°ä¸Šã€‚   image-20240708001842029  å¤§ä½“çš„ç»“æ„å’ŒGPT-2è¿˜æ˜¯éå¸¸ç›¸ä¼¼çš„ï¼ŒDecoder-onlyï¼Œå‰ç½®Normç­‰ç­‰ã€‚å…·ä½“ç»“æ„å¯è§ä¸‹é¢çš„ä»£ç ï¼š class LlamaModel(Llam">
<meta property="og:type" content="article">
<meta property="og:title" content="llamaç»“æ„æµ…æ">
<meta property="og:url" content="http://example.com/post/llama.html">
<meta property="og:site_name" content="iroha">
<meta property="og:description" content="ä»¥ä¸‹çš„å†…å®¹æ¥è‡ªäºLLaMAgithubä»“åº“ä¸Huggingfaceä¸­çš„LLaMAå®ç°ã€‚ ç»“æ„ llamaçš„ç»“æ„è‡ªå§‹è‡³ç»ˆæ²¡æœ‰ä»€ä¹ˆæ”¹å˜ï¼Œæ— éå°±æ˜¯æ¯ä¸€ä»£éƒ½æŠŠGQAçš„å®ç°ä¸‹æ”¾åˆ°å°å‚æ•°ä¸Šã€‚   image-20240708001842029  å¤§ä½“çš„ç»“æ„å’ŒGPT-2è¿˜æ˜¯éå¸¸ç›¸ä¼¼çš„ï¼ŒDecoder-onlyï¼Œå‰ç½®Normç­‰ç­‰ã€‚å…·ä½“ç»“æ„å¯è§ä¸‹é¢çš„ä»£ç ï¼š class LlamaModel(Llam">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20240708001842029.png">
<meta property="og:image" content="http://example.com/images/image-20240718154653674.png">
<meta property="og:image" content="http://example.com/images/image-20240718154454439.png">
<meta property="article:published_time" content="2024-07-18T08:08:00.000Z">
<meta property="article:modified_time" content="2024-07-22T09:33:37.060Z">
<meta property="article:author" content="iroha">
<meta property="article:tag" content="å¤§æ¨¡å‹">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20240708001842029.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="æ–‡ç« ç›®å½•"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="ç«™ç‚¹æ¦‚è§ˆ"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="iroha"><img width="96" loading="lazy" src="/yun.png" alt="iroha"><span class="site-author-status" title="æ°¸è¿œç›¸ä¿¡ç¾å¥½çš„äº‹æƒ…å³å°†å‘ç”Ÿ">ğŸ˜Š</span></a><div class="site-author-name"><a href="/about/">iroha</a></div><span class="site-name">iroha</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="é¦–é¡µ"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="å½’æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">14</span></a></div><div class="site-state-item"><a href="/categories/" title="åˆ†ç±»"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">4</span></a></div><div class="site-state-item"><a href="/tags/" title="æ ‡ç­¾"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">2</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="æ–‡æ¡£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="å‹é“¾" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E6%9E%84"><span class="toc-number">1.</span> <span class="toc-text">ç»“æ„</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#embedding"><span class="toc-number">1.1.</span> <span class="toc-text">Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#decoder-layer"><span class="toc-number">1.2.</span> <span class="toc-text">Decoder Layer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#rmsnorm"><span class="toc-number">1.2.1.</span> <span class="toc-text">RMSNorm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#attention"><span class="toc-number">1.2.2.</span> <span class="toc-text">Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rmsnorm-1"><span class="toc-number">1.2.3.</span> <span class="toc-text">RMSNorm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mlp"><span class="toc-number">1.2.4.</span> <span class="toc-text">MLP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rmsnorm-2"><span class="toc-number">1.2.5.</span> <span class="toc-text">RMSNorm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#linear"><span class="toc-number">1.2.6.</span> <span class="toc-text">Linear</span></a></li></ol></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://example.com/post/llama.html"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="iroha"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="iroha"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">llamaç»“æ„æµ…æ</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="åˆ›å»ºæ—¶é—´ï¼š2024-07-18 16:08:00" itemprop="dateCreated datePublished" datetime="2024-07-18T16:08:00+08:00">2024-07-18</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="ä¿®æ”¹æ—¶é—´ï¼š2024-07-22 17:33:37" itemprop="dateModified" datetime="2024-07-22T17:33:37+08:00">2024-07-22</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/NLP/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">NLP</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">å¤§æ¨¡å‹</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>ä»¥ä¸‹çš„å†…å®¹æ¥è‡ªäº<a target="_blank" rel="noopener" href='https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md'>LLaMAgithubä»“åº“</a>ä¸Huggingfaceä¸­çš„LLaMAå®ç°ã€‚</p>
<h1 id="ç»“æ„">ç»“æ„</h1>
<p>llamaçš„ç»“æ„è‡ªå§‹è‡³ç»ˆæ²¡æœ‰ä»€ä¹ˆæ”¹å˜ï¼Œæ— éå°±æ˜¯æ¯ä¸€ä»£éƒ½æŠŠGQAçš„å®ç°ä¸‹æ”¾åˆ°å°å‚æ•°ä¸Šã€‚</p>
<figure>
<img src="../images/image-20240708001842029.png"
alt="image-20240708001842029" />
<figcaption aria-hidden="true">image-20240708001842029</figcaption>
</figure>
<p>å¤§ä½“çš„ç»“æ„å’ŒGPT-2è¿˜æ˜¯éå¸¸ç›¸ä¼¼çš„ï¼ŒDecoder-onlyï¼Œå‰ç½®Normç­‰ç­‰ã€‚å…·ä½“ç»“æ„å¯è§ä¸‹é¢çš„ä»£ç ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LlamaModel(LlamaPreTrainedModel):
    &quot;&quot;&quot;
    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [&#96;LlamaDecoderLayer&#96;]

    Args:
        config: LlamaConfig
    &quot;&quot;&quot;

    def __init__(self, config: LlamaConfig):
        super().__init__(config)
        self.padding_idx &#x3D; config.pad_token_id
        self.vocab_size &#x3D; config.vocab_size

        self.embed_tokens &#x3D; nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers &#x3D; nn.ModuleList(
            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm &#x3D; LlamaRMSNorm(config.hidden_size, eps&#x3D;config.rms_norm_eps)
        self.gradient_checkpointing &#x3D; False

        # Initialize weights and apply final processing
        self.post_init()
        
class LlamaDecoderLayer(nn.Module):
    def __init__(self, config: LlamaConfig, layer_idx: int):
        super().__init__()
        self.hidden_size &#x3D; config.hidden_size

        self.self_attn &#x3D; LLAMA_ATTENTION_CLASSES[config._attn_implementation](config&#x3D;config, layer_idx&#x3D;layer_idx)

        self.mlp &#x3D; LlamaMLP(config)
        self.input_layernorm &#x3D; LlamaRMSNorm(config.hidden_size, eps&#x3D;config.rms_norm_eps)
        self.post_attention_layernorm &#x3D; LlamaRMSNorm(config.hidden_size, eps&#x3D;config.rms_norm_eps)
        
     
class LlamaForCausalLM(LlamaPreTrainedModel):
    _tied_weights_keys &#x3D; [&quot;lm_head.weight&quot;]

    def __init__(self, config):
        super().__init__(config)
        self.model &#x3D; LlamaModel(config)
        self.vocab_size &#x3D; config.vocab_size
        self.lm_head &#x3D; nn.Linear(config.hidden_size, config.vocab_size, bias&#x3D;False)

        # Initialize weights and apply final processing
        self.post_init()</code></pre>
<h2 id="embedding">Embedding</h2>
<p>æ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼ŒBERTåŒæ¬¾ï¼Œå¯è®­ç»ƒçš„Embeddingã€‚</p>
<h2 id="decoder-layer">Decoder Layer</h2>
<p>Decoder Layerä¸»è¦ç”±æ³¨æ„åŠ›å±‚ï¼ŒMLPä¸RMSNormç»„æˆã€‚å‰å‘ä¼ æ’­ä»£ç å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">      residual &#x3D; hidden_states # 1. ä¿ç•™æ®‹å·®x

      hidden_states &#x3D; self.input_layernorm(hidden_states) #2. è®¡ç®—å‰ç½®RMSNorm

      #3. Self Attention
      hidden_states, self_attn_weights, present_key_value &#x3D; self.self_attn(
          hidden_states&#x3D;hidden_states,
          attention_mask&#x3D;attention_mask,
          position_ids&#x3D;position_ids,
          past_key_value&#x3D;past_key_value,
          output_attentions&#x3D;output_attentions,
          use_cache&#x3D;use_cache,
          cache_position&#x3D;cache_position,
          **kwargs,
      )
      # 4. è®¡ç®—æ®‹å·®
      hidden_states &#x3D; residual + hidden_states

      # ä¿ç•™æ®‹å·®
      residual &#x3D; hidden_states
      # 6. MLPä¹‹å‰çš„RMSNorm
      hidden_states &#x3D; self.post_attention_layernorm(hidden_states)
      # 7.è®¡ç®—MLP
      hidden_states &#x3D; self.mlp(hidden_states)
      # 8.è®¡ç®—æ®‹å·®
      hidden_states &#x3D; residual + hidden_states

      outputs &#x3D; (hidden_states,)
# æ˜¯å¦è¾“å‡ºAttention Weight
      if output_attentions:
          outputs +&#x3D; (self_attn_weights,)
# KV Cache
      if use_cache:
          outputs +&#x3D; (present_key_value,)

      return outputs</code></pre>
<h3 id="rmsnorm">RMSNorm</h3>
<p>åŸè®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/pdf/1910.07467.pdf</p>
<p>æ€»ç»“ä¸€ä¸‹å°±æ˜¯çœç•¥è®¡ç®—meançš„æ“ä½œä¸ä¼šå½±å“æ€§èƒ½ï¼Œä½†å¯ä»¥èŠ‚çœå¤§é‡è®¡ç®—å¼€é”€ã€‚</p>
<p>ä½œä¸ºå¯¹æ¯”ï¼Œæˆ‘ä»¬é¦–å…ˆå›é¡¾ä¸€ä¸‹LayerNormã€‚ <span class="math display">\[
y = \frac{x-E(x)}{\sqrt{var(x)+\epsilon}}
\]</span></p>
<p>å…¶ä¸­xä¸ºå¯¹åº”dimä¸Šçš„ä¸€ç»„æ•°ã€‚</p>
<p>ä»£ç å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LayerNorm(nn.Module):
	def __init__(self,d_model,eps&#x3D;1e-12):
		super().__init__()
		self.gamma &#x3D; nn.Parameter(torch.ones(d_model))
        self.beta &#x3D; nn.Parameter(torch.zeros(d_model))
        self.eps &#x3D; 1e-12
    def forward(self,x):
        var,mean &#x3D; torch.var_mean(x,dim&#x3D;-1,keepdim&#x3D;True)
        out &#x3D; (x-mean) &#x2F; torch.sqrt(var+self.eps)
        out &#x3D; self.gamma * out + self.beta
        return out</code></pre>
<p>LayerNormé€šè¿‡å½’ä¸€åŒ–å¤„ç†ï¼Œé˜²æ­¢æ¯ä¸€å±‚çš„åˆ†å¸ƒå‘ç”Ÿå‰§çƒˆå˜åŒ–ï¼Œå‡å°‘Internal
Covariate
Shiftï¼Œè®©åˆ†å¸ƒè¶‹äºç¨³å®šï¼Œä½¿å¾—æ¢¯åº¦ä¼ æ’­æ›´åŠ ç¨³å®šï¼Œæœ‰åŠ©äºå‡å°‘æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸ã€‚ä¹Ÿå‡å°‘äº†å¯¹è®­ç»ƒæ•°æ®åˆ†å¸ƒçš„ä¾èµ–ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p>RMSNormå…¨åä¸º Root Mean Square
Normï¼Œå»é™¤äº†LayerNormä¸­çš„å‡å€¼éƒ¨åˆ†ï¼ŒåŒ…æ‹¬åˆ†å­ä¸­å‡å»å‡å€¼çš„æ“ä½œå’Œåˆ†æ¯ä¸­è®¡ç®—æ–¹å·®æ—¶å‡å»å‡å€¼çš„æ“ä½œã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåˆ†æ¯å˜æˆäº†å‡æ–¹æ ¹ã€‚
<span class="math display">\[
y = \frac{x}{RMS(x)} ,RMS(x) = \sqrt{\frac{\sum_{i=1}^Nx_i^2}{N}}
\]</span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class RMSNorm(nn.Module):
	def __init__(self,d_model,eps&#x3D;1e-12):
		super().__init__()
		self.gamma &#x3D; nn.Parameter(torch.ones(d_model))
		self.beta &#x3D; nn.Parameter(torch.zeros(d_model))
	def forward(self,x):
		rms &#x3D; x.pow(2).mean(dim&#x3D;-1,keepdim&#x3D;True)
		out &#x3D; x * torch.rsqrt(rms + self.eps)
		return self.gamma * out + self.beta</code></pre>
<h3 id="attention">Attention</h3>
<p>Attentionå¯é€‰ä¸‰ç§ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">LLAMA_ATTENTION_CLASSES &#x3D; &#123;
    &quot;eager&quot;: LlamaAttention,
    &quot;flash_attention_2&quot;: LlamaFlashAttention2,
    &quot;sdpa&quot;: LlamaSdpaAttention,
&#125;</code></pre>
<p>å…¶ä¸­<code>LlamaAttention</code>ä¸<code>LlamaSdpaAttention</code>çš„è®¡ç®—ç›¸åŒï¼Œåè€…ä½¿ç”¨äº†torchå®˜æ–¹çš„sdpa
APIã€‚è€Œ<code>LlamaFlashAttention2</code>æ˜¯å¯¹FlashAttentionV2çš„å®ç°ã€‚æˆ‘ä»¬è¿™é‡Œåªçœ‹<code>LlamaAttention</code>ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LlamaAttention(nn.Module):
    &quot;&quot;&quot;Multi-headed attention from &#39;Attention Is All You Need&#39; paper&quot;&quot;&quot;

    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] &#x3D; None):
        super().__init__()
        self.config &#x3D; config
        self.layer_idx &#x3D; layer_idx
        if layer_idx is None:
            logger.warning_once(
                f&quot;Instantiating &#123;self.__class__.__name__&#125; without passing a &#96;layer_idx&#96; is not recommended and will &quot;
                &quot;lead to errors during the forward call if caching is used. Please make sure to provide a &#96;layer_idx&#96; &quot;
                &quot;when creating this class.&quot;
            )

        self.attention_dropout &#x3D; config.attention_dropout
        self.hidden_size &#x3D; config.hidden_size
        self.num_heads &#x3D; config.num_attention_heads
        self.head_dim &#x3D; self.hidden_size &#x2F;&#x2F; self.num_heads
        self.num_key_value_heads &#x3D; config.num_key_value_heads
        self.num_key_value_groups &#x3D; self.num_heads &#x2F;&#x2F; self.num_key_value_heads
        self.max_position_embeddings &#x3D; config.max_position_embeddings
        self.rope_theta &#x3D; config.rope_theta
        self.is_causal &#x3D; True

        if (self.head_dim * self.num_heads) !&#x3D; self.hidden_size:
            raise ValueError(
                f&quot;hidden_size must be divisible by num_heads (got &#96;hidden_size&#96;: &#123;self.hidden_size&#125;&quot;
                f&quot; and &#96;num_heads&#96;: &#123;self.num_heads&#125;).&quot;
            )
		# æ³¨æ„ï¼Œè¿™é‡Œåªæœ‰Qæ˜¯å®Œæ•´çš„4096Ã—4096ç»´ï¼Œè€ŒKVéƒ½æ˜¯1024Ã—1024ç»´
        self.q_proj &#x3D; nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias&#x3D;config.attention_bias)
        self.k_proj &#x3D; nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias&#x3D;config.attention_bias)
        self.v_proj &#x3D; nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias&#x3D;config.attention_bias)
        self.o_proj &#x3D; nn.Linear(self.hidden_size, self.hidden_size, bias&#x3D;config.attention_bias)
        self._init_rope()</code></pre>
<p>åœ¨åˆå§‹åŒ–ä¸­ï¼Œå¤§ä½“å’ŒåŸå§‹çš„Attentionæ˜¯ä¸€æ ·çš„ã€‚åŒºåˆ«çš„åœ°æ–¹åœ¨äºä½¿ç”¨äº†GQAï¼Œæ‰€ä»¥KVçš„headæ•°ä¸Qä¼šä¸åŒã€‚</p>
<p>åœ¨llama3-8Bä¸­ï¼Œå®Œæ•´çš„configå¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-json" data-language="json"><code class="language-json">&#123;
  &quot;architectures&quot;: [
    &quot;LlamaForCausalLM&quot;
  ],
  &quot;attention_bias&quot;: false,
  &quot;attention_dropout&quot;: 0.0,
  &quot;bos_token_id&quot;: 128000,
  &quot;eos_token_id&quot;: 128009,
  &quot;hidden_act&quot;: &quot;silu&quot;,
  &quot;hidden_size&quot;: 4096,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 14336,
  &quot;max_position_embeddings&quot;: 8192,
  &quot;model_type&quot;: &quot;llama&quot;,
  &quot;num_attention_heads&quot;: 32,
  &quot;num_hidden_layers&quot;: 32,
  &quot;num_key_value_heads&quot;: 8,
  &quot;pretraining_tp&quot;: 1,
  &quot;rms_norm_eps&quot;: 1e-05,
  &quot;rope_scaling&quot;: null,
  &quot;rope_theta&quot;: 500000.0,
  &quot;tie_word_embeddings&quot;: false,
  &quot;torch_dtype&quot;: &quot;bfloat16&quot;,
  &quot;transformers_version&quot;: &quot;4.40.0.dev0&quot;,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 128256
&#125;</code></pre>
<p>å¯ä»¥çœ‹åˆ°Qçš„headæ•°æ˜¯32ï¼Œæ¯ä¸€ä¸ªheadåŒ…å«128ç»´çš„å‘é‡ã€‚è€ŒKVçš„headæ•°åªæœ‰8ï¼Œæ„å‘³ç€æ¯4ä¸ªQä¸ºä¸€ç»„ï¼Œå…±äº«ä¸€ç»„KVã€‚</p>
<p>åœ¨è®¡ç®—Attention
Scoreä¹‹å‰ï¼Œéœ€è¦è®¡ç®—å‡ºæ—‹è½¬çŸ©é˜µï¼Œä¸è¾“å…¥çš„xç›¸ä¹˜ã€‚ç”±äºé»˜è®¤æƒ…å†µä¸‹ï¼Œ<code>rope_scaling=null</code>ï¼Œæ‰€ä»¥ä¼šé‡‡ç”¨æœ€æ™®é€šçš„æ—‹è½¬çŸ©é˜µã€‚</p>
<figure>
<img src="../images/image-20240718154653674.png"
alt="image-20240718154653674" />
<figcaption aria-hidden="true">image-20240718154653674</figcaption>
</figure>
<figure>
<img src="../images/image-20240718154454439.png"
alt="image-20240718154454439" />
<figcaption aria-hidden="true">image-20240718154454439</figcaption>
</figure>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LlamaRotaryEmbedding(nn.Module):
    def __init__(self, dim, max_position_embeddings&#x3D;2048, base&#x3D;10000, device&#x3D;None, scaling_factor&#x3D;1.0):
        super().__init__()
        self.scaling_factor &#x3D; scaling_factor
        self.dim &#x3D; dim # head_dim &#x3D; 128
        self.max_position_embeddings &#x3D; max_position_embeddings
        self.base &#x3D; base # Î¸
        # 10000^(i&#x2F;d) i &#x3D; [0,2,4,...,126]
        inv_freq &#x3D; 1.0 &#x2F; (self.base ** (torch.arange(0, self.dim, 2, dtype&#x3D;torch.int64).float().to(device) &#x2F; self.dim))
        self.register_buffer(&quot;inv_freq&quot;, inv_freq, persistent&#x3D;False)
        # æœ€å¤§åºåˆ—é•¿åº¦ä¸º8192
        self.max_seq_len_cached &#x3D; max_position_embeddings
        # å…¬å¼ä¸­çš„mï¼Œä¹Ÿå°±æ˜¯pos
        t &#x3D; torch.arange(self.max_seq_len_cached, device&#x3D;device, dtype&#x3D;torch.int64).type_as(self.inv_freq)
        t &#x3D; t &#x2F; self.scaling_factor
        # è®¡ç®—å¤–ç§¯ï¼Œä¹Ÿå°±æ˜¯[pos*10000^(i&#x2F;d)],å…¶å®å°±æ˜¯cosä¸sinä¸­çš„è§’åº¦
        freqs &#x3D; torch.outer(t, self.inv_freq)
        # Different from paper, but it uses a different permutation in order to obtain the same calculation
        # åœ¨æœ€åä¸€ç»´è¿›è¡Œæ‹¼æ¥ [seq_len,2*head_dim]
        emb &#x3D; torch.cat((freqs, freqs), dim&#x3D;-1)
        # ç™»è®°cos(emb)ä¸sin(emb)ï¼Œä¹Ÿå°±æ˜¯cosmÎ¸ä¸sinmÎ¸
        self.register_buffer(&quot;_cos_cached&quot;, emb.cos().to(torch.get_default_dtype()), persistent&#x3D;False)
        self.register_buffer(&quot;_sin_cached&quot;, emb.sin().to(torch.get_default_dtype()), persistent&#x3D;False)
        
    @torch.no_grad()
    def forward(self, x, position_ids):
        # x: [bs, num_attention_heads, seq_len, head_size]
        inv_freq_expanded &#x3D; self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)
        position_ids_expanded &#x3D; position_ids[:, None, :].float()
        # Force float32 since bfloat16 loses precision on long contexts
        # See https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;pull&#x2F;29285
        device_type &#x3D; x.device.type
        device_type &#x3D; device_type if isinstance(device_type, str) and device_type !&#x3D; &quot;mps&quot; else &quot;cpu&quot;
        with torch.autocast(device_type&#x3D;device_type, enabled&#x3D;False):
            freqs &#x3D; (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb &#x3D; torch.cat((freqs, freqs), dim&#x3D;-1)
            cos &#x3D; emb.cos()
            sin &#x3D; emb.sin()
        return cos.to(dtype&#x3D;x.dtype), sin.to(dtype&#x3D;x.dtype)
    
def apply_rotary_pos_emb(q, k, cos, sin, position_ids&#x3D;None, unsqueeze_dim&#x3D;1):
    &quot;&quot;&quot;Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (&#96;torch.Tensor&#96;): The query tensor.
        k (&#96;torch.Tensor&#96;): The key tensor.
        cos (&#96;torch.Tensor&#96;): The cosine part of the rotary embedding.
        sin (&#96;torch.Tensor&#96;): The sine part of the rotary embedding.
        position_ids (&#96;torch.Tensor&#96;, *optional*):
            Deprecated and unused.
        unsqueeze_dim (&#96;int&#96;, *optional*, defaults to 1):
            The &#39;unsqueeze_dim&#39; argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim&#x3D;1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim&#x3D;2.
    Returns:
        &#96;tuple(torch.Tensor)&#96; comprising of the query and key tensors rotated using the Rotary Position Embedding.
    &quot;&quot;&quot;
    cos &#x3D; cos.unsqueeze(unsqueeze_dim)
    sin &#x3D; sin.unsqueeze(unsqueeze_dim)
    q_embed &#x3D; (q * cos) + (rotate_half(q) * sin)
    k_embed &#x3D; (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed</code></pre>
<p>ä¸‹é¢çœ‹å‰å‘ä¼ æ’­ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">  def forward(
      self,
      hidden_states: torch.Tensor,
      attention_mask: Optional[torch.Tensor] &#x3D; None,
      position_ids: Optional[torch.LongTensor] &#x3D; None,
      past_key_value: Optional[Cache] &#x3D; None,
      output_attentions: bool &#x3D; False,
      use_cache: bool &#x3D; False,
      cache_position: Optional[torch.LongTensor] &#x3D; None,
      **kwargs,
  ) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
      bsz, q_len, _ &#x3D; hidden_states.size()
# å…³äºå¼ é‡å¹¶è¡Œçš„å†…å®¹
      if self.config.pretraining_tp &gt; 1:
          key_value_slicing &#x3D; (self.num_key_value_heads * self.head_dim) &#x2F;&#x2F; self.config.pretraining_tp
          query_slices &#x3D; self.q_proj.weight.split(
              (self.num_heads * self.head_dim) &#x2F;&#x2F; self.config.pretraining_tp, dim&#x3D;0
          )
          key_slices &#x3D; self.k_proj.weight.split(key_value_slicing, dim&#x3D;0)
          value_slices &#x3D; self.v_proj.weight.split(key_value_slicing, dim&#x3D;0)

          query_states &#x3D; [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]
          query_states &#x3D; torch.cat(query_states, dim&#x3D;-1)

          key_states &#x3D; [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]
          key_states &#x3D; torch.cat(key_states, dim&#x3D;-1)

          value_states &#x3D; [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]
          value_states &#x3D; torch.cat(value_states, dim&#x3D;-1)

      else:# è·å–æŠ•å½±çŸ©é˜µW_Q,W_K,W_V
          query_states &#x3D; self.q_proj(hidden_states)
          key_states &#x3D; self.k_proj(hidden_states)
          value_states &#x3D; self.v_proj(hidden_states)
# å¸¸è§„æ“ä½œï¼Œä¸ºäº†å¹¶è¡Œå°†seqç»´åº¦ä¸headç»´åº¦äº’æ¢
      query_states &#x3D; query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
      key_states &#x3D; key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
      value_states &#x3D; value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

      past_key_value &#x3D; getattr(self, &quot;past_key_value&quot;, past_key_value)
      cos, sin &#x3D; self.rotary_emb(value_states, position_ids)
      # å¯¹Q Kè¿ç”¨æ—‹è½¬çŸ©é˜µ
      query_states, key_states &#x3D; apply_rotary_pos_emb(query_states, key_states, cos, sin)

      if past_key_value is not None:
          # sin and cos are specific to RoPE models; cache_position needed for the static cache
          cache_kwargs &#x3D; &#123;&quot;sin&quot;: sin, &quot;cos&quot;: cos, &quot;cache_position&quot;: cache_position&#125;
          key_states, value_states &#x3D; past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

      # ç”±äºè¿ç”¨äº†GQAï¼Œæ‰€ä»¥éœ€è¦é‡å¤ï¼Œæ¯4ä¸ªQå…¬ç”¨ä¸€ç»„KV
      key_states &#x3D; repeat_kv(key_states, self.num_key_value_groups)
      value_states &#x3D; repeat_kv(value_states, self.num_key_value_groups)
# æ¥ä¸‹æ¥çš„æ“ä½œå°±ä¸Attentionçš„æ“ä½œä¸€æ¨¡ä¸€æ ·äº†
      attn_weights &#x3D; torch.matmul(query_states, key_states.transpose(2, 3)) &#x2F; math.sqrt(self.head_dim)

      if attention_mask is not None:  # no matter the length, we just slice it
          causal_mask &#x3D; attention_mask[:, :, :, : key_states.shape[-2]]
          attn_weights &#x3D; attn_weights + causal_mask

      # upcast attention to fp32
      attn_weights &#x3D; nn.functional.softmax(attn_weights, dim&#x3D;-1, dtype&#x3D;torch.float32).to(query_states.dtype)
      attn_weights &#x3D; nn.functional.dropout(attn_weights, p&#x3D;self.attention_dropout, training&#x3D;self.training)
      attn_output &#x3D; torch.matmul(attn_weights, value_states)

      if attn_output.size() !&#x3D; (bsz, self.num_heads, q_len, self.head_dim):
          raise ValueError(
              f&quot;&#96;attn_output&#96; should be of size &#123;(bsz, self.num_heads, q_len, self.head_dim)&#125;, but is&quot;
              f&quot; &#123;attn_output.size()&#125;&quot;
          )

      attn_output &#x3D; attn_output.transpose(1, 2).contiguous()

      attn_output &#x3D; attn_output.reshape(bsz, q_len, self.hidden_size)

      if self.config.pretraining_tp &gt; 1:
          attn_output &#x3D; attn_output.split(self.hidden_size &#x2F;&#x2F; self.config.pretraining_tp, dim&#x3D;2)
          o_proj_slices &#x3D; self.o_proj.weight.split(self.hidden_size &#x2F;&#x2F; self.config.pretraining_tp, dim&#x3D;1)
          attn_output &#x3D; sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])
      else:
          attn_output &#x3D; self.o_proj(attn_output)

      if not output_attentions:
          attn_weights &#x3D; None

      return attn_output, attn_weights, past_key_value</code></pre>
<p>å…³äºrepeat_kvçš„å®ç°å¦‚ä¸‹ï¼Œå…¶å®å°±æ˜¯å¯¹KVè¿›è¡Œå¤åˆ¶ï¼Œæ‰©å±•æˆä¸Qçš„å½¢çŠ¶ç›¸åŒçš„tensorã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -&gt; torch.Tensor:
    &quot;&quot;&quot;
    This is the equivalent of torch.repeat_interleave(x, dim&#x3D;1, repeats&#x3D;n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    &quot;&quot;&quot;
    batch, num_key_value_heads, slen, head_dim &#x3D; hidden_states.shape
    # æ¯ä¸€ä¸ªQå…±ç”¨ä¸€ä¸ªKVï¼Œé‚£å°±æ˜¯MHA
    if n_rep &#x3D;&#x3D; 1:
        return hidden_states
    # å…ˆæ‰©å±•ä¸€ä¸ªç»´åº¦ï¼Œè®©å…¶åœ¨dim3ä¸Šé‡å¤4æ¬¡ï¼Œå…ˆå˜æˆ[bcz,head,1,seq,head_dim]ï¼Œç„¶åexpandæˆ[bcz,head,4,seq,head_dim]
    hidden_states &#x3D; hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    # reshapeæˆ[bcz,num_head,slen,head_dim]ï¼Œè¿™æ ·å°±ä¸Qçš„ç»´åº¦ç›¸åŒäº†
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)</code></pre>
<h3 id="rmsnorm-1">RMSNorm</h3>
<p>åŒä¸Šï¼Œç”¨äºå½’ä¸€åŒ–æ³¨æ„åŠ›æ¨¡å—çš„è¾“å‡ºã€‚</p>
<h3 id="mlp">MLP</h3>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LlamaMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config &#x3D; config
        self.hidden_size &#x3D; config.hidden_size
        self.intermediate_size &#x3D; config.intermediate_size
        self.gate_proj &#x3D; nn.Linear(self.hidden_size, self.intermediate_size, bias&#x3D;False)
        self.up_proj &#x3D; nn.Linear(self.hidden_size, self.intermediate_size, bias&#x3D;False)
        self.down_proj &#x3D; nn.Linear(self.intermediate_size, self.hidden_size, bias&#x3D;False)
        self.act_fn &#x3D; ACT2FN[config.hidden_act]

    def forward(self, x):
        #çœç•¥äº†åœ¨å¼ é‡å¹¶è¡Œè®­ç»ƒä¸‹çš„ä»£ç 
        down_proj &#x3D; self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))

        return down_proj</code></pre>
<p>xä¼šæœ‰ä¸¤ä»½ï¼Œå…¶ä¸­ä¸€ä»½ä¼šç»è¿‡ä¸€ä¸ªé—¨æ§ä¿¡å·ï¼Œä¹Ÿå°±æ˜¯gate_projï¼Œåœ¨ç»è¿‡<code>silu</code>æ¿€æ´»å‡½æ•°åä¸å¦ä¸€ä»½ç»è¿‡å‡ç»´æ“ä½œçš„xè¿›è¡Œå“ˆè¾¾ç›ç§¯ã€‚å…¶ä¸­<span
class="math inline">\(silu(x) =
xâ‹…Ïƒ(x)\)</span>ï¼ŒÏƒ(x)æ˜¯sigmoidå‡½æ•°ã€‚æœ€åå†è¿›è¡Œé™ç»´ã€‚</p>
<h3 id="rmsnorm-2">RMSNorm</h3>
<p>ç”¨äºå½’ä¸€åŒ–LlamaMLPçš„è¾“å‡ºã€‚</p>
<h3 id="linear">Linear</h3>
<p>æœ¬æ–‡ä»¥<code>LlamaForCausalLM</code>ä¸ºä¾‹ï¼Œæ‰€ä»¥åœ¨æœ€åä¼šè¾“å…¥ä¸€ä¸ªLinearå±‚å¾—åˆ°logitsï¼Œä¹Ÿå°±æ˜¯æ½œåœ¨ç”Ÿæˆè¯çš„åŸå§‹åˆ†æ•°ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class LlamaForCausalLM(LlamaPreTrainedModel):
    _tied_weights_keys &#x3D; [&quot;lm_head.weight&quot;]

    def __init__(self, config):
        super().__init__(config)
        self.model &#x3D; LlamaModel(config)
        self.vocab_size &#x3D; config.vocab_size
        self.lm_head &#x3D; nn.Linear(config.hidden_size, config.vocab_size, bias&#x3D;False) # æ­¤å¤„
</code></pre>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>æœ¬æ–‡ä½œè€…ï¼š</strong>iroha</li><li class="post-copyright-link"><strong>æœ¬æ–‡é“¾æ¥ï¼š</strong><a href="http://example.com/post/llama.html" title="llamaç»“æ„æµ…æ">http://example.com/post/llama.html</a></li><li class="post-copyright-license"><strong>ç‰ˆæƒå£°æ˜ï¼š</strong>æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é»˜è®¤é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> è®¸å¯åè®®ã€‚</li></ul></article><div class="post-nav"><div class="post-nav-item"></div><div class="post-nav-item"><a class="post-nav-next" href="/post/data_collator.html" rel="next" title="Transformersä¸­çš„DataCollator"><span class="post-nav-text">Transformersä¸­çš„DataCollator</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 â€“ 2024 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> iroha</span></div><div class="powered"><span>ç”± <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> é©±åŠ¨ v6.3.0</span><span class="footer-separator">|</span><span>ä¸»é¢˜ - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.9</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>